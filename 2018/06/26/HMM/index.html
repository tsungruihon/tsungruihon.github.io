<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="拖了有一段时间了，今天终于有机会来总结一下HMM，这是一个非常优美的算法。 下文将从基础介绍、HMM的三个问题以及代码一步步了解HMM世界。 一、基础介绍 在这一节中，主要从形式定义、隐马尔科夫模型的两个基本假设以及举例对HMM有一个初步的认识。 1.形式定义 首先看下面这张图，对模型有一个感性的认识。  描述： 图分成两行，第一行是y序列，第二行是x序列，每个x都只有一个y指向它，而每个y都会有">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="Hidden Markov Model">
<meta property="og:url" content="http://yoursite.com/2018/06/26/HMM/index.html">
<meta property="og:site_name" content="彼得的博客">
<meta property="og:description" content="拖了有一段时间了，今天终于有机会来总结一下HMM，这是一个非常优美的算法。 下文将从基础介绍、HMM的三个问题以及代码一步步了解HMM世界。 一、基础介绍 在这一节中，主要从形式定义、隐马尔科夫模型的两个基本假设以及举例对HMM有一个初步的认识。 1.形式定义 首先看下面这张图，对模型有一个感性的认识。  描述： 图分成两行，第一行是y序列，第二行是x序列，每个x都只有一个y指向它，而每个y都会有">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/HMM_1.png">
<meta property="og:image" content="http://yoursite.com/images/HMM_2.png">
<meta property="og:updated_time" content="2018-07-01T17:28:45.608Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hidden Markov Model">
<meta name="twitter:description" content="拖了有一段时间了，今天终于有机会来总结一下HMM，这是一个非常优美的算法。 下文将从基础介绍、HMM的三个问题以及代码一步步了解HMM世界。 一、基础介绍 在这一节中，主要从形式定义、隐马尔科夫模型的两个基本假设以及举例对HMM有一个初步的认识。 1.形式定义 首先看下面这张图，对模型有一个感性的认识。  描述： 图分成两行，第一行是y序列，第二行是x序列，每个x都只有一个y指向它，而每个y都会有">
<meta name="twitter:image" content="http://yoursite.com/images/HMM_1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/26/HMM/"/>




<link href="/js/google-code-prettify/prettify.css" type="text/css" rel="stylesheet" />
<link href="/js/google-code-prettify/github-v2.min.css" type="text/css" rel="stylesheet" />

  <title>Hidden Markov Model | 彼得的博客</title>
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">彼得的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/26/HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Hidden Markov Model
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-26T21:32:44+08:00">
                2018-06-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>拖了有一段时间了，今天终于有机会来总结一下<code>HMM</code>，这是一个非常优美的算法。</p>
<p>下文将从<code>基础介绍</code>、<code>HMM的三个问题</code>以及<code>代码</code>一步步了解<code>HMM</code>世界。</p>
<p><strong>一、基础介绍</strong></p>
<p>在这一节中，主要从<code>形式定义</code>、<code>隐马尔科夫模型的两个基本假设</code>以及<code>举例</code>对<code>HMM</code>有一个初步的认识。</p>
<p><strong>1.形式定义</strong></p>
<p>首先看下面这张图，对模型有一个感性的认识。</p>
<p><img src="/images/HMM_1.png"></p>
<p>描述：</p>
<p>图分成两行，第一行是<code>y</code>序列，第二行是<code>x</code>序列，每个<code>x</code>都只有一个<code>y</code>指向它，而每个<code>y</code>都会有另一个<code>y</code>指向它。在这里，我们就引出了<code>HMM</code>的定义了：</p>
<ul>
<li><strong>状态序列（I）</strong>：隐藏的马尔科夫链随机生成的状态序列，成为<code>状态序列(state sequence)</code></li>
<li><strong>观测序列（O）</strong>：每个状态生成一个观测，而由此产生的观测随机序列，成为<code>观测序列(observation sequence)</code></li>
<li><strong>隐马尔科夫模型</strong>：隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成<strong>不可观测的状态随机序列</strong>，再由各个<strong>状态</strong>生成一个观测而<strong>产生观测随机序列</strong>的过程。</li>
</ul>
<p>下面我们用数学语言定义：</p>
<p>设<span class="math inline">\(Q\)</span>是所有可能状态的集合，<span class="math inline">\(V\)</span>是所有可能观测的集合。 <span class="math display">\[
Q= \{q_1, q_2, ..., q_N\}, \ \ \ \ V = \{v_1, v_2, ..., v_M\}
\]</span> 其中，<span class="math inline">\(N\)</span>是可能的状态数，<code>M</code>是可能的观测数。</p>
<p><span class="math inline">\(I\)</span>是长度为<span class="math inline">\(T\)</span>的状态序列，<span class="math inline">\(O\)</span>是对应的观测序列。 <span class="math display">\[
I= (i_1, i_2, ..., i_T), \ \ \ \ O = (o_1, o_2, ..., o_T)
\]</span> <strong>A是状态转移概率矩阵：</strong> <span class="math inline">\(A = [a_{ij}]_{N \times N}\)</span></p>
<p><span class="math inline">\(i = 1, 2, ..., N; \ \ j = 1, 2, ..., N\)</span></p>
<p>其中，在时刻<span class="math inline">\(t\)</span>，处于状态<span class="math inline">\(q_i\)</span>的条件下在时刻<span class="math inline">\(t+1\)</span>转移到状态<span class="math inline">\(q_j\)</span>的概率为： <span class="math display">\[
a_{ij} = P(i_{t+1} = q_j | i_t = q_i)
\]</span> <strong>B是观测概率矩阵：</strong><span class="math inline">\(B = [b_j(k)]_{N \times M}\)</span></p>
<p><span class="math inline">\(k = 1, 2, ..., M; j = 1, 2, ..., N\)</span></p>
<p>其中，在时刻<span class="math inline">\(t\)</span>，处于状态<span class="math inline">\(q_j\)</span>的条件下生成观测<span class="math inline">\(v_k\)</span>的概率为： <span class="math display">\[
b_j(k) = P(o_t  = v_k | i_t = q_j)
\]</span></p>
<p><strong><span class="math inline">\(\pi\)</span>是初始状态概率向量</strong>： <span class="math inline">\(\pi = (\pi_i)\)</span></p>
<p>也就是说，在时刻<span class="math inline">\(t=1\)</span>处于状态<span class="math inline">\(q_i\)</span>的概率为<span class="math inline">\(\pi_i = P(i_i = q_i), \  \ \ i = 1, 2, ..., N\)</span></p>
<p><strong>总结一下</strong>，<code>隐马尔可夫模型(HMM)</code>由<strong>初始状态概率向量<span class="math inline">\(\pi\)</span></strong>、<strong>状态转移概率矩阵<span class="math inline">\(A\)</span></strong>和<strong>观测概率矩阵</strong><span class="math inline">\(B\)</span>决定。<span class="math inline">\(\pi\)</span>和<span class="math inline">\(A\)</span>决定状态序列，<span class="math inline">\(B\)</span>决定观测序列。因此，隐马尔科夫模型<span class="math inline">\(\lambda\)</span>可以由三元符号表述，即 <span class="math display">\[
\lambda = (A, B, \pi)
\]</span> <span class="math inline">\(A\)</span>、<span class="math inline">\(B\)</span>、<span class="math inline">\(\pi\)</span>成为隐马尔科夫模型的<strong>三要素</strong>。</p>
<p><strong>2.隐马尔科夫模型的两个基本假设</strong></p>
<ul>
<li><p><strong>齐次马尔科夫性假设：</strong>假设隐马尔可夫链在任意时刻<span class="math inline">\(t\)</span>的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻<span class="math inline">\(t\)</span>无关，即 <span class="math display">\[
P(i_t|i_{t-1}, o_{t-1}, ..., i_1, o_1) = P(i_t |i_{t-1}), \ \ \ t = 1, 2, ..., T
\]</span></p></li>
<li><p><strong>观测独立性假设：</strong>假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。 <span class="math display">\[
P(o_t | i_T, o_T, i_{T-1}, o_{T-1}, ..., i_{t+1}, o_{t+1}, i_t, o_{t-1}, ..., i_1, o_1) = p(o_t | i_t)
\]</span></p></li>
</ul>
<p><strong>3.例子</strong></p>
<p>在<a href="https://en.wikipedia.org/wiki/Viterbi_algorithm#Example" target="_blank" rel="noopener">wikipidia</a>上有一个关于感冒预测的例子，我觉得非常好，在这里我尝试简单的描述：</p>
<p>假设Peter是一个医生，眼前有一个病人Sophia，Peter的任务是确定Sophia是否得了感冒。</p>
<p>接下来我们就去套用<code>HMM</code>的定义：</p>
<ul>
<li>病人的状态(<span class="math inline">\(Q\)</span>)有两种： {<code>感冒</code>、<code>没有感冒</code>}</li>
<li>病人的感觉(观测<span class="math inline">\(V\)</span>)有三种：{<code>正常</code>、<code>冷</code>、<code>头晕</code>}</li>
<li>目前Peter有Sophia的病例，他可以从病例的第一天确定<span class="math inline">\(\pi\)</span><strong>（初始状态概率向量）</strong>，也就是Sopia在第一天<code>感冒</code>和<code>没有感冒</code>的概率。</li>
<li>Peter也掌握了Sophia某天是否感冒(<span class="math inline">\(i_t\)</span>)与第二天是否感冒(<span class="math inline">\(i_{t+1}\)</span>)的关系<strong>（状态转移矩阵）</strong>。</li>
<li>Peter还掌握了Sophia某天的感觉与是否感冒的关系<strong>(观测概率矩阵)</strong>。</li>
</ul>
<p>下面我们用一张图来看看Dr.Peter掌握了什么信息。</p>
<p><img src="/images/HMM_2.png"></p>
<p>下面我们用代码来体现我们目前的定义。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># 状态集合Q</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">states <span class="op">=</span> (<span class="st">&#39;Healthy&#39;</span>, <span class="st">&#39;Fever&#39;</span>)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co"># 观测结合V</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">observations <span class="op">=</span> (<span class="st">&#39;dizzy&#39;</span>, <span class="st">&#39;cold&#39;</span>, <span class="st">&#39;normal&#39;</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="co"># 初始状态概率向量 \pi</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">start_probability <span class="op">=</span> {<span class="st">&#39;Healthy&#39;</span>: <span class="fl">0.6</span>, <span class="st">&#39;Fever&#39;</span>: <span class="fl">0.4</span>}</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co"># 状态转移概率矩阵A</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8">transition_probability <span class="op">=</span> {</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">    <span class="st">&#39;Healthy&#39;</span>: {<span class="st">&#39;Healthy&#39;</span>: <span class="fl">0.7</span>, <span class="st">&#39;Fever&#39;</span>: <span class="fl">0.3</span>},</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">    <span class="st">&#39;Fever&#39;</span>: {<span class="st">&#39;Healthy&#39;</span>: <span class="fl">0.4</span>, <span class="st">&#39;Fever&#39;</span>: <span class="fl">0.6</span>},</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="co"># 观测概率矩阵B</span></a>
<a class="sourceLine" id="cb1-13" data-line-number="13">emission_probability <span class="op">=</span> {</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">    <span class="st">&#39;Healthy&#39;</span>: {<span class="st">&#39;normal&#39;</span>: <span class="fl">0.5</span>, <span class="st">&#39;cold&#39;</span>: <span class="fl">0.4</span>, <span class="st">&#39;dizzy&#39;</span>: <span class="fl">0.1</span>},</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">    <span class="st">&#39;Fever&#39;</span>: {<span class="st">&#39;normal&#39;</span>: <span class="fl">0.1</span>, <span class="st">&#39;cold&#39;</span>: <span class="fl">0.3</span>, <span class="st">&#39;dizzy&#39;</span>: <span class="fl">0.6</span>},</a>
<a class="sourceLine" id="cb1-16" data-line-number="16">}</a></code></pre></div>
<p>然后为了方便后续的计算，我们需要把数据源的map形式转换成矩阵的形式。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> generate_index_map(labels):</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    id2label <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">    label2id <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">    i <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">    <span class="cf">for</span> l <span class="kw">in</span> labels:</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">        id2label[i] <span class="op">=</span> l</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">        label2id[l] <span class="op">=</span> i</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">        i <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9">    <span class="cf">return</span> id2label, label2id</a>
<a class="sourceLine" id="cb2-10" data-line-number="10"></a>
<a class="sourceLine" id="cb2-11" data-line-number="11">states_id2label, states_label2id <span class="op">=</span> generate_index_map(states)</a>
<a class="sourceLine" id="cb2-12" data-line-number="12">observations_id2label, observations_label2id <span class="op">=</span> generate_index_map(observations)</a>
<a class="sourceLine" id="cb2-13" data-line-number="13"><span class="bu">print</span>(states_id2label, states_label2id)</a>
<a class="sourceLine" id="cb2-14" data-line-number="14"><span class="bu">print</span>(observations_id2label, observations_label2id)</a>
<a class="sourceLine" id="cb2-15" data-line-number="15">[output]</a>
<a class="sourceLine" id="cb2-16" data-line-number="16">{<span class="dv">0</span>: <span class="st">&#39;Healthy&#39;</span>, <span class="dv">1</span>: <span class="st">&#39;Fever&#39;</span>} {<span class="st">&#39;Fever&#39;</span>: <span class="dv">1</span>, <span class="st">&#39;Healthy&#39;</span>: <span class="dv">0</span>}</a>
<a class="sourceLine" id="cb2-17" data-line-number="17">{<span class="dv">0</span>: <span class="st">&#39;dizzy&#39;</span>, <span class="dv">1</span>: <span class="st">&#39;cold&#39;</span>, <span class="dv">2</span>: <span class="st">&#39;normal&#39;</span>} {<span class="st">&#39;cold&#39;</span>: <span class="dv">1</span>, <span class="st">&#39;normal&#39;</span>: <span class="dv">2</span>, <span class="st">&#39;dizzy&#39;</span>: <span class="dv">0</span>}</a>
<a class="sourceLine" id="cb2-18" data-line-number="18"></a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="kw">def</span> convert_map_to_vector(map_, label2id):</a>
<a class="sourceLine" id="cb2-20" data-line-number="20">    <span class="co">&quot;&quot;&quot;将初始状态概率向量从dict转移成array&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-21" data-line-number="21">    v <span class="op">=</span> np.zeros(<span class="bu">len</span>(map_), dtype<span class="op">=</span><span class="bu">float</span>)</a>
<a class="sourceLine" id="cb2-22" data-line-number="22">    <span class="cf">for</span> e <span class="kw">in</span> map_:</a>
<a class="sourceLine" id="cb2-23" data-line-number="23">        v[label2id[e]] <span class="op">=</span> map_[e]</a>
<a class="sourceLine" id="cb2-24" data-line-number="24">    <span class="cf">return</span> v</a>
<a class="sourceLine" id="cb2-25" data-line-number="25"></a>
<a class="sourceLine" id="cb2-26" data-line-number="26">pi <span class="op">=</span> convert_map_to_vector(start_probability, states_label2id)</a>
<a class="sourceLine" id="cb2-27" data-line-number="27"><span class="bu">print</span>(pi)</a>
<a class="sourceLine" id="cb2-28" data-line-number="28">[output]</a>
<a class="sourceLine" id="cb2-29" data-line-number="29">array([<span class="fl">0.6</span>, <span class="fl">0.4</span>])</a>
<a class="sourceLine" id="cb2-30" data-line-number="30"></a>
<a class="sourceLine" id="cb2-31" data-line-number="31"><span class="kw">def</span> convert_map_to_matrix(map_, label2id1, label2id2):</a>
<a class="sourceLine" id="cb2-32" data-line-number="32">    <span class="co">&quot;&quot;&quot;将观测概率和状态转移概率从dict转换成矩阵&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-33" data-line-number="33">    m <span class="op">=</span> np.zeros((<span class="bu">len</span>(label2id1), <span class="bu">len</span>(label2id2)), dtype<span class="op">=</span><span class="bu">float</span>)</a>
<a class="sourceLine" id="cb2-34" data-line-number="34">    <span class="cf">for</span> line <span class="kw">in</span> map_:</a>
<a class="sourceLine" id="cb2-35" data-line-number="35">        <span class="cf">for</span> col <span class="kw">in</span> map_[line]:</a>
<a class="sourceLine" id="cb2-36" data-line-number="36">            m[label2id1[line]][label2id2[col]] <span class="op">=</span> map_[line][col]</a>
<a class="sourceLine" id="cb2-37" data-line-number="37">    <span class="cf">return</span> m</a>
<a class="sourceLine" id="cb2-38" data-line-number="38"></a>
<a class="sourceLine" id="cb2-39" data-line-number="39">A <span class="op">=</span> convert_map_to_matrix(transition_probability, states_label2id, states_label2id)</a>
<a class="sourceLine" id="cb2-40" data-line-number="40">B <span class="op">=</span> convert_map_to_matrix(emission_probability, states_label2id, observations_label2id)</a>
<a class="sourceLine" id="cb2-41" data-line-number="41"><span class="bu">print</span>(A)</a>
<a class="sourceLine" id="cb2-42" data-line-number="42"><span class="bu">print</span>(B)</a>
<a class="sourceLine" id="cb2-43" data-line-number="43">[output]</a>
<a class="sourceLine" id="cb2-44" data-line-number="44">array([[<span class="fl">0.7</span>, <span class="fl">0.3</span>],</a>
<a class="sourceLine" id="cb2-45" data-line-number="45">       [<span class="fl">0.4</span>, <span class="fl">0.6</span>]])</a>
<a class="sourceLine" id="cb2-46" data-line-number="46"></a>
<a class="sourceLine" id="cb2-47" data-line-number="47">array([[<span class="fl">0.1</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>],</a>
<a class="sourceLine" id="cb2-48" data-line-number="48">       [<span class="fl">0.6</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>]])</a></code></pre></div>
<p>然后我们来看看如何生成观测序列。</p>
<p>根据隐马尔科夫模型定义，可以将一个长度为<span class="math inline">\(T\)</span>的观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>的生成过程描述如下：</p>
<p><strong>输入</strong>：隐马尔科夫模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>，观测序列长度<span class="math inline">\(T\)</span>；</p>
<p><strong>输出</strong>：观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>.</p>
<p>（1）按照初始状态分布<span class="math inline">\(\pi\)</span>产生状态<span class="math inline">\(i_1\)</span></p>
<p>（2）令<span class="math inline">\(t=1\)</span></p>
<p>（3）按照状态<span class="math inline">\(i_t\)</span>的观测概率分布<span class="math inline">\(b_{i_t}(k)\)</span>生成<span class="math inline">\(o_t\)</span></p>
<p>（4）按照状态<span class="math inline">\(i_t\)</span>的状态转移概率分布<span class="math inline">\({ai_ti_{t+1}}\)</span>产生状态<span class="math inline">\(i_{t+1}\)</span>， <span class="math inline">\(i_{t+1} = 1, 2, ..., N\)</span></p>
<p>（5）令<span class="math inline">\(t=t+1\)</span>；如果<span class="math inline">\(t&lt;T\)</span>，转步（3）；否则，终止</p>
<p>下面我们来用代码实现。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">def</span> simulate(T):</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    </a>
<a class="sourceLine" id="cb3-3" data-line-number="3">    <span class="kw">def</span> draw_from(probs):</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">        <span class="cf">return</span> np.where(np.random.multinomial(<span class="dv">1</span>, probs) <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">    </a>
<a class="sourceLine" id="cb3-6" data-line-number="6">    observations <span class="op">=</span> np.zeros(T, dtype<span class="op">=</span><span class="bu">int</span>)</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    states <span class="op">=</span> np.zeros(T, dtype<span class="op">=</span><span class="bu">int</span>)</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">    states[<span class="dv">0</span>] <span class="op">=</span> draw_from(pi)</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">    observations[<span class="dv">0</span>] <span class="op">=</span> draw_from(B[states[<span class="dv">0</span>], :])</a>
<a class="sourceLine" id="cb3-10" data-line-number="10">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">        states[t] <span class="op">=</span> draw_from(A[states[t<span class="dv">-1</span>],:])</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">        observations[t] <span class="op">=</span> draw_from(B[states[t], :])</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">    <span class="cf">return</span> observations, states</a>
<a class="sourceLine" id="cb3-14" data-line-number="14"></a>
<a class="sourceLine" id="cb3-15" data-line-number="15">observations_data, states_data <span class="op">=</span> simulate(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb3-16" data-line-number="16"><span class="bu">print</span>(states_data)</a>
<a class="sourceLine" id="cb3-17" data-line-number="17"><span class="bu">print</span>(observations_data)</a>
<a class="sourceLine" id="cb3-18" data-line-number="18">[output]</a>
<a class="sourceLine" id="cb3-19" data-line-number="19">[<span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb3-20" data-line-number="20">[<span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span>]</a></code></pre></div>
<p><strong>二、HMM的三个问题</strong></p>
<p><code>HMM</code>有三个基本问题：</p>
<ul>
<li><p>概率计算问题：给定模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>和观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，计算在模型<span class="math inline">\(\lambda\)</span>下观测序列<span class="math inline">\(O\)</span>出现的概率<span class="math inline">\(P(O|\lambda)\)</span>.</p></li>
<li><p>学习问题：<strong>已知观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，估计模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>，使<span class="math inline">\(P(O|\lambda)\)</span>最大。即用</strong>极大似然估计的方法估计参数。</p></li>
<li><p>预测问题（也称为解码（decoding）问题）：已知模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>和观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，求对给定观测序列条件概率<span class="math inline">\(P(I|O)\)</span>最大的状态序列<span class="math inline">\(I=(i_1, i_2, ..., i_T)\)</span>。即给定观测序列，求最有可能的对应状态序列。</p></li>
</ul>
<p>回到感冒的例子，这三个问题就是：</p>
<ul>
<li><p><strong>概率计算问题</strong>：如果给定模型<span class="math inline">\(\lambda=(A, B, \pi)\)</span>，计算Sophia出现特定观测序列的概率。</p></li>
<li><p><strong>学习问题</strong>：根据Sophia某一系列观测序列，学习模型参数。</p></li>
<li><p><strong>预测问题：</strong>根据学到的模型，通过Sophia的观测序列判断这几天是否有感冒。</p></li>
</ul>
<h4 id="概率计算问题"><strong>2.1概率计算问题</strong></h4>
<p>概率计算问题计算的是：在模型<span class="math inline">\(\lambda\)</span>下观测序列<span class="math inline">\(O\)</span>出现的概率<span class="math inline">\(P(O|\lambda)\)</span></p>
<p>在这里有两种方法：<strong>直接计算法</strong>和<strong>前向（或者后向算法）</strong></p>
<p><strong>直接计算法：</strong></p>
<p>对于状态序列<span class="math inline">\(I=(i_1, i_2, ..., iT)\)</span>的概率是： <span class="math display">\[
P(I|\lambda) = \pi_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_T-1i_T}
\]</span> 对上面这种状态序列，产生观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>的概率是： <span class="math display">\[
P(O|I, \lambda) = b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)
\]</span> <span class="math inline">\(I\)</span>和<span class="math inline">\(O\)</span>的联合概率为： <span class="math display">\[
P(O, I|\lambda) =P(O|I, \lambda) \cdot P(I|\lambda)  = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)
\]</span> 对所有可能的<span class="math inline">\(I\)</span>进行求和，得到： <span class="math display">\[
P(O|\lambda) = \sum_I P(O, I|\lambda) =\sum_{i_1, ..., i_T} \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)
\]</span> 如果直接计算，时间负责度太高，是<span class="math inline">\(O(TN^T)\)</span>。</p>
<p><strong><u>前向算法</u></strong>：</p>
<p>首先引入<strong>前向概率</strong>：</p>
<p>给定模型<span class="math inline">\(\lambda\)</span>，定义到时刻<span class="math inline">\(t\)</span>部分观测序列为<span class="math inline">\(o_1, o_2, ..., o_t\)</span>且状态为<span class="math inline">\(q_i\)</span>的概率为前向概率。记作： <span class="math display">\[
\alpha_t(i) = P(o_1, o_2, ..., o_t, i_t = q_t | \lambda)
\]</span></p>
<p>用感冒例子描述，就是某一天是否感冒以及这天<span class="math inline">\(t\)</span>及这天之前<span class="math inline">\((1—t)\)</span>所有观测数据的联合概率。</p>
<p><strong>观测序列概率的前向算法</strong></p>
<p><strong>输入：</strong>隐马尔科夫模型<span class="math inline">\(\lambda\)</span>，观测序列<span class="math inline">\(O\)</span>；</p>
<p><strong>输出：</strong>观测序列概率<span class="math inline">\(P(O|\lambda)\)</span></p>
<p>（1）初值 <span class="math display">\[
\alpha_i(i) = \pi_ib_i(o_i)
\]</span> 前向概率的定义中一共限定了两个条件，一是到当前为止的观测序列，另外一个是当前的状态。所以初值的计算也有两项（观测和状态），一项是初始状态概率，另一项是发射到当前观测的概率。</p>
<p>（2）递推 对<span class="math inline">\(t=1, 2, .., T-1\)</span> <span class="math display">\[
\alpha_{t+1}(i) =     \begin{bmatrix}
 \sum_{j=1}^N \alpha_t(j)a_{ji}
    \end{bmatrix} b_i(o_{i+1}), \ \ \ \ \ i = 1, 2, ... ,N
\]</span> 每次递推同样由两部分构成，大括号中是当前状态为<span class="math inline">\(i\)</span>且观测序列的前<span class="math inline">\(t\)</span>个符合要求的概率，括号外的是状态<span class="math inline">\(j\)</span>发射到观测<span class="math inline">\(t+1\)</span>的概率。</p>
<p>（3）终止 <span class="math display">\[
P(O|\lambda) = \sum_{i=1}^{N} \alpha_T(i)
\]</span></p>
<p>由于到了时间<span class="math inline">\(T\)</span>，一共有N中状态发射了最后的那个预测，所以最终的结果要将这些概率加起来。</p>
<p><strong>前向算法理解：</strong></p>
<p>前向算法使用前向概率的概念，记录每个时间下的前向概率，使得在递推计算下一个前向概率时，只需要上一个时间点的所有前向概率。减少计算量的原因在于每一次计算直接引用前一个时刻的计算机过，避免重复计算。原理上是用空间换时间。时间复杂度是<span class="math inline">\(O(N^2T)\)</span>。</p>
<p>下面我们来看代码的实现。</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">def</span> forward(obs_seq):</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;前向算法&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">    N <span class="op">=</span> A.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb4-4" data-line-number="4">    T <span class="op">=</span> <span class="bu">len</span>(obs_seq)</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">    </a>
<a class="sourceLine" id="cb4-6" data-line-number="6">    <span class="co"># F保存前向概率矩阵</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">    F <span class="op">=</span> np.zeros((N,T))</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">    F[:,<span class="dv">0</span>] <span class="op">=</span> pi <span class="op">*</span> B[:, obs_seq[<span class="dv">0</span>]]</a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</a>
<a class="sourceLine" id="cb4-11" data-line-number="11">        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(N):</a>
<a class="sourceLine" id="cb4-12" data-line-number="12">            F[n,t] <span class="op">=</span> np.dot(F[:,t<span class="dv">-1</span>], (A[:,n])) <span class="op">*</span> B[n, obs_seq[t]]</a>
<a class="sourceLine" id="cb4-13" data-line-number="13"></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">    <span class="cf">return</span> F</a></code></pre></div>
<p><strong><u>后向算法</u></strong></p>
<p>给定隐马尔科夫模型<span class="math inline">\(\lambda\)</span>，定义在时刻<span class="math inline">\(t\)</span>，状态为<span class="math inline">\(q_i\)</span>的条件下，从<span class="math inline">\(t+1\)</span>到到<span class="math inline">\(T\)</span>的部分观测序列为<span class="math inline">\(o_{t+1}, o_{t+2}, ..., o_T\)</span>的概率为后向概率，记作 <span class="math display">\[
\beta_t(i) = P(o_{t+1}, o_{t+2}, ..., o_T|i_t = q_i, \lambda)
\]</span></p>
<p>可以用递推的方法求得后向概率<span class="math inline">\(\beta_t(i)\)</span>及观测序列概率<span class="math inline">\(P(O|\lambda)\)</span>。</p>
<p><strong>观测序列概率的后向算法</strong></p>
<p><strong>输入</strong>：隐马尔科夫模型<span class="math inline">\(\lambda\)</span>，观测序列<span class="math inline">\(O\)</span>；</p>
<p><strong>输出：</strong>观测序列概率<span class="math inline">\(P(O|\lambda)\)</span></p>
<p>（1）初值 <span class="math display">\[
\beta_T(i) = 1, \ \ \ i = 1, 2, ..., N
\]</span> 根据定义，从<span class="math inline">\(T+1\)</span>到<span class="math inline">\(T\)</span>的部分观测序列其实不存在，所以硬性规定这个值是1。</p>
<p>（2）对<span class="math inline">\(t=T-1, T-2, ..., 1\)</span> <span class="math display">\[
\beta_t(i) = \sum_{j=1}^{N} a_{ij}b_j(o_{i+1})\beta_{t+1}(j), \ \ \ i = 1, 2, ..., N
\]</span> <span class="math inline">\(a_{ij}\)</span>表示状态<span class="math inline">\(i\)</span>转移到状态<span class="math inline">\(j\)</span>的概率，<span class="math inline">\(b_j\)</span>表示发射<span class="math inline">\(o_{i+1}\)</span>的观测概率，<span class="math inline">\(\beta_{t+1}(j)\)</span>表示<span class="math inline">\(j\)</span>后面的序列对应的后向概率。</p>
<p>（3） <span class="math display">\[
P(O|\lambda) = \sum_{i=1}^{N}\pi_ib_i(o_1)\beta_i(i)
\]</span> 最后求和是因为，在第一个时间点上有<span class="math inline">\(N\)</span>中后向概率都能输出从2到<span class="math inline">\(T\)</span>的观察序列，所以乘上输出<span class="math inline">\(O_1\)</span>的概率后求和得到最终结果。</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">def</span> backward(obs_seq):</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;后向算法&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3">    N <span class="op">=</span> A.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb5-4" data-line-number="4">    T <span class="op">=</span> <span class="bu">len</span>(obs_seq)</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">    <span class="co"># X保存后向概率矩阵</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">    X <span class="op">=</span> np.zeros((N, T))</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">    X[:, <span class="dv">-1</span>:] <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb5-8" data-line-number="8">    </a>
<a class="sourceLine" id="cb5-9" data-line-number="9">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(T<span class="dv">-1</span>)):</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(N):</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">            X[n, t] <span class="op">=</span> np.<span class="bu">sum</span>(X[:, t<span class="op">+</span><span class="dv">1</span>] <span class="op">*</span> A[n, :] <span class="op">*</span> B[:, obs_seq[t<span class="op">+</span><span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">            </a>
<a class="sourceLine" id="cb5-13" data-line-number="13">    <span class="cf">return</span> X</a></code></pre></div>
<p>利用前向概率和后向概率的定义可以将观测序列概率<span class="math inline">\(P(O|\lambda)\)</span>统一写成： <span class="math display">\[
P(O|\lambda) = \sum_{i=1}^N\sum_{j=1}^N \alpha_t(i)a_{ij}b_j o_{t+1}\beta_{t+1}(j), \ \ \ t = 1, 2, ..., T -1
\]</span></p>
<h4 id="学习问题"><strong>2.2学习问题</strong></h4>
<p>关于学习问题就是，<strong>已知观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，估计模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>，使<span class="math inline">\(P(O|\lambda)\)</span>最大。即用</strong>极大似然估计的方法估计参数。</p>
<p>隐马尔科夫模型的学习，分为监督学习与无监督学习：</p>
<ul>
<li>监督学习：训练数据包括观测序列和对应的状态序列。</li>
<li>无监督学习：训练数据只有观测序列。</li>
</ul>
<p><strong>监督学习方法</strong></p>
<p>假设已给训练数据包含<span class="math inline">\(S\)</span>个长度相同的观测序列和对应的状态序列<span class="math inline">\({(O_1, I_1), (O_2, I_2), ..., (O_S, I_S)}\)</span>，那么可以利用极大似然估计法来估计隐马尔科夫模型的参数。具体方法如下：</p>
<p><strong>1.转移概率<span class="math inline">\(a_{ij}\)</span>的估计</strong></p>
<p>设样本中时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(i\)</span>时刻<span class="math inline">\(t+1\)</span>转移到状态<span class="math inline">\(j\)</span>的频数为<span class="math inline">\(A_{ij}\)</span>，则状态转移概率<span class="math inline">\(a_{ij}\)</span>的估计是 <span class="math display">\[
\hat a_{ij} = \frac{A_{ij}}{\sum_{i=1}^NA_{ij}} \ \ \ i =1 , 2, ..., N , \ \ \ j = 1, 2, ..., N
\]</span></p>
<p><strong>2.观测概率<span class="math inline">\(b_j(k)\)</span>的估计</strong></p>
<p>设样本中状态为<span class="math inline">\(j\)</span>并观测为<span class="math inline">\(k\)</span>的频数为<span class="math inline">\(B_{jk}\)</span>，那么状态为<span class="math inline">\(j\)</span>观测为<span class="math inline">\(k\)</span>的概率估计是 <span class="math display">\[
\hat b_j(k) = \frac{B_{jk}}{\sum_{k=1}^MB_{jk}}
\]</span></p>
<p><strong>3.初始状态概率<span class="math inline">\(\pi_i\)</span>的估计<span class="math inline">\(\hat \pi_i\)</span>为<span class="math inline">\(S\)</span>个样本中初始状态为<span class="math inline">\(q_i\)</span>的频率</strong></p>
<p>由于监督学习需要使用训练数据，而人工标注训练数据往往代价很高，有时就会利用非监督学习。</p>
<p><strong>Baum-Welch算法</strong></p>
<p>假设给定训练数据只包含<span class="math inline">\(S\)</span>个长度为<span class="math inline">\(T\)</span>的观测序列<span class="math inline">\({O_1, O_2, ..., O_S}\)</span>而没有对应的状态序列，目标是学习隐马尔科夫模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>的参数。我们将观测序列数据看作观测数据<span class="math inline">\(O\)</span>，状态序列数据看作不可观测的隐数据<span class="math inline">\(I\)</span>，则隐马尔科夫模型就能够抽象成一个含有隐变量的概率模型 <span class="math display">\[
P(O|\lambda) = \sum_t P(O|I, \lambda) P (I|\lambda)
\]</span> 它的参数学习可以由<code>EM算法</code>实现。</p>
<p><strong>1.确定完全数据的对数似然函数</strong></p>
<p>所有观测数据写成<span class="math inline">\(O = (o_1, o_2, ..., o_T)\)</span>，所有隐数据写成<span class="math inline">\(I=(i_1, i_2, ..., i_T)\)</span>，完全数据是<span class="math inline">\((O, I) = (o_1, o_2, ..., o_T, i_1, i_2, ..., i_T)\)</span>。完全数据的对数似然函数是<span class="math inline">\(logP(O,I|\lambda)\)</span>。</p>
<p><strong>2.EM算法的E步：求Q函数</strong><span class="math inline">\(Q(\lambda, \bar \lambda)\)</span> <span class="math display">\[
Q(\lambda, \bar \lambda) = \sum_I logP(O, I|\lambda) P(O, I | \bar \lambda)
\]</span></p>
<p>其中，<span class="math inline">\(\bar \lambda\)</span>是隐马尔科夫模型参数的当前估计值，<span class="math inline">\(\lambda\)</span>是要极大化的隐马尔科夫模型参数。而 <span class="math display">\[
P(O,I|\lambda) = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_T-1i_T}b_T(o_T)
\]</span> 于是函数<span class="math inline">\(Q(\lambda, \bar \lambda)\)</span>可以写成： <span class="math display">\[
\begin{aligned}
Q(\lambda, \bar \lambda)  &amp;= \sum_Ilog\pi_{i_1}P(O, I | \bar \lambda) \\\\
&amp; +\sum_I\Biggl(\sum_{t=1}^{T-1}loga_{i_1i_{i+1}}\Biggr)P(O,I|
\bar \lambda)  + \sum_I\Biggl(\sum_{t=1}^{T}logb_{i_1}(o_t)\Biggr) P (O,I|\bar \lambda)
\end{aligned}
\]</span> <strong>3.EM算法的M步：极大化<span class="math inline">\(Q\)</span>函数<span class="math inline">\(Q(\lambda, \bar \lambda)\)</span>求模型参数<span class="math inline">\(A, B, \pi\)</span>。</strong></p>
<p>由于要极大化的参数在Q函数表达式中单独出现在3个项中，所以只需对各项分别极大化。</p>
<p><strong>第1项可以写成：</strong> <span class="math display">\[
\sum_Ilog\pi_{i_0}P(O, I | \bar \lambda) = \sum_{i=1}^Nlog\pi_iP(O, i_1= i | \lambda)
\]</span></p>
<p>注意到<span class="math inline">\(\pi_i\)</span>满足约束条件<span class="math inline">\(\sum_{i=1}^{N}\pi_i=1\)</span>，利用拉格朗日乘子法，写出拉格朗日函数： <span class="math display">\[
\sum_{i=1}^N log\pi_i P(O, i_1 = i | \bar \lambda) + \gamma\biggl(\sum_{i=1}^N\pi_i - 1\biggr)
\]</span></p>
<p>对其求偏导并令结果为0 <span class="math display">\[
\frac{\partial}{\partial \pi_i}[\sum_{i=1}^N log\pi_i P(O, i_1 = i | \bar \lambda) + \gamma\biggl(\sum_{i=1}^N\pi_i - 1\biggr)] = 0
\]</span></p>
<p>得到 <span class="math display">\[
P(O, i_1 = 1| \bar \lambda)  + \gamma\pi_i = 0
\]</span> 对<span class="math inline">\(i\)</span>求和得到<span class="math inline">\(\gamma\)</span> <span class="math display">\[
\gamma = - P(O|\bar \lambda)
\]</span></p>
<p>代入<span class="math inline">\(P(O, i_1 = 1| \bar \lambda) + \gamma\pi_i = 0\)</span>中得到 <span class="math display">\[
\pi_i = \frac{P(O, i_1 = i|\bar \lambda)}{P(O|\bar \lambda)}
\]</span></p>
<p><strong>第2项可以写成</strong> <span class="math display">\[
\sum_I\Biggl(\sum_{t=1}^{T-1}loga_{i_1i_{i+1}}\Biggr)P(O,I|
\bar \lambda) = \sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}log a_{ij}P(O, i_t = i, i_{t+1} = j \ |\  \bar \lambda)
\]</span></p>
<p>应用约束条件<span class="math inline">\(\sum_{j=1}^{N}a_{ij} =1\)</span>，利用拉格朗日乘子法，可以求出 <span class="math display">\[
a_{ij} = \frac{\sum_{t=1}^{T-1} P (O, i_t = 1, i_{t+1} = j \ | \ \bar \lambda)}{\sum^{T-1}_{t=1}P(O, i_t=i \ | \ \bar \lambda)}
\]</span></p>
<p><strong>第3项可以写成</strong> <span class="math display">\[
\sum_I\Biggl(\sum_{t=1}^{T}logb_{i_1}(o_t)\Biggr) P (O,I|\bar \lambda)  = \sum_{j=1}^{N}\sum_{t=1}^Tlogb_j(o_t) P(O, i_t = j \ | \ \bar \lambda )
\]</span> 应用约束条件<span class="math inline">\(\sum_{k=1}^{N}b_{j}(k) =1\)</span>，利用拉格朗日乘子法（注意，只有在<span class="math inline">\(o_t = v_k\)</span>时<span class="math inline">\(b_j(o_t)\)</span>对<span class="math inline">\(b_j(k)\)</span>的偏导数才不为0，以<span class="math inline">\(I(o_t = v_k)表示\)</span>），求出 <span class="math display">\[
b_j(k) = \frac{\sum_{t=1}^{T}P(O, i_t = j \ | \ \bar \lambda) I (o_t = v_k)}{\sum_{t=1}^{T} P (O, i_t = j \ | \ \bar \lambda)}
\]</span> <strong>Baum-Welch模型参数估计公式</strong></p>
<p>将这三个式子中的各概率分别简写如下： <span class="math display">\[
a_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1}\gamma_t(i)}
\]</span></p>
<p><span class="math display">\[
b_j(k) = \frac{\sum_{t=1,o_t=v_k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}
\]</span></p>
<p><span class="math display">\[
\pi_i = \gamma_1(i)
\]</span></p>
<p>我们定义一个概率<span class="math inline">\(\gamma\)</span>， 它表示，给定模型参数和所有预测，时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_i\)</span>的概率。 <span class="math display">\[
\begin{aligned}
\gamma_t(i) &amp;= P(i_t = q_i | O, \lambda) \\\\
&amp; = \frac{P(i_t=q_i, O|\lambda)}{P(O|\lambda)}\\\\
&amp; =\frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^{N}\alpha_t(j)\beta_t(j)}
\end{aligned}
\]</span> 我们定义一个概率<span class="math inline">\(\xi\)</span>，它表示，给定模型参数和所有观测，时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_i\)</span>且时刻<span class="math inline">\(t+1\)</span>处于状态<span class="math inline">\(q_j\)</span>的概率。 <span class="math display">\[
\begin{aligned}
\xi_t(i, j) &amp;= P(i_t = q_i, i_{t+1} =q_j | O, \lambda) \\\\
&amp; =\frac{P(i_t = q_i, i_{t+1} = q_j , O \ | \ \lambda) }{P(O|\lambda)} \\\\
&amp; = \frac{P(i_t = q_i , i_{t+1} = q_j, O | \lambda)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(i_t = q_i, i_{t+1} = q_j, O \ | \ \lambda)}  \\\\
&amp; = \frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(i_t = q_i, i_{t+1} = q_j, O \ | \ \lambda)}
\end{aligned}
\]</span> 然后我们来看看<strong>Baum-Welch</strong>算法的Python实现</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">def</span> baum_welch_train(observations, A, B, pi, criterion<span class="op">=</span><span class="fl">0.05</span>):</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;无监督学习算法——Baum-Weich算法&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">    n_states <span class="op">=</span> A.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb6-4" data-line-number="4">    n_samples <span class="op">=</span> <span class="bu">len</span>(observations)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5"></a>
<a class="sourceLine" id="cb6-6" data-line-number="6">    done <span class="op">=</span> <span class="va">False</span></a>
<a class="sourceLine" id="cb6-7" data-line-number="7">    <span class="cf">while</span> <span class="kw">not</span> done:</a>
<a class="sourceLine" id="cb6-8" data-line-number="8">        <span class="co"># alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)</span></a>
<a class="sourceLine" id="cb6-9" data-line-number="9">        <span class="co"># Initialize alpha</span></a>
<a class="sourceLine" id="cb6-10" data-line-number="10">        alpha <span class="op">=</span> forward(observations)</a>
<a class="sourceLine" id="cb6-11" data-line-number="11"></a>
<a class="sourceLine" id="cb6-12" data-line-number="12">        <span class="co"># beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)</span></a>
<a class="sourceLine" id="cb6-13" data-line-number="13">        <span class="co"># Initialize beta</span></a>
<a class="sourceLine" id="cb6-14" data-line-number="14">        beta <span class="op">=</span> backward(observations)</a>
<a class="sourceLine" id="cb6-15" data-line-number="15">        <span class="co"># ξ_t(i,j)=P(i_t=q_i,i_{i+1}=q_j|O,λ)</span></a>
<a class="sourceLine" id="cb6-16" data-line-number="16">        xi <span class="op">=</span> np.zeros((n_states,n_states,n_samples<span class="dv">-1</span>))</a>
<a class="sourceLine" id="cb6-17" data-line-number="17">        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(n_samples<span class="dv">-1</span>):</a>
<a class="sourceLine" id="cb6-18" data-line-number="18">            denom <span class="op">=</span> np.dot(np.dot(alpha[:,t].T, A) <span class="op">*</span> B[:,observations[t<span class="op">+</span><span class="dv">1</span>]].T, beta[:,t<span class="op">+</span><span class="dv">1</span>])</a>
<a class="sourceLine" id="cb6-19" data-line-number="19">            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_states):</a>
<a class="sourceLine" id="cb6-20" data-line-number="20">                numer <span class="op">=</span> alpha[i,t] <span class="op">*</span> A[i,:] <span class="op">*</span> B[:,observations[t<span class="op">+</span><span class="dv">1</span>]].T <span class="op">*</span> beta[:,t<span class="op">+</span><span class="dv">1</span>].T</a>
<a class="sourceLine" id="cb6-21" data-line-number="21">                xi[i,:,t] <span class="op">=</span> numer <span class="op">/</span> denom</a>
<a class="sourceLine" id="cb6-22" data-line-number="22"></a>
<a class="sourceLine" id="cb6-23" data-line-number="23">        <span class="co"># γ_t(i)：gamma_t(i) = P(q_t = S_i | O, hmm)</span></a>
<a class="sourceLine" id="cb6-24" data-line-number="24">        gamma <span class="op">=</span> np.<span class="bu">sum</span>(xi,axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb6-25" data-line-number="25">        <span class="co"># Need final gamma element for new B</span></a>
<a class="sourceLine" id="cb6-26" data-line-number="26">        <span class="co"># xi的第三维长度n_samples-1，少一个，所以gamma要计算最后一个</span></a>
<a class="sourceLine" id="cb6-27" data-line-number="27">        prod <span class="op">=</span>  (alpha[:,n_samples<span class="dv">-1</span>] <span class="op">*</span> beta[:,n_samples<span class="dv">-1</span>]).reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb6-28" data-line-number="28">        gamma <span class="op">=</span> np.hstack((gamma,  prod <span class="op">/</span> np.<span class="bu">sum</span>(prod))) <span class="co">#append one more to gamma!!!</span></a>
<a class="sourceLine" id="cb6-29" data-line-number="29">        </a>
<a class="sourceLine" id="cb6-30" data-line-number="30">        <span class="co"># 更新模型参数</span></a>
<a class="sourceLine" id="cb6-31" data-line-number="31">        newpi <span class="op">=</span> gamma[:,<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb6-32" data-line-number="32">        newA <span class="op">=</span> np.<span class="bu">sum</span>(xi,<span class="dv">2</span>) <span class="op">/</span> np.<span class="bu">sum</span>(gamma[:,:<span class="op">-</span><span class="dv">1</span>],axis<span class="op">=</span><span class="dv">1</span>).reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb6-33" data-line-number="33">        newB <span class="op">=</span> np.copy(B)</a>
<a class="sourceLine" id="cb6-34" data-line-number="34">        num_levels <span class="op">=</span> B.shape[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb6-35" data-line-number="35">        sumgamma <span class="op">=</span> np.<span class="bu">sum</span>(gamma,axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb6-36" data-line-number="36">        <span class="cf">for</span> lev <span class="kw">in</span> <span class="bu">range</span>(num_levels):</a>
<a class="sourceLine" id="cb6-37" data-line-number="37">            mask <span class="op">=</span> observations <span class="op">==</span> lev</a>
<a class="sourceLine" id="cb6-38" data-line-number="38">            newB[:,lev] <span class="op">=</span> np.<span class="bu">sum</span>(gamma[:,mask],axis<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> sumgamma</a>
<a class="sourceLine" id="cb6-39" data-line-number="39">        </a>
<a class="sourceLine" id="cb6-40" data-line-number="40">        <span class="co"># 检查是否满足阈值</span></a>
<a class="sourceLine" id="cb6-41" data-line-number="41">        <span class="cf">if</span> np.<span class="bu">max</span>(<span class="bu">abs</span>(pi <span class="op">-</span> newpi)) <span class="op">&lt;</span> criterion <span class="kw">and</span> <span class="op">\</span></a>
<a class="sourceLine" id="cb6-42" data-line-number="42">                        np.<span class="bu">max</span>(<span class="bu">abs</span>(A <span class="op">-</span> newA)) <span class="op">&lt;</span> criterion <span class="kw">and</span> <span class="op">\</span></a>
<a class="sourceLine" id="cb6-43" data-line-number="43">                        np.<span class="bu">max</span>(<span class="bu">abs</span>(B <span class="op">-</span> newB)) <span class="op">&lt;</span> criterion:</a>
<a class="sourceLine" id="cb6-44" data-line-number="44">            done <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb6-45" data-line-number="45">        A[:], B[:], pi[:] <span class="op">=</span> newA, newB, newpi</a>
<a class="sourceLine" id="cb6-46" data-line-number="46">    <span class="cf">return</span> newA, newB, newpi</a></code></pre></div>
<p>回到预测感冒的问题，下面我们先自己建立一个HMM模型，再模拟出一个观测序列和一个状态序列。</p>
<p>然后，只用观测序列去学习模型，获得模型参数。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1">A <span class="op">=</span> np.array([[<span class="fl">0.5</span>, <span class="fl">0.5</span>],[<span class="fl">0.5</span>, <span class="fl">0.5</span>]])</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">B <span class="op">=</span> np.array([[<span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>],[<span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>]])</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">pi <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="fl">0.5</span>])</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"></a>
<a class="sourceLine" id="cb7-5" data-line-number="5">observations_data, states_data <span class="op">=</span> simulate(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">newA, newB, newpi <span class="op">=</span> baum_welch_train(observations_data, A, B, pi)</a>
<a class="sourceLine" id="cb7-7" data-line-number="7"><span class="bu">print</span>(<span class="st">&quot;newA: &quot;</span>, newA)</a>
<a class="sourceLine" id="cb7-8" data-line-number="8"><span class="bu">print</span>(<span class="st">&quot;newB: &quot;</span>, newB)</a>
<a class="sourceLine" id="cb7-9" data-line-number="9"><span class="bu">print</span>(<span class="st">&quot;newpi: &quot;</span>, newpi)</a>
<a class="sourceLine" id="cb7-10" data-line-number="10">[output]</a>
<a class="sourceLine" id="cb7-11" data-line-number="11">newA:  [[<span class="fl">0.5</span> <span class="fl">0.5</span>]</a>
<a class="sourceLine" id="cb7-12" data-line-number="12"> [<span class="fl">0.5</span> <span class="fl">0.5</span>]]</a>
<a class="sourceLine" id="cb7-13" data-line-number="13">newB:  [[<span class="fl">0.26</span> <span class="fl">0.38</span> <span class="fl">0.36</span>]</a>
<a class="sourceLine" id="cb7-14" data-line-number="14"> [<span class="fl">0.26</span> <span class="fl">0.38</span> <span class="fl">0.36</span>]]</a>
<a class="sourceLine" id="cb7-15" data-line-number="15">newpi:  [<span class="fl">0.5</span> <span class="fl">0.5</span>]</a></code></pre></div>
<h4 id="预测问题"><strong>2.3预测问题</strong></h4>
<p>考虑到预测问题是求给定预测序列条件概率<span class="math inline">\(P(I|O)\)</span>最大的状态序列<span class="math inline">\(I = (i_1, i_2, ..., i_T)\)</span>，类比这个问题和最短路径问题：</p>
<p>我们可以把求<span class="math inline">\(P(I|O)\)</span>的最大值类比成求节点间距离的最小值，于是考虑<strong>类似于动态规划的viterbi算法</strong>。</p>
<p>首先导入两个变量$<span class="math inline">\(和\)</span>$</p>
<p>定义<strong>在时刻<span class="math inline">\(t\)</span>状态为<span class="math inline">\(i\)</span>的所有单个路径</strong><span class="math inline">\((i_1, i_2, ..., i_t)\)</span>中概率值最大为 <span class="math display">\[
\delta_t(i) = max_{i_1, i_2, ..., i_{t-1}} P(i_t= i , i_{t-1} , ...,  i_1, o_t, ..., o_1 | \lambda) \ \ \ i = 1, 2, ..., N
\]</span> 根据定义，可得递推公式 <span class="math display">\[
\begin{aligned}
\delta_{t+1}(i) &amp;= max_{i_1, i_2, ..., i_t} P(i_{t+1}= i , i_{t} , ...,  i_1, o_t, ..., o_1 | \lambda) \\\\
&amp; = max_{1\le j \le N}[\delta_{t}(j)a_{ji}]b_i(o_{t+1})
\end{aligned}
\]</span> 定义<strong>在时刻<span class="math inline">\(t\)</span>状态为<span class="math inline">\(i\)</span>的所有单个路径</strong><span class="math inline">\((i_1, i_2, ..., i_t)\)</span>中概率值最大的路径的第<span class="math inline">\(t-1\)</span>个结点为 <span class="math display">\[
\psi_t(i)  =argmax_{\ 1 \le j \le N} [\delta_{t-1}(j)a_{ji}] \ \ \ \ i = 1, 2, ..., N
\]</span> <strong><u>维特比(viterbi)算法(动态规划）</u></strong></p>
<p><strong>输入：</strong>模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>和观测<span class="math inline">\(O = (o_1, o_2, ..., o_T)\)</span></p>
<p><strong>输出</strong>：最优路径<span class="math inline">\(I* = (i^*_i, i^*_2, ..., i^*_T)\)</span></p>
<p>（1）初始化 <span class="math display">\[
\delta_1(i) = \pi_ib_i(o_1)
\]</span></p>
<p><span class="math display">\[
\psi_1(i) = 0
\]</span></p>
<p>（2）递推. 对<span class="math inline">\(t=2, 3, ..., T\)</span> <span class="math display">\[
\delta_{t}(i) = max_{1\le j \le N}[\delta_{t-1}(j)a_{ji}]b_i(o_{t})  \ \ \ \ i = 1, 2, ..., N
\]</span></p>
<p><span class="math display">\[
\psi_t(i)  =argmax_{1 \le j \le N} [\delta_{t-1}(j)a_{ji}] \ \ \ \ i = 1, 2, ..., N
\]</span></p>
<p>（3）终止 <span class="math display">\[
P^* = max_{1 \le i \le N } \delta _T(i)
\]</span></p>
<p><span class="math display">\[
i^*_T = argmax_{1 \le i \le N}[\delta_T(i)]
\]</span></p>
<p>（4）最优路径回溯， 对<span class="math inline">\(t=T-1, T-2, ..., 1\)</span> <span class="math display">\[
i^*_t = \psi_{t+1} (i^*_{t+1})
\]</span></p>
<p>求得最优路径<span class="math inline">\(I* = (i^*_1, i^*_2, ..., i^*_T)\)</span></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"># python</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/16/EM-Algorithm/" rel="next" title="EM Algorithm">
                <i class="fa fa-chevron-left"></i> EM Algorithm
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/pandas.jpeg"
               alt="Peter Tsung" />
          <p class="site-author-name" itemprop="name">Peter Tsung</p>
           
              <p class="site-description motion-element" itemprop="description">I Never Save Anything For The Swim Back.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">38</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#概率计算问题"><span class="nav-number">1.</span> <span class="nav-text">2.1概率计算问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#学习问题"><span class="nav-number">2.</span> <span class="nav-text">2.2学习问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#预测问题"><span class="nav-number">3.</span> <span class="nav-text">2.3预测问题</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Peter Tsung</span>
</div>

<!--
<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
                    TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
                            messageStyle: "none"
                                }); 
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Queue(function() {
                    var all = MathJax.Hub.getAllJax(), i;
                            for(i=0; i < all.length; i += 1) {
                                            all[i].SourceElement().parentNode.className += ' has-jax';
                                                    }
                                                        });
        </script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

-->


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  

  

  

  


  <script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
  <script type="text/javascript">
  $(window).load(function(){
     $('pre').addClass('prettyprint linenums').attr('style', 'overflow:auto;');
     prettyPrint();
   })    
  </script>
</body>
</html>
