<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Deep Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="大概是去年八月份的时候，我第一次接触到语言模型。那个时候尝试了N-gram，但是发现N-gram在句子语义捕捉和泛化的表现都很有限，因此当时慢慢接触到LSTM，简直有种相见恨晚的感觉。 为了更好地介绍LSTM，我觉得结合语言模型(Language Model)这个实际案例去分析会更好。 一、语言模型  语言模型（LM）是自然语言处理（NLP）中最基础且重要的组成部分，它的核心任务就是在给定先前单">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Long Short-term Memory">
<meta property="og:url" content="http://yoursite.com/2019/05/04/Long-Short-term-Memory/index.html">
<meta property="og:site_name" content="彼得的博客">
<meta property="og:description" content="大概是去年八月份的时候，我第一次接触到语言模型。那个时候尝试了N-gram，但是发现N-gram在句子语义捕捉和泛化的表现都很有限，因此当时慢慢接触到LSTM，简直有种相见恨晚的感觉。 为了更好地介绍LSTM，我觉得结合语言模型(Language Model)这个实际案例去分析会更好。 一、语言模型  语言模型（LM）是自然语言处理（NLP）中最基础且重要的组成部分，它的核心任务就是在给定先前单">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/images/LSTM.png">
<meta property="og:image" content="http://yoursite.com/images/the-count.jpg">
<meta property="og:image" content="http://yoursite.com/images/220px-Pierre-Simon_Laplace.jpg">
<meta property="og:image" content="http://yoursite.com/images/tears.png">
<meta property="og:image" content="http://yoursite.com/images/RNN_fold.png">
<meta property="og:image" content="http://yoursite.com/images/RNN_unfold.png">
<meta property="og:image" content="http://yoursite.com/images/BPTT.png">
<meta property="og:updated_time" content="2019-05-09T14:07:17.248Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Long Short-term Memory">
<meta name="twitter:description" content="大概是去年八月份的时候，我第一次接触到语言模型。那个时候尝试了N-gram，但是发现N-gram在句子语义捕捉和泛化的表现都很有限，因此当时慢慢接触到LSTM，简直有种相见恨晚的感觉。 为了更好地介绍LSTM，我觉得结合语言模型(Language Model)这个实际案例去分析会更好。 一、语言模型  语言模型（LM）是自然语言处理（NLP）中最基础且重要的组成部分，它的核心任务就是在给定先前单">
<meta name="twitter:image" content="http://yoursite.com/images/LSTM.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/05/04/Long-Short-term-Memory/"/>




<link href="/js/google-code-prettify/prettify.css" type="text/css" rel="stylesheet" />
<link href="/js/google-code-prettify/github-v2.min.css" type="text/css" rel="stylesheet" />

  <title>Long Short-term Memory | 彼得的博客</title>
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">彼得的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/04/Long-Short-term-Memory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Long Short-term Memory
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-04T23:21:46+08:00">
                2019-05-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep_Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="/images/LSTM.png"></p>
<p>大概是去年八月份的时候，我第一次接触到语言模型。那个时候尝试了<code>N-gram</code>，但是发现<code>N-gram</code>在句子语义捕捉和泛化的表现都很有限，因此当时慢慢接触到<code>LSTM</code>，简直有种相见恨晚的感觉。</p>
<p>为了更好地介绍<code>LSTM</code>，我觉得结合<code>语言模型(Language Model)</code>这个实际案例去分析会更好。</p>
<p><strong>一、语言模型</strong></p>
<p><img src="/images/the-count.jpg"></p>
<p><b>语言模型（LM）</b>是自然语言处理（NLP）中最基础且重要的组成部分，它的核心任务就是在给定先前单词的情况下预测下一个单词出现的概率。正因为语言模型能够在单词或者句子的句法和语义关系中有一个很好的捕捉，因此在语音识别、机器翻译、分词、文本生成和句法分析中都有广泛的应用。</p>
<p>在具体应用中，例如拼音输入法，就有提供快速文本输入的预测能力。下面我们利用语言模型判断给定的句子是否通顺。而无论是基于<code>n-gram</code>还是基于<code>LSTM</code>，语言模型核心的公式都是以下这个： <span class="math display">\[
\begin{aligned}
P(w_1 \ldots w_n) &amp;= P(w_1) P(w_2|w_1) P(w_3|w_1 w_2) \ldots P(w_n|w_1 \ldots w_{n-1}) \\\\
&amp;= \prod_{i} P(w_i|w_1w_2 \ldots w_{i-1})
\end{aligned}
\]</span> <strong>1.1 Statistic Language Model</strong></p>
<p>首先我们来看一下<code>n-gram</code>的数学公式： <span class="math display">\[
P(w_1 \dots w_m) = \prod_{i}^{m} P(w_i|w_1w_2 \dots w_{i-1}) \approx \prod_{i}^{m} P(w_i | w_{i-(n-1)} \dots w_{i-1})
\]</span></p>
<p><span class="math display">\[
P(w_i|w_{i-(n-1)} \dots w_{i-1}) = P(w_{i-(n-1)}, w_{i-1}, w_i)\ / \ P(w_{i-(n-1)}, \dots, w_{i-1})
\]</span></p>
<p>如果<span class="math inline">\(n=2\)</span>， 这种情况叫做<span class="math inline">\(bigram\)</span>，则<span class="math inline">\(P(w_n | w_{n-1}) = P(w_{n-1}w_n)\ / \ P(w_{n-1})\)</span></p>
<p>例如，</p>
<p>$ P(我,想,吃,番茄,鸡蛋,面) P(我|BOS)P(想|我)P(吃|想)P(番茄|吃)P(鸡蛋|番茄)P(面|鸡蛋)P(EOS|面) $</p>
<p>那么其实我们可以有很多中方法来实现<code>n-gram</code>模型。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># 首先要做一些无聊的初步行动。。</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> re</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">from</span> collections <span class="im">import</span> Counter</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer </a>
<a class="sourceLine" id="cb1-5" data-line-number="5"></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">TEXT <span class="op">=</span> <span class="bu">open</span>(<span class="st">&#39;data/big.txt&#39;</span>).read()</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co"># 一、统计n个单词连续出现的次数</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co"># 可以用pythonic的方法</span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="kw">def</span> ngrams_pythonic(input_list, n):</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">    <span class="co">&quot;统计文本中n个单词连续出现的次数&quot;</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11">    <span class="cf">return</span> <span class="bu">zip</span>(<span class="op">*</span>[input_list[i:] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)])</a>
<a class="sourceLine" id="cb1-12" data-line-number="12"></a>
<a class="sourceLine" id="cb1-13" data-line-number="13">text_split <span class="op">=</span> TEXT.split(<span class="st">&#39; &#39;</span>)</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">result <span class="op">=</span> <span class="bu">list</span>(ngrams_pythonic(text_split, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">count_bigram_pythonic <span class="op">=</span> Counter(result)</a>
<a class="sourceLine" id="cb1-16" data-line-number="16"><span class="bu">print</span>(result[:<span class="dv">10</span>])</a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="bu">print</span>(count_bigram_pythonic.most_common(<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb1-18" data-line-number="18"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb1-19" data-line-number="19">[(<span class="st">&#39;The&#39;</span>, <span class="st">&#39;Project&#39;</span>), (<span class="st">&#39;Project&#39;</span>, <span class="st">&#39;Gutenberg&#39;</span>), (<span class="st">&#39;Gutenberg&#39;</span>, <span class="st">&#39;EBook&#39;</span>), (<span class="st">&#39;EBook&#39;</span>, <span class="st">&#39;of&#39;</span>), (<span class="st">&#39;of&#39;</span>, <span class="st">&#39;The&#39;</span>), (<span class="st">&#39;The&#39;</span>, <span class="st">&#39;Adventures&#39;</span>), (<span class="st">&#39;Adventures&#39;</span>, <span class="st">&#39;of&#39;</span>), (<span class="st">&#39;of&#39;</span>, <span class="st">&#39;Sherlock&#39;</span>), (<span class="st">&#39;Sherlock&#39;</span>, <span class="st">&#39;Holmes</span><span class="ch">\n</span><span class="st">by&#39;</span>), (<span class="st">&#39;Holmes</span><span class="ch">\n</span><span class="st">by&#39;</span>, <span class="st">&#39;Sir&#39;</span>)]</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">[((<span class="st">&#39;&#39;</span>, <span class="st">&#39;&#39;</span>), <span class="dv">19750</span>), ((<span class="st">&#39;of&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">10086</span>), ((<span class="st">&#39;in&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">4720</span>), ((<span class="st">&#39;to&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">3486</span>), ((<span class="st">&#39;and&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">2368</span>), ((<span class="st">&#39;on&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">1809</span>), ((<span class="st">&#39;at&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">1593</span>), ((<span class="st">&#39;by&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">1523</span>), ((<span class="st">&#39;of&#39;</span>, <span class="st">&#39;a&#39;</span>), <span class="dv">1443</span>), ((<span class="st">&#39;from&#39;</span>, <span class="st">&#39;the&#39;</span>), <span class="dv">1383</span>)]</a>
<a class="sourceLine" id="cb1-21" data-line-number="21"></a>
<a class="sourceLine" id="cb1-22" data-line-number="22"><span class="co"># 或者用sklearn的CountVectorizer方法</span></a>
<a class="sourceLine" id="cb1-23" data-line-number="23">vectorizer <span class="op">=</span> CountVectorizer(ngram_range<span class="op">=</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">analyzer <span class="op">=</span> vectorizer.build_analyzer()</a>
<a class="sourceLine" id="cb1-25" data-line-number="25">result <span class="op">=</span> analyzer(TEXT)</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">count_bigram_sklearn <span class="op">=</span> Counter(result)</a>
<a class="sourceLine" id="cb1-27" data-line-number="27"><span class="bu">print</span>(result[:<span class="dv">10</span>])</a>
<a class="sourceLine" id="cb1-28" data-line-number="28"><span class="bu">print</span>(count_bigram_sklearn.most_common(<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb1-29" data-line-number="29"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb1-30" data-line-number="30">[<span class="st">&#39;the project&#39;</span>, <span class="st">&#39;project gutenberg&#39;</span>, <span class="st">&#39;gutenberg ebook&#39;</span>, <span class="st">&#39;ebook of&#39;</span>, <span class="st">&#39;of the&#39;</span>, <span class="st">&#39;the adventures&#39;</span>, <span class="st">&#39;adventures of&#39;</span>, <span class="st">&#39;of sherlock&#39;</span>, <span class="st">&#39;sherlock holmes&#39;</span>, <span class="st">&#39;holmes by&#39;</span>]</a>
<a class="sourceLine" id="cb1-31" data-line-number="31">[(<span class="st">&#39;of the&#39;</span>, <span class="dv">12530</span>), (<span class="st">&#39;in the&#39;</span>, <span class="dv">6447</span>), (<span class="st">&#39;to the&#39;</span>, <span class="dv">4464</span>), (<span class="st">&#39;and the&#39;</span>, <span class="dv">3213</span>), (<span class="st">&#39;on the&#39;</span>, <span class="dv">2527</span>), (<span class="st">&#39;at the&#39;</span>, <span class="dv">2103</span>), (<span class="st">&#39;by the&#39;</span>, <span class="dv">1938</span>), (<span class="st">&#39;from the&#39;</span>, <span class="dv">1865</span>), (<span class="st">&#39;with the&#39;</span>, <span class="dv">1735</span>), (<span class="st">&#39;it is&#39;</span>, <span class="dv">1673</span>)]</a>
<a class="sourceLine" id="cb1-32" data-line-number="32"></a>
<a class="sourceLine" id="cb1-33" data-line-number="33"><span class="co"># 或者用正常的index记录方法</span></a>
<a class="sourceLine" id="cb1-34" data-line-number="34"><span class="kw">def</span> ngrams_normal(input_data, n):</a>
<a class="sourceLine" id="cb1-35" data-line-number="35">    <span class="co">&quot;统计文本中n个单词连续出现的次数&quot;</span></a>
<a class="sourceLine" id="cb1-36" data-line-number="36">    input_data <span class="op">=</span> input_data.split(<span class="st">&#39; &#39;</span>)</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">    output <span class="op">=</span> []</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(input_data)<span class="op">-</span>n<span class="op">+</span><span class="dv">1</span>):</a>
<a class="sourceLine" id="cb1-39" data-line-number="39">        output.append(<span class="st">&#39; &#39;</span>.join(input_data[i:i<span class="op">+</span>n]))</a>
<a class="sourceLine" id="cb1-40" data-line-number="40">    <span class="cf">return</span> output</a>
<a class="sourceLine" id="cb1-41" data-line-number="41"></a>
<a class="sourceLine" id="cb1-42" data-line-number="42">result <span class="op">=</span> ngrams_normal(TEXT, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">count_bigram <span class="op">=</span> Counter(result)</a>
<a class="sourceLine" id="cb1-44" data-line-number="44"><span class="bu">print</span>(result[:<span class="dv">10</span>])</a>
<a class="sourceLine" id="cb1-45" data-line-number="45"><span class="bu">print</span>(count_bigram.most_common(<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb1-46" data-line-number="46"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb1-47" data-line-number="47">[<span class="st">&#39;The Project&#39;</span>, <span class="st">&#39;Project Gutenberg&#39;</span>, <span class="st">&#39;Gutenberg EBook&#39;</span>, <span class="st">&#39;EBook of&#39;</span>, <span class="st">&#39;of The&#39;</span>, <span class="st">&#39;The Adventures&#39;</span>, <span class="st">&#39;Adventures of&#39;</span>, <span class="st">&#39;of Sherlock&#39;</span>, <span class="st">&#39;Sherlock Holmes</span><span class="ch">\n</span><span class="st">by&#39;</span>, <span class="st">&#39;Holmes</span><span class="ch">\n</span><span class="st">by Sir&#39;</span>]</a>
<a class="sourceLine" id="cb1-48" data-line-number="48">[(<span class="st">&#39; &#39;</span>, <span class="dv">19750</span>), (<span class="st">&#39;of the&#39;</span>, <span class="dv">10086</span>), (<span class="st">&#39;in the&#39;</span>, <span class="dv">4720</span>), (<span class="st">&#39;to the&#39;</span>, <span class="dv">3486</span>), (<span class="st">&#39;and the&#39;</span>, <span class="dv">2368</span>), (<span class="st">&#39;on the&#39;</span>, <span class="dv">1809</span>), (<span class="st">&#39;at the&#39;</span>, <span class="dv">1593</span>), (<span class="st">&#39;by the&#39;</span>, <span class="dv">1523</span>), (<span class="st">&#39;of a&#39;</span>, <span class="dv">1443</span>), (<span class="st">&#39;from the&#39;</span>, <span class="dv">1383</span>)]</a>
<a class="sourceLine" id="cb1-49" data-line-number="49"></a>
<a class="sourceLine" id="cb1-50" data-line-number="50"><span class="co"># 二、规范化单词并且生成每个单词的概率分布</span></a>
<a class="sourceLine" id="cb1-51" data-line-number="51"><span class="kw">def</span> tokens(text):</a>
<a class="sourceLine" id="cb1-52" data-line-number="52">    <span class="co">&quot;列出文本中所有的单词，并且标准化为小写&quot;</span></a>
<a class="sourceLine" id="cb1-53" data-line-number="53">    <span class="cf">return</span> re.findall(<span class="st">&#39;[a-z]+&#39;</span>, text.lower()) </a>
<a class="sourceLine" id="cb1-54" data-line-number="54"></a>
<a class="sourceLine" id="cb1-55" data-line-number="55"><span class="kw">def</span> pdist(counter):</a>
<a class="sourceLine" id="cb1-56" data-line-number="56">    <span class="co">&quot;生成每个单词的概率分布&quot;</span></a>
<a class="sourceLine" id="cb1-57" data-line-number="57">    N <span class="op">=</span> <span class="bu">sum</span>(counter.values())</a>
<a class="sourceLine" id="cb1-58" data-line-number="58">    <span class="cf">return</span> <span class="kw">lambda</span> x: counter[x]<span class="op">/</span>N</a>
<a class="sourceLine" id="cb1-59" data-line-number="59"></a>
<a class="sourceLine" id="cb1-60" data-line-number="60">count_single_words <span class="op">=</span> Counter(tokens(TEXT))</a>
<a class="sourceLine" id="cb1-61" data-line-number="61">Pword <span class="op">=</span> pdist(count_single_words)</a>
<a class="sourceLine" id="cb1-62" data-line-number="62">P2word <span class="op">=</span> pdist(count_bigram_sklearn)</a>
<a class="sourceLine" id="cb1-63" data-line-number="63"></a>
<a class="sourceLine" id="cb1-64" data-line-number="64"><span class="co"># 三、定义连乘函数、独立的句子概率函数以及条件概率函数</span></a>
<a class="sourceLine" id="cb1-65" data-line-number="65"><span class="kw">def</span> product(nums):</a>
<a class="sourceLine" id="cb1-66" data-line-number="66">    <span class="co">&quot;连乘&quot;</span></a>
<a class="sourceLine" id="cb1-67" data-line-number="67">    result <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-68" data-line-number="68">    <span class="cf">for</span> x <span class="kw">in</span> nums:</a>
<a class="sourceLine" id="cb1-69" data-line-number="69">        result <span class="op">*=</span> x</a>
<a class="sourceLine" id="cb1-70" data-line-number="70">    <span class="cf">return</span> result</a>
<a class="sourceLine" id="cb1-71" data-line-number="71"></a>
<a class="sourceLine" id="cb1-72" data-line-number="72"><span class="kw">def</span> Pwords(words):</a>
<a class="sourceLine" id="cb1-73" data-line-number="73">    <span class="co">&quot;在每个单词都是独立的假设下计算句子概率&quot;</span></a>
<a class="sourceLine" id="cb1-74" data-line-number="74">    <span class="cf">return</span> product(Pword(w) <span class="cf">for</span> w <span class="kw">in</span> words)</a>
<a class="sourceLine" id="cb1-75" data-line-number="75"></a>
<a class="sourceLine" id="cb1-76" data-line-number="76"><span class="kw">def</span> cPword(word, prev):</a>
<a class="sourceLine" id="cb1-77" data-line-number="77">    <span class="co">&quot;在给定先前一个单词A的情况下计算单词B的条件概率&quot;</span></a>
<a class="sourceLine" id="cb1-78" data-line-number="78">    bigram <span class="op">=</span> prev <span class="op">+</span> <span class="st">&#39; &#39;</span> <span class="op">+</span> word</a>
<a class="sourceLine" id="cb1-79" data-line-number="79">    <span class="cf">if</span> P2word(bigram) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> Pword(prev) <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb1-80" data-line-number="80">        <span class="cf">return</span> P2word(bigram) <span class="op">/</span> Pword(prev)</a>
<a class="sourceLine" id="cb1-81" data-line-number="81">    <span class="cf">else</span>: <span class="co"># 如果prev没有出现在文本中，则假设prev的概率和word的概率一样</span></a>
<a class="sourceLine" id="cb1-82" data-line-number="82">        <span class="cf">return</span> Pword(word) <span class="op">/</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb1-83" data-line-number="83">    </a>
<a class="sourceLine" id="cb1-84" data-line-number="84"><span class="kw">def</span> Pwords2(words, prev<span class="op">=</span><span class="st">&#39;&lt;S&gt;&#39;</span>):</a>
<a class="sourceLine" id="cb1-85" data-line-number="85">    <span class="co">&quot;在bigram的假设下计算句子概率&quot;</span></a>
<a class="sourceLine" id="cb1-86" data-line-number="86">    <span class="cf">return</span> product(cPword(w, (prev <span class="cf">if</span> (i <span class="op">==</span> <span class="dv">0</span>) <span class="cf">else</span> words[i<span class="dv">-1</span>]))</a>
<a class="sourceLine" id="cb1-87" data-line-number="87">                   <span class="cf">for</span> (i, w) <span class="kw">in</span> <span class="bu">enumerate</span>(words))</a>
<a class="sourceLine" id="cb1-88" data-line-number="88">  </a>
<a class="sourceLine" id="cb1-89" data-line-number="89"> tests <span class="op">=</span> [<span class="st">&#39;this is a test&#39;</span>, </a>
<a class="sourceLine" id="cb1-90" data-line-number="90">         <span class="st">&#39;this is a unusual test&#39;</span>]</a>
<a class="sourceLine" id="cb1-91" data-line-number="91">    </a>
<a class="sourceLine" id="cb1-92" data-line-number="92"><span class="bu">print</span>(Pwords(tokens(<span class="st">&#39;this is a a test&#39;</span>)))</a>
<a class="sourceLine" id="cb1-93" data-line-number="93"><span class="bu">print</span>(Pwords2(tokens(<span class="st">&#39;this is a a test&#39;</span>)))</a>
<a class="sourceLine" id="cb1-94" data-line-number="94"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb1-95" data-line-number="95"><span class="fl">5.710178770217587e-13</span></a>
<a class="sourceLine" id="cb1-96" data-line-number="96"><span class="fl">3.1528382685054453e-13</span></a>
<a class="sourceLine" id="cb1-97" data-line-number="97"></a>
<a class="sourceLine" id="cb1-98" data-line-number="98"><span class="bu">print</span>(Pwords2(tokens(<span class="st">&#39;this is a neverbeforeseen test&#39;</span>)))</a>
<a class="sourceLine" id="cb1-99" data-line-number="99"><span class="bu">print</span>(Pwords2(tokens(<span class="st">&#39;neverbeforeseen&#39;</span>)))</a>
<a class="sourceLine" id="cb1-100" data-line-number="100"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb1-101" data-line-number="101"><span class="fl">0.0</span></a>
<a class="sourceLine" id="cb1-102" data-line-number="102"><span class="fl">0.0</span></a></code></pre></div>
<p>在这里我们看到一旦句子中出现文本没有出现过的单词，那么整个句子的概率就会被判断为0。但实际上，我们并不希望单个未出现的单词影响了整个句子概率的判断，因此我们可以使用平滑方法来解决这个问题。 而目前的平滑方法一般有</p>
<ul>
<li>Laplace Smoothing</li>
<li>Lidstone’s law</li>
<li>Backoff</li>
<li>Interpolation</li>
<li>Absolute Discounting</li>
<li>Kneser-Ney Smoothing</li>
<li><a href="https://blog.csdn.net/baimafujinji/article/details/51297802" target="_blank" rel="noopener">自然语言处理中N-Gram模型的Smoothing算法小结</a></li>
</ul>
<p>在这里我们使用最简单的方法——<code>Laplace</code>提出的平滑方法。</p>
<p><span class="math inline">\(P(w_n | w_{n-1}) = (count(w_n, w_{n-1}) + c) \ / \ (count(w_{n-1}) + V)\)</span></p>
<p><img src="/images/220px-Pierre-Simon_Laplace.jpg"></p>
<p> <i>What we know is little, and what we are ignorant of is immense.(我们现在所知的真是很少,不知的却无限)<i>— Pierre Simon Laplace, 1749-1827</i></i></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> cPword_smoothing(word, prev):</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    <span class="co">&quot;在给定先前一个单词A的情况下计算单词B的条件概率&quot;</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">    bigram <span class="op">=</span> prev <span class="op">+</span> <span class="st">&#39; &#39;</span> <span class="op">+</span> word</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">    <span class="cf">if</span> P2word(bigram) <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> Pword(prev) <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">        <span class="cf">return</span> P2word(bigram) <span class="op">/</span> Pword(prev)</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">        <span class="co"># add-1 laplace smoothing</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8">        <span class="cf">return</span> (count_bigram_sklearn[bigram] <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> (count_single_words[prev] <span class="op">+</span> <span class="bu">len</span>(count_single_words))</a>
<a class="sourceLine" id="cb2-9" data-line-number="9">    </a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="kw">def</span> Pwords2_smoothing(words, prev<span class="op">=</span><span class="st">&#39;&lt;S&gt;&#39;</span>):</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">    <span class="co">&quot;在bigram的假设下计算句子概率&quot;</span></a>
<a class="sourceLine" id="cb2-12" data-line-number="12">    <span class="cf">return</span> product(cPword_smoothing(w, (prev <span class="cf">if</span> (i <span class="op">==</span> <span class="dv">0</span>) <span class="cf">else</span> words[i<span class="dv">-1</span>]))</a>
<a class="sourceLine" id="cb2-13" data-line-number="13">                   <span class="cf">for</span> (i, w) <span class="kw">in</span> <span class="bu">enumerate</span>(words))</a>
<a class="sourceLine" id="cb2-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb2-15" data-line-number="15"><span class="bu">print</span>(Pwords2_smoothing(tokens(<span class="st">&#39;this is a neverbeforeseen test&#39;</span>)))</a>
<a class="sourceLine" id="cb2-16" data-line-number="16"><span class="bu">print</span>(Pwords2_smoothing(tokens(<span class="st">&#39;neverbeforeseen&#39;</span>)))</a>
<a class="sourceLine" id="cb2-17" data-line-number="17"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb2-18" data-line-number="18"><span class="fl">4.6915593493550787e-20</span></a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="fl">3.4297081318379805e-05</span></a></code></pre></div>
<p>但实际上，为了考虑性能，也就是考虑计算速度以及内存方面，我们一般会采用网上成型的方法。那么kenLM在性能和应用中都很符合我们的需求。</p>
<ul>
<li><a href="https://github.com/kpu/kenlm" target="_blank" rel="noopener">kenLM: Faster and Smaller Language Model Queries</a></li>
<li>SRILM</li>
<li>IRSTLM</li>
<li>MITLM</li>
<li>RandLM</li>
<li>BerkeleyLM</li>
<li>Sheffield</li>
<li>TPT</li>
</ul>
<p><span class="math inline">\(Perplexity(C) = \sqrt[N]{\frac {1}{P(w_1, w_2, ..., w_n)}} = \sqrt[N]{\frac {1}{\prod p(w_i)}}\)</span>，<span class="math inline">\(Perplexity\)</span>值越小，则说明句子越接近真实情况。</p>
<p>我自己训练了一个<code>kenLM</code>，就放一个示例给大家看看。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">import</span> kenlm</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">import</span> jieba</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">klm_model_path <span class="op">=</span> <span class="st">&#39;3_grams_model.klm&#39;</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">klm_model <span class="op">=</span> kenlm.Model(klm_model_path)</a>
<a class="sourceLine" id="cb3-6" data-line-number="6"><span class="kw">def</span> perplexity(string):</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    <span class="cf">return</span> klm_model.perplexity(<span class="st">&#39; &#39;</span>.join(<span class="bu">list</span>(jieba.cut(string))))</a>
<a class="sourceLine" id="cb3-8" data-line-number="8"></a>
<a class="sourceLine" id="cb3-9" data-line-number="9"><span class="bu">print</span>(perplexity(<span class="st">&#39;我想吃番茄鸡蛋面&#39;</span>))</a>
<a class="sourceLine" id="cb3-10" data-line-number="10"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb3-11" data-line-number="11"><span class="fl">62.6007247134787</span> </a>
<a class="sourceLine" id="cb3-12" data-line-number="12"></a>
<a class="sourceLine" id="cb3-13" data-line-number="13"><span class="bu">print</span>(perplexity(<span class="st">&#39;番茄鸡蛋面想吃我&#39;</span>))</a>
<a class="sourceLine" id="cb3-14" data-line-number="14"><span class="op">**</span>[output]<span class="op">**</span></a>
<a class="sourceLine" id="cb3-15" data-line-number="15"><span class="fl">2730.8233144363626</span> </a></code></pre></div>
<p><u><strong>Statistical Language Model总结</strong></u></p>
<p><strong>优点：</strong></p>
<ol type="1">
<li>计算和应用的速度非常快</li>
<li>应用大规模语聊的话可以很好的解决某些词汇概率为0的情况（当然利用<code>smoothing</code>也可以）</li>
</ol>
<p><strong>缺点：</strong></p>
<ol type="1">
<li>离散而非连续地看待语义信息，譬如在<code>SLM</code>中，会认为猫和狗是不一样的，假设“狗”没有出现在词汇表汇总，则“我喜欢猫”和“我喜欢狗”的<span class="math inline">\(perplexity\)</span>值将会有很大的差别。但实际上我们希望它们的概率是接近的。</li>
<li>因为受限于n的大小，<code>SLM</code>缺乏对较远信息的捕捉（而且一般n都是取3）。也就是说，<code>SLM</code>能往前看的视野范围很有限，因此会漏掉前面可能出现的重要信息。</li>
</ol>
<p><img src="/images/tears.png"></p>
<p>后来，<code>Neural Networks Language Model</code>就盛行了。</p>
<p><strong>1.2 Neural Networks Language Model</strong></p>
<p>终于来到<code>LSTM</code>的部分啦，关于<code>Neural Networks Language Model</code>，现在已经有很多<code>state-of-the-art</code>的模型了，这部分后面再慢慢跟大家分享，今天主要是分享基于<code>LSTM</code>的<code>language model</code>，而且希望借此机会，深入认识<code>LSTM</code>的各个细节。</p>
<ul>
<li><strong><code>Recurrent Neural Network(RNN)</code></strong></li>
</ul>
<p><code>LSTM</code>的前身是<code>RNN(Recurrent Neural Network)</code>，<code>RNN</code>的主要用途是处理和预测序列数据，挖掘数据中的时序信息和语义信息。<code>RNN</code>擅长解决序列化相关问题，包括不限于序列化标注问题、<code>NER(Named-entity recognition)</code>、<code>POS(Part-Of-Speech)</code>、语音识别等。</p>
<p>针对语言模型来看，相比于传统的神经网络只有一个有限的窗口来考虑前<span class="math inline">\(n\)</span>个元素，也就是说依然都是<code>n-gram</code>那一套，<code>RNN</code>可以捕捉到所有出现过的单词，也就是说在判断单词出现概率的时候，<span class="math inline">\(n\)</span>可以在<code>RNN</code>中实现无限大。</p>
<p>在<code>RNN</code>中，只有一个自连接的隐藏层，我们可以简单看成输入<span class="math inline">\(x_t\)</span>， 经过一个网络，输出一个值<span class="math inline">\(h_t\)</span>。而且这个网络的特点就是，隐藏层的输出作为下一时刻它的输入。</p>
<p><img src="/images/RNN_fold.png"></p>
<p>如果将<code>RNN</code>展开，或许就会知道<code>RNN</code>其实并没有看上去那么神秘。我们可以把<code>RNN</code>看成是由多个隐藏层水平连接，这些隐藏层共用一套参数。每个隐藏层都会将对应的信息传递给后者。下面我们来展开看看：</p>
<p><img src="/images/RNN_unfold.png"></p>
<p>我们可以看到每一个时刻都有一个新的输入<span class="math inline">\(x_t\)</span>，同时会考虑之前所有的输入<span class="math inline">\(x_1\)</span>到<span class="math inline">\(x_{t-1}\)</span>，最后预测当前时刻的输出<span class="math inline">\(h_t\)</span>。如果用公式来表示，这个过程相当简洁美妙： <span class="math display">\[
\begin{aligned}
h_t &amp;= tanh\biggl(W_{matrix}\begin{pmatrix}
    x_t \\
    h_{t-1}
    \end{pmatrix}\biggr) \\\\
    &amp;=  tanh(W_{xh}x_t \ \ + w_{hh}h_{t-1} )
\end{aligned}
\]</span></p>
<p>其中，<span class="math inline">\(x_t\)</span>是长度为<span class="math inline">\(I\)</span>的向量，表示<span class="math inline">\(t\)</span>时刻的输入。<span class="math inline">\(h_t\)</span>是长度为<span class="math inline">\(H\)</span>的向量，表示<span class="math inline">\(t\)</span>时刻的输出。<span class="math inline">\(W\)</span>的维度是<span class="math inline">\(H \mathsf x (I+H)\)</span>。而通常需要对<span class="math inline">\(h_t\)</span>做一次映射，即<span class="math inline">\(o_t = W_{ho} h_t\)</span>，<span class="math inline">\(W_{ho}(V)\)</span>的维度是<span class="math inline">\(K \mathsf x H\)</span>，然后再接入一个<span class="math inline">\(softmax\)</span>进行<span class="math inline">\(t\)</span>时刻的预测，即<span class="math inline">\(y^{&#39;} = softmax(W_{ho}h_t)\)</span>。通过公式我们可以看出，我们一共需要学习<span class="math inline">\(H \mathsf x (I+H) + HK\)</span>个参数，然后就可以处理任意长度的序列。</p>
<p>在这里我们顺便复习一下<code>sigmoid</code>和<code>tanh</code>两个激活函数吧。</p>
<ul>
<li><code>sigmoid</code> <span class="math inline">\(\in [0, 1]\)</span></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
y &amp;= \frac{1}{1 + e^{-x}} \\\\
\frac{\partial y}{\partial x} &amp; = -\frac{1}{1 + e^{-x}} \cdot e^{-x} \cdot (-1) \\
&amp;= \frac{e^{-x}}{(1+e^{-x})^2} \\
&amp;= \frac{1}{1+e^{-x}} \cdot \frac{1 + e^{-x} - 1}{1+e^{-x}} \\
&amp;= y \cdot (1 - y)
\end{aligned}
\]</span></p>
<ul>
<li><code>tanh</code> <span class="math inline">\(\in [-1, 1]\)</span> <span class="math display">\[
\begin{aligned}
y &amp; = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\
\frac{\partial y}{\partial x} &amp;= \frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2}
\end{aligned}
\]</span></li>
</ul>
<p>但接下来在介绍<code>RNN</code>的反向传播过程，也就是<code>BackProgagation Through Time(BPTT)</code>，我们就会发现<code>RNN</code>在处理较长序列时，就会很容易出现<code>梯度消失</code>或者<code>梯度爆炸</code>的情况。</p>
<p>就像很多后向传播算法一样，<code>BPTT</code>重复地使用链式法则。区别在于，对于<code>RNN</code>而言，损失函数不仅依赖于当前时刻的输出层，还需要依赖下一时刻。除了输出层的参数<span class="math inline">\(V=W_{ho}\)</span>，<span class="math inline">\(U=W_{xh}\)</span>和<span class="math inline">\(W = W_{hh}\)</span>在计算更新梯度时都需要考虑当前时刻的梯度和下一时刻的梯度。</p>
<p>首先我们定义<span class="math inline">\(E_t\)</span>为当前时刻在给定模型时预测<span class="math inline">\(y^{&#39;}\)</span>和实际标注<span class="math inline">\(y_t\)</span>的损失函数，或者我们叫交叉熵损失函数<code>(cross-entroy loss)</code>，<span class="math inline">\(E_t = E(y^{&#39;}, y_t) = \sum_t E( \hat y, y_t) = - \sum_t y_tlog(\hat y_{t})\)</span> 。</p>
<p>然后再令<span class="math inline">\(z_t = W_{ho}h_t\)</span>，令<span class="math inline">\(a_t = W_{xh}x_t \ \ + w_{hh}h_{t-1}\)</span>以及<span class="math inline">\(\delta_t\)</span>表示在<span class="math inline">\(t\)</span>时刻<span class="math inline">\(a_t\)</span>接收到的梯度。</p>
<p>然后对<span class="math inline">\(W_{ho}\)</span>求导： <span class="math display">\[
\frac{\partial E_t}{\partial W_{ho}} 
= \frac{\partial E_t}{\partial y^{&#39;}} \cdot \frac{\partial y^{&#39;}}{\partial W_{ho}}
= \frac{\partial E_t}{\partial y^{&#39;}} \cdot \frac{\partial y^{&#39;}}{\partial z_t} \cdot \frac{\partial z_t}{\partial W_{ho}} 
\]</span></p>
<p><span class="math display">\[
\delta_t = \frac{\partial E_t}{\partial y^{&#39;}} \cdot \frac{\partial y^{&#39;}}{\partial h_t} \cdot \frac{\partial h_t}{\partial a_t} + \delta_{t+1} \cdot \frac{\partial a_{t+1}}{\partial h_t} \cdot \frac{\partial h_t}{a_t}
\]</span></p>
<p>求出<span class="math inline">\(\delta_t\)</span>后，就可以求当前时刻对<span class="math inline">\(W_{xh}\)</span>和<span class="math inline">\(W_{hh}\)</span>的导数，分别是<span class="math inline">\(\delta_xx_{t}^T\)</span>和<span class="math inline">\(\delta_th_{t-1}^T\)</span>。可以看出，<span class="math inline">\(t\)</span>时刻的<span class="math inline">\(E_t\)</span>求出的导数会依次传到<span class="math inline">\(t-1\)</span>，<span class="math inline">\(t-2\)</span>，<span class="math inline">\(…\)</span>， <span class="math inline">\(1\)</span>时刻。</p>
<p><img src="/images/BPTT.png"></p>
<p>最开始的时刻<span class="math inline">\(1\)</span> 得到时刻<span class="math inline">\(t\)</span>损失函数<span class="math inline">\(E_t\)</span>传来的导数是： <span class="math display">\[
\frac{\partial E_t}{\partial y^{&#39;}} \cdot \frac{\partial y^{&#39;}}{\partial h_t} 
\cdot (\frac{\partial h_t}{\partial h_{t-1}} 
\cdot \frac{\partial h_{t-1}}{\partial h_{t-2}} ...
\frac{\partial h_{2}}{\partial h_{1}})
\cdot \frac{\partial h_1}{\partial a_1}
\]</span> 其中 <span class="math display">\[
\frac{\partial h}{\partial h_{t-1}} = W_{hh}^T \cdot ( 1 - tanh(a_t)^2)
\]</span> <span class="math inline">\(tanh(a_t)\)</span>的导数是<span class="math inline">\(1-tanh(a_t)^2\)</span>，范围是<span class="math inline">\([0, 1]\)</span>，<span class="math inline">\(\frac{\partial h}{\partial h_{h-1}}\)</span>通常是小于<span class="math inline">\(1\)</span>的数，所以括号里面导数的连乘结果会非常接近<span class="math inline">\(0\)</span>，这就会导致<code>梯度消失</code>。如果忽略激活函数<span class="math inline">\(tanh\)</span>的导数，上面时刻<span class="math inline">\(1\)</span>获得时刻<span class="math inline">\(t\)</span>传来的导数就是<span class="math inline">\((W_{hh}^T)^t\)</span>，设<span class="math inline">\(W_{hh}^T\)</span>的特征值对角矩阵是<span class="math inline">\(\wedge\)</span>，<span class="math inline">\((W_{hh}^T)^t\)</span>就和<span class="math inline">\(\wedge\)</span>有关，如果某个特征值大于<span class="math inline">\(1\)</span>，<span class="math inline">\((W_{hh}^T)^t\)</span>就会很大，可能出现<code>梯度爆炸</code>。如果某个特征值小于<span class="math inline">\(1\)</span>，<span class="math inline">\((W_{hh}^T)^t\)</span>就会很小，可能出现<code>梯度消失</code>。所以当在训练日志中如果发现很多数值出现<code>nan</code>，就说明很有可能出现梯度爆炸。当然我们有很多方法可以防止<code>梯度消失</code>或者<code>梯度爆炸</code>，譬如<code>梯度裁剪</code>、更换激活函数（用<span class="math inline">\(ReLu\)</span>替换<span class="math inline">\(tanh\)</span>）、<code>残差结构</code>、<code>Batch Norm</code>等等。但这篇文章要讨论的方案是利用<code>LSTM</code>替换<code>RNN</code>。</p>
<p>事实上，<span class="math inline">\(\frac{\partial h}{\partial h_{t-1}}\)</span>本质上是代表当我们在前一时刻对<code>hidden state</code>做调整改变的话，对<code>t</code>时刻的<code>hidden state</code>有多大的影响。所以根据上面的公式来看，如果出现<code>梯度消失</code>，则意味着较早的<code>hidden state</code>对后来的<code>hidden state</code>没有实质性的影响，也就意味着没有起到长期依赖的作用。</p>
<ul>
<li><strong><code>Long Short-term Memory(LSTM)</code></strong></li>
</ul>
<p>早在1977年，<a href="https://www.wikiwand.com/en/Sepp_Hochreiter" target="_blank" rel="noopener">Sepp Hochreiter</a> and <a href="https://www.wikiwand.com/en/Jürgen_Schmidhuber" target="_blank" rel="noopener">Jürgen Schmidhuber</a>就已经提出利用<a href="https://www.bioinf.jku.at/publications/older/2604.pdf" target="_blank" rel="noopener">LSTM</a>解决<code>RNN</code>训练过程中长距离依赖的问题。<code>LSTM</code>主要是靠引入“门”机制，主要是<code>输入门(input gate)</code>、<code>遗忘门(forget gate)</code>、<code>输出门(output gate)</code>以及<code>单元状态值(cell state)</code>。那么我们就来看看这些变量的含义以及数学表达。</p>
<ol type="1">
<li><p><code>遗忘门(forget gate)</code></p>
<p>在<code>LSTM</code>的第一个过程就是我们要决定前一个单元状态的遗忘程度。 <span class="math display">\[
f_t = \sigma(W_{xh}^fx_t + W_{hh}^f h_{t-1})
\]</span></p></li>
<li><p><code>输入门(input gate)</code> &amp; <code>输入门相关状态(candidate values)</code></p>
<p>第二步就是确定新状态通过的程度。这里有两个部分，第一个部分是<code>input gate</code>，它决定了有多少比例的值需要更新。第二部分是<code>candidate values</code>，这部分值是用来加入到当前的单元状态。 <span class="math display">\[
\begin{aligned}
i_t &amp;= \sigma(W_{xh}^ix_t + W_{hh}^i h_{t-1}) \\\\
g_t &amp;= tanh(W_{xh}^gx_t + W_{hh}^g h_{t-1})
\end{aligned}
\]</span></p></li>
<li><p><code>单元状态值(cell state)</code></p>
<p>现在是时候从前一个<code>cell state</code>更新到新的<code>cell state</code>。 <span class="math display">\[
c_t = f_t \circ c_{t-1} + i_t \circ g_t
\]</span></p></li>
<li><p><code>输出门(output gate)</code> &amp; <code>当前隐藏状态输出(Hidden state output)</code></p>
<p>最后，我们就要决定输出的值是什么，这个输出值是基于已经更新的<code>cell state</code>。首先我们需要决定有多少比例的<code>cell state</code>需要被输出，然后我们将<span class="math inline">\(c_t\)</span>经过<span class="math inline">\(tanh\)</span>激活函数映射成<span class="math inline">\([-1, 1]\)</span>，再和<span class="math inline">\(sigmoid\)</span>值相乘来输出我们决定要输出的部分。 <span class="math display">\[
\begin{aligned}
o_t = \sigma(W_{xh}^ox_t + W_{hh}^o h_{t-1}) \\\\
h_t = o_t \circ tanh(c_t)
\end{aligned}
\]</span></p></li>
</ol>
<p>而其实当<span class="math inline">\(i_t = 1, f_t = 0, o_t = 1\)</span>，就几乎变成了<code>RNN</code>了。</p>
<p>下面我们来看看<code>LSTM</code>在求偏导的时候发生了什么，以致<code>LSTM</code>能解决<code>梯度消失</code>或<code>梯度爆炸</code>问题。 <span class="math display">\[
\begin{aligned}
\frac{\partial c_t}{\partial c_{t-1}} &amp;= \frac{\partial c_t}{\partial f_t}\frac{\partial f_t}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial c_{t-1}} 
+ \frac{\partial c_t}{\partial i_t}\frac{\partial i_t}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial c_{t-1}} \\\\
&amp;+ \frac{\partial c_t}{\partial g_t}\frac{\partial g_t}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial c_{t-1}}
+ \frac{\partial c_t}{\partial c_{t-1}} \\\\
\frac{\partial c_t}{\partial c_{t-1}} 
&amp; = c_{t-1} \sigma^{&#39;}(\cdot)W_f \circ o_{t-1} tanh^{&#39;}(c_{t-1}) \\\\
&amp;+ g_{t} \sigma^{&#39;}(\cdot)W_i \circ o_{t-1} tanh^{&#39;}(c_{t-1}) \\\\
&amp;+i_ttanh^{&#39;}(\cdot)W_C \circ o_{t-1}tanh^{&#39;}(c_{t-1}) \\\\
&amp;+ f_t
\end{aligned}
\]</span> 现在我们就可以看到，当<code>LSTM</code>在进行反向传播<span class="math inline">\(k\)</span>次的时候，我们是连乘了<span class="math inline">\(k\)</span>次上面这个公式。而这与<code>RNN</code>不同的是，<span class="math inline">\(\frac{\partial h}{\partial h_{t-1}}\)</span>最终总是大于1，或者一直在<span class="math inline">\([0, 1]\)</span>，这最终导致了<code>梯度消失</code>或者<code>梯度爆炸</code>。而<span class="math inline">\(\frac{\partial c_t}{\partial c_{t-1}}\)</span>是可以动态调整，如果开始的部分趋向于<span class="math inline">\(0\)</span>，那么我们可以通过调整<span class="math inline">\(f_t\)</span>的值或者其他部分(<span class="math inline">\(f_t\)</span>，<span class="math inline">\(o_t\)</span>， <span class="math inline">\(i_t\)</span>， <span class="math inline">\(g_t\)</span>)来使<span class="math inline">\(\frac{\partial c_t}{\partial c_{t-1}}\)</span>爬升接近<span class="math inline">\(1\)</span>，这样可以防止<code>梯度消失</code>或者说是降低<code>梯度消失的程度</code>。</p>
<p>下面我们来看一个比较简单的<code>LSTM</code>语言模型的<code>PyTorch</code>版本。</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="kw">class</span> RNNLM(nn.Module):</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, embed_size, hidden_size, num_layers):</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">        <span class="bu">super</span>(RNNLM, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">        <span class="va">self</span>.embed <span class="op">=</span> nn.Embedding(vocab_size, embed_size)</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(embed_size, hidden_size, num_layers, batch_first<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb4-9" data-line-number="9">        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(hidden_size, vocab_size)</a>
<a class="sourceLine" id="cb4-10" data-line-number="10"></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">    <span class="kw">def</span> forward(<span class="va">self</span>, x, h):</a>
<a class="sourceLine" id="cb4-12" data-line-number="12">        x <span class="op">=</span> <span class="va">self</span>.embed(x)</a>
<a class="sourceLine" id="cb4-13" data-line-number="13">        out, (h, c) <span class="op">=</span> <span class="va">self</span>.lstm(x, h)</a>
<a class="sourceLine" id="cb4-14" data-line-number="14">        out <span class="op">=</span> out.reshape(out.size(<span class="dv">0</span>)<span class="op">*</span>out.size(<span class="dv">1</span>), out.size(<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb4-15" data-line-number="15">        out <span class="op">=</span> <span class="va">self</span>.linear(out)</a>
<a class="sourceLine" id="cb4-16" data-line-number="16">        <span class="cf">return</span> out, (h, c)</a></code></pre></div>
<p>收工～</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/25/CRF/" rel="next" title="Condition Random Field Model">
                <i class="fa fa-chevron-left"></i> Condition Random Field Model
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/pandas.jpeg"
               alt="Peter Tsung" />
          <p class="site-author-name" itemprop="name">Peter Tsung</p>
           
              <p class="site-description motion-element" itemprop="description">I Never Save Anything For The Swim Back.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">7</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Peter Tsung</span>
</div>

<!--
<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
                    TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
                            messageStyle: "none"
                                }); 
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Queue(function() {
                    var all = MathJax.Hub.getAllJax(), i;
                            for(i=0; i < all.length; i += 1) {
                                            all[i].SourceElement().parentNode.className += ' has-jax';
                                                    }
                                                        });
        </script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

-->


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  

  

  

  


  <script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
  <script type="text/javascript">
  $(window).load(function(){
     $('pre').addClass('prettyprint linenums').attr('style', 'overflow:auto;');
     prettyPrint();
   })    
  </script>
</body>
</html>
