<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="I Never Save Anything For The Swim Back.">
<meta property="og:type" content="website">
<meta property="og:title" content="彼得的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="彼得的博客">
<meta property="og:description" content="I Never Save Anything For The Swim Back.">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="彼得的博客">
<meta name="twitter:description" content="I Never Save Anything For The Swim Back.">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>




<link href="/js/google-code-prettify/prettify.css" type="text/css" rel="stylesheet" />
<link href="/js/google-code-prettify/github-v2.min.css" type="text/css" rel="stylesheet" />

  <title>彼得的博客</title>
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
       <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">彼得的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/04/Long-Short-term-Memory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/05/04/Long-Short-term-Memory/" itemprop="url">
                  Long Short-term Memory
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-04T23:21:46+08:00">
                2019-05-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/images/LSTM.png"></p>
<p>今天终于有机会来手撕<code>LSTM</code>了。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/25/CRF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/08/25/CRF/" itemprop="url">
                  Condition Random Field Model
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-25T21:16:13+08:00">
                2018-08-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>拖了很多的一段时间了，今天就总结一下<code>CRF(Condition Random Field Model)</code>，这是一个非常优美的判别式算法。（截止到今天已经是2019年5月2号了，自从上次建立这篇文章已经过去了大半年了，当时也只写了标题。。前两天才慢慢总结回来。证明如果有些事情想做，最好就是在当下）</p>
<p>第一次接触<code>CRF</code>是因为要做音乐名<code>NER</code>，后来在做<code>TTS(Text To Speech)</code>的时候也有尝试用<code>Bi-LSTM+CRF</code>来判断一句没有标点的长句子中应该在哪里进行断句。</p>
<p><strong>一、从随机场到线性链条件随机场</strong></p>
<p>我们知道，在<code>HMM</code>中，观测节点<span class="math inline">\(o_i\)</span>依赖隐藏状态节点<span class="math inline">\(i_i\)</span>，也就意味着观测节点只依赖当前时刻的隐藏状态。但在实际场景下，观测序列是需要很多特征来刻画，譬如标注<span class="math inline">\(i_i\)</span>不仅跟当前状态<span class="math inline">\(o_i\)</span>相关，而且还跟前后标注<span class="math inline">\(o_j(j \neq i)\)</span>相关。</p>
<p>从这个层面来看，<code>CRF</code>跟普通分类器不同的地方在于，它在标记数据的时候，可以考虑相邻数据的标记信息。这一点是普通的分类器难以做到的。<code>随机场</code>是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做<code>随机场</code>。而<code>马尔科夫随机场</code>是随机场的特例，它假设<code>随机场</code>中某一个位置的赋值仅仅与它相邻的位置赋值有关，和跟它不相邻的位置赋值无关。</p>
<p>而<code>条件随机场</code>是<code>马尔科夫随机场</code>的特例，它假设<code>马尔科夫随机场</code>中只有<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>两种变量，<span class="math inline">\(X\)</span>一般是给定的，而<span class="math inline">\(Y\)</span>一般是在给定<span class="math inline">\(X\)</span>的条件下的输出。</p>
<p>作为一个判断别式算法，<code>CRF</code>是给定一组输入随机变量<span class="math inline">\(X\)</span>（具体，对应观测序列<span class="math inline">\(o_1, …, o_i\)</span>）条件下，另一组输出随机变量<span class="math inline">\(Y\)</span>（具体，对应隐藏状态序列<span class="math inline">\(i_1, …, i_i\)</span>的条件概率分布模型，也就是用于预测与输入序列相对应标注序列的模型。</p>
<p><strong>准确的<code>CRF</code>数学语言描述是</strong>：设<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>是随机变量，<span class="math inline">\(P(Y|X)\)</span>是给定<span class="math inline">\(X\)</span>时<span class="math inline">\(Y\)</span>的条件概率分布，若随机变量<span class="math inline">\(Y\)</span>构成的是一个<code>马尔科夫随机场</code>，则成条件概率分布<span class="math inline">\(P(Y|X)\)</span>是条件随机场。</p>
<p>这里主要介绍定义在线性链上的特殊的条件随机场，称为线性链条件随机场(<code>linear chain conditional random field</code>)。简单来说就是我们要求<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>都具有相同的结构，即 <span class="math display">\[
X = (X_1, X_2, ..., X_n), Y=(Y_1, Y_2, ..., Y_n)
\]</span> <strong><code>linear-CRF</code>的数学定义：</strong>设<span class="math inline">\(X = (X_1, X_2, ..., X_n)\)</span>，<span class="math inline">\(Y=(Y_1, Y_2, ..., Y_n)\)</span>均为线性链表示的随机变量序列，在给定随机变量序列<span class="math inline">\(X\)</span>的情况下，随机变量<span class="math inline">\(Y\)</span>的条件概率分布<span class="math inline">\(P(Y|X)\)</span>构成条件随机场，即满足马尔科夫性： <span class="math display">\[
P(Y_i|X, Y_1, Y_2, ..., Y_n) = P(Y_i|X, Y_{i-1}, Y_{i+1})
\]</span> 在<code>linear-CRF</code>中，特征函数分为两类，第一类是定义在<span class="math inline">\(Y\)</span>节点上的<strong>节点特征函数</strong>，这类特征函数只和当前节点有关，记为： <span class="math display">\[
s_l(y_i, x, i), \ \ l = 1, 2, ..., L
\]</span> 其中<span class="math inline">\(L\)</span>是定义在该节点的节点特征函数的总个数，<span class="math inline">\(i\)</span>是当前节点在序列的位置。</p>
<p>第二类是定义在<span class="math inline">\(Y\)</span>上下文的<strong>局部特征函数</strong>，这类特征函数只和当前节点和上一个节点有关，记为： <span class="math display">\[
t_k(y_{i-1}, y_i, x, i), \ \ k = 1, 2, ..., K
\]</span> 其中<span class="math inline">\(K\)</span>是定义在该节点的局部特征函数的总个数，<span class="math inline">\(i\)</span>是当前节点在序列的位置。之所以只有上下文相关的局部特征函数，没有不相邻节点之前的特征函数，是因为<code>linear-CRF</code>符合<code>马尔科夫性</code>。</p>
<p>无论是节点特征函数还是局部特征函数，它们的取值只能是0或者1。即满足特征条件或者不满足特征条件，同时我们可以为每个特征函数赋予一个权值，用以表达对这个特征函数的信任度。假设<span class="math inline">\(t_k\)</span>的权重系数是<span class="math inline">\(\lambda_k\)</span>，<span class="math inline">\(s_l\)</span>的权重系数是<span class="math inline">\(\mu_l\)</span>，则<code>linear-CRF</code>是由<span class="math inline">\(t_k, \lambda_k, s_l, \mu_l\)</span>共同决定。</p>
<p>此时就可以得到<code>linear-CRF</code>的参数化形式： <span class="math display">\[
P(y|x) = \frac{1}{Z(x)}exp\biggl(\sum_{i,k}\lambda_kt_k(y_{i-1}, y_i, x, i) + \sum_{i,l}\mu_ls_l(y_i, x, i)\biggr)
\]</span> 其中，<span class="math inline">\(Z(x)\)</span>为规范化因子： <span class="math display">\[
Z(x) = \sum_yexp\biggl(\sum_{i,k}\lambda_kt_k(y_{i-1}, y_i, x, i) + \sum_{i,l}\mu_ls_l(y_i, x, i)\biggr)
\]</span> 在表示上来看，我们用<span class="math inline">\(s_l\)</span>表示节点特征函数，用<span class="math inline">\(t_k\)</span>表示局部特征函数，同时也用了不同的符号表示权重系数，导致表示起来比较麻烦。在这边我们对特征函数稍加整理统一起来。</p>
<p>假设我们有<span class="math inline">\(K_1\)</span>个局部特征函数和<span class="math inline">\(K_2\)</span>个节点特征函数，总共有<span class="math inline">\(K=K_1 + K_2\)</span>个特征函数。我们用一个特征函数<span class="math inline">\(f_k(y_{i-1}, y_i, x, i)\)</span>来统一表示： <span class="math display">\[
f_k(y_{i-1}, y_i, x, i) = \begin{cases} 
t_k(y_i, y_i, x, i) \ \ k = 1, 2, ..., K_1 \\
s_l(y_i, x, i) \ \ k = K_1 + l, l = 1, 2, ..., K_2
\end{cases}
\]</span> 对<span class="math inline">\(f_k(y_{i-1}, y_i, x, i)\)</span>在各个序列位置求和得到： <span class="math display">\[
f_k(y, x) = \sum_{i=1}^{n}f_k(y_{i-1}, y_i, x, i)
\]</span> 同时我们也统一<span class="math inline">\(f_k(y_{i-1}, y_i, x, i)\)</span>对应的权重系数<span class="math inline">\(w_k\)</span>如下： <span class="math display">\[
w_k = \begin{cases} 
\lambda_k \ \ k = 1, 2, ..., K_1 \\
\mu_l \ \ k = K_1 + l, l = 1, 2, ..., K_2
\end{cases}
\]</span> 这样，<code>linear-CRF</code>的参数化形式简化为： <span class="math display">\[
P(y|x) = \frac{1}{Z(x)}exp\sum_{k=1}^Kw_kf_k(y, x)
\]</span> 其中，<span class="math inline">\(Z(x)\)</span>为规范化因子： <span class="math display">\[
Z(x) = \sum_yexp\sum_{k=1}^Kw_kf_k(y, x)
\]</span> 如果将上式中的<span class="math inline">\(w_k\)</span>与<span class="math inline">\(f_k\)</span>用向量表示，即 <span class="math display">\[
w = (w_1, w_2, ..., w_K)^T \ \ \ \ F(y, x) = (f_1(y, x), f_2(y, x), ..., f_K(y, x))^T 
\]</span> 则<code>linear-CRF</code>的参数化形式简化为内积形式如下： <span class="math display">\[
P_w(y|x) = \frac{exp(w \cdot F(y, x))}{Z_w(x)} = \frac{exp(w \cdot F(y, x))}{\sum_yexp(w \cdot F(y, x))}
\]</span> 而同时我们可以将<code>linear-CRF</code>的参数化形式写成矩阵形式。首先定义一个$m  x  m <span class="math inline">\(的矩阵\)</span>M$， <span class="math inline">\(m\)</span>为<span class="math inline">\(y\)</span>所有可能状态的取值个数，<span class="math inline">\(M\)</span>的定义如下： <span class="math display">\[
\left[
\begin{array}{cc|c}
M_i(y_{i-1}, y_i|x) 
\end{array}
\right] = 
\left[
\begin{array}{cc|c}
exp(W_i(y_{i-1}, y_i|x))
\end{array}
\right]
= 
\left[
\begin{array}{cc|c}
exp(\sum_{k=1}^{K}w_kf_k(y_{i-1, y_i, x, i}))
\end{array}
\right]
\]</span> 我们引入起点和终点标记<span class="math inline">\(y_0=start\)</span>, <span class="math inline">\(y_{n+1} = stop\)</span>，这样序列<span class="math inline">\(y\)</span>的非规范化概率可以通过<span class="math inline">\(n+1\)</span>个矩阵元素的乘积得到，即： <span class="math display">\[
P_w(y|x) = \frac{1}{Z_w(x)} \prod_{i=1}^{n+1}M_i(y_{i-1}, y_i|x)
\]</span> 其中<span class="math inline">\(Z_w(x)\)</span>为规范化因子。</p>
<p><strong>二、前向后向算法评估标记序列概率</strong></p>
<p><strong>2.1 <code>linear-CRF</code>的三个基本问题</strong></p>
<p><strong><code>linear-CRF</code>第一个问题：评估。</strong>即给定<code>linear-CRF</code>的条件概率分布<span class="math inline">\(P(y|x)\)</span>，在给定输入序列<span class="math inline">\(x\)</span>和输出序列<span class="math inline">\(y\)</span>时，计算条件概率<span class="math inline">\(P(y_i|x)\)</span>和<span class="math inline">\(P(y_{i-1}, y_i|x)\)</span>以及对应的期望。</p>
<p><strong><code>linear-CRF</code>第二个问题：学习。</strong>即给定训练数据<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>，学习<code>linear-CRF</code>的模型参数<span class="math inline">\(w_k\)</span>和条件概率<span class="math inline">\(P_w(y|x)\)</span>，梯度下降或者拟牛顿法都可以解决。</p>
<p><strong><code>linear-CRF</code>第三个问题：解码。</strong>即给定<code>linear-CRF</code>的条件概率分布<span class="math inline">\(P(y|x)\)</span>，和输入序列<span class="math inline">\(x\)</span>， 计算使条件概率最大的输出序列<span class="math inline">\(y\)</span>，而我们使用<code>维特比算法</code>就可以解决这个问题。</p>
<p><strong>2.2 <code>linear-CRF</code>的前向后向概率概述</strong></p>
<p>首先来看前向概率计算。</p>
<p><strong>我们定义<span class="math inline">\(\alpha_i(y_i|x)\)</span>表示序列位置<span class="math inline">\(i\)</span>的标记是<span class="math inline">\(y_i\)</span>时，在位置<span class="math inline">\(i\)</span>之前的部分标记序列的非规范化概率。</strong></p>
<p>我们之前定义了一个矩阵<span class="math inline">\(M\)</span>： <span class="math display">\[
M_i(y_{i-1}, y_i|x) = exp(\sum_{k=1}^Kw_kf_k(y_{i-1}, y_i, x, i))
\]</span> 这个式子定义了在给定<span class="math inline">\(y_{i-1}\)</span>时，从<span class="math inline">\(y_{i-1}\)</span>转移到<span class="math inline">\(y_i\)</span>的非规范化概率。</p>
<p>这样就可以得到序列位置<span class="math inline">\(i+1\)</span>的标记是<span class="math inline">\(y_{i+1}\)</span>时，在位置<span class="math inline">\(i+1\)</span>之前的部分标记序列的非规范化概率<span class="math inline">\(\alpha_{i+1}(y_{i+1}|x)\)</span>的递推公式： <span class="math display">\[
\alpha_{i+1}(y_{i+1}|x) = \alpha_i(y_i|x)M_{i+1}(y_{i+1}, y_i|x) \ \ i = 1, 2, ..., n+1
\]</span> 在起点处，我们定义： <span class="math display">\[
\alpha_0(y_0|x) = \begin{cases} 
1 \ \ \ y_0 = start \\
0 \ \ \ else
\end{cases}
\]</span> 假设我们可能标记的总数是<span class="math inline">\(m\)</span>，则<span class="math inline">\(y_i\)</span>的取值就有<span class="math inline">\(m\)</span>个，我们用<span class="math inline">\(\alpha_i(x)\)</span>表示<span class="math inline">\(m\)</span>个值组成的前向向量如下： <span class="math display">\[
\alpha_i(x) = (\alpha_i(y_i == 1|x), \alpha_i(y_i == 2|x), ..., \alpha_i(y_i == m)|x) ^T
\]</span> 同时用矩阵<span class="math inline">\(M_i(x)\)</span>表示由<span class="math inline">\(M_i(y_{i-1}, y_i|x)\)</span>形成的$m  x  m $矩阵： <span class="math display">\[
M_i(x) = \left[
\begin{array}{cc|c}
M_i(y_{i-1}, y_i|x) 
\end{array}
\right]
\]</span> 那么这样递推公式可以用矩阵乘积表示： <span class="math display">\[
\alpha_{i+1}^T(x) = \alpha_i^T(x)M_{i+1}(x)
\]</span> 同样，<strong>我们定义<span class="math inline">\(\beta_i(y_i|x)\)</span>表示序列位置<span class="math inline">\(i\)</span>的标记是<span class="math inline">\(y_i\)</span>时，在位置<span class="math inline">\(i\)</span>之后的从<span class="math inline">\(i+1\)</span>到<span class="math inline">\(n\)</span>的部分标记序列的非规范化概率。</strong>这样就可以得到序列位置<span class="math inline">\(i+1\)</span>的标记是<span class="math inline">\(y_{i+1}\)</span>时，在位置<span class="math inline">\(i\)</span>之后的部分标记序列的非规范化概率<span class="math inline">\(\beta_i(y_i|x)\)</span>的递推公式： <span class="math display">\[
\beta_i(y_i|x) = M_{i+1}(y_i, y_{i+1}|x)\beta_{i+1}(y_{i+1}|x)
\]</span> 在终点处，我们定义： <span class="math display">\[
\beta_{n+1}(y_{n+1}|x) = \begin{cases} 
1 \ \ \ y_{n+1} = stop \\
0 \ \ \ else
\end{cases}
\]</span> 如果用向量表示，则有： <span class="math display">\[
\beta_i(x) = M_{i+1}(x)\beta_{i+1}(x)
\]</span> 而规范化因子<span class="math inline">\(Z(x)\)</span>的表达式是： <span class="math display">\[
Z(x) = \sum_{c=1}^{m}a_n(y_c|x) = \sum_{c=1}^m \beta_1(y_c|x)
\]</span> 用向量表示<span class="math inline">\(Z(x)\)</span>： <span class="math display">\[
Z(x) = \alpha_n^T(x) \cdot 1 = 1^T \cdot \beta_1(x)
\]</span> 其中<span class="math inline">\(1\)</span>是<span class="math inline">\(m\)</span>维全1的向量。</p>
<p><strong>2.3 <code>linear-CRF</code>的前向后向概率计算</strong></p>
<p>有了前向后向的定义和计算方法，我们就可以计算序列位置<span class="math inline">\(i\)</span>的标记是<span class="math inline">\(y_i\)</span>时的条件概率<span class="math inline">\(P(y_i|x)\)</span>： <span class="math display">\[
P(y_i|x) = \frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)} = \frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{\alpha_n^T(x) \cdot 1}
\]</span> 也可以计算出序列位置<span class="math inline">\(i\)</span>的标记为<span class="math inline">\(y_i\)</span>时，位置<span class="math inline">\(i-1\)</span>的标记是<span class="math inline">\(y_{i-1}\)</span>时的条件概率<span class="math inline">\(P(y_{i-1}, y_i|x)\)</span>： <span class="math display">\[
P(y_{i-1}, y_i|x) =
\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1}, y_i|x)\beta_i(y_i|x)}{Z(x)}
= \frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1}, y_i|x}{\alpha_n^T(x) \cdot 1}
\]</span></p>
<p><strong>三、模型学习与维特比算法解码</strong></p>
<p><strong>3.1 <code>linear-CRF</code>模型参数学习思路</strong></p>
<p>在<code>linear-CRF</code>模型参数学习问题中，我们给定训练数据集<span class="math inline">\(X\)</span>和对应的标记序列<span class="math inline">\(Y\)</span>，<span class="math inline">\(K\)</span>个特征函数<span class="math inline">\(f_k(x, y)\)</span>，需要学习<code>linear-CRF</code>的模型参数<span class="math inline">\(w_k\)</span>和条件概率<span class="math inline">\(P_w(y|x)\)</span>，其中条件概率<span class="math inline">\(P_w(y|x)\)</span>和模型参数<span class="math inline">\(w_k\)</span>满足以下关系： <span class="math display">\[
P_w(y|x) = 
P(y|x) = 
\frac{1}{Z_w(x)}exp\sum_{k=1}^Kw_kf_k(x, y) 
= \frac{exp\sum_{k=1}^Kw_kf_k(x, y)}{\sum_y exp\sum_{k=1}^Kw_kf_k(x, y)}
\]</span> 我们的目标就是要求出所有的模型参数<span class="math inline">\(w_k\)</span>，我们可以使用梯度下降来进行求解。</p>
<p><strong>3.2 <code>linear-CRF</code>模型参数学习之梯度下降法求解</strong></p>
<p>首先需要定义优化函数，一般极大化条件分布<span class="math inline">\(P_w(y|x)\)</span>的对数似然函数如下： <span class="math display">\[
L(w) = log \prod_{x, y} P_w(y|x)^{\bar P(x, y)} = \sum_{x, y} \bar P(x, y)logP_w(y|x)
\]</span> 其中<span class="math inline">\(\bar P(x, y)\)</span>为经验分布，可以从先验知识和训练集样本中得到。下面我们极小化<span class="math inline">\(f(w) = -L(P_w)\)</span>： <span class="math display">\[
\begin{aligned}
f(w) &amp;= -\sum_{x, y} \bar P(x, y)logP_w(y | x) \\
&amp;= \sum_{x, y} \bar P(x, y)logZ_w(x) - \sum_{x, y} \bar P(x, y) \sum_{k=1}^Kw_kf_k(x, y) \\
&amp;= \sum_{x} \bar P(x)logZ_w(x) - \sum_{x, y} \bar P(x, y) \sum_{k=1}^Kw_kf_k(x, y) \\
&amp;= \sum_x \bar P(x)log\sum_y exp \sum_{k=1}^Kw_kf_k(x, y) - \sum_{x, y} \bar P(x, y) \sum_{k=1}^Kw_kf_k(x, y)
\end{aligned}
\]</span> 经过一系列变态的xxll变换后，我们对<span class="math inline">\(w\)</span>求导得： <span class="math display">\[
\frac{\partial f(w)}{\partial w} = \sum_{x, y} \bar P(x) P_w(y|x)f(x, y) - \sum_{x, y} \bar P (x, y)f(x, y)
\]</span> 然后就可以用梯度下降法来迭代求解最优的<span class="math inline">\(w\)</span>。在迭代过程中，每次更新<span class="math inline">\(w\)</span>后，都需要同步更新<span class="math inline">\(P_w(x, y)\)</span>，以用于下一次迭代的梯度计算。</p>
<p><strong>3.3 <code>linear-CRF</code>模型维特比算法解码思路</strong></p>
<p>在这个问题中，给定条件随机场的条件概率<span class="math inline">\(P(y|x)\)</span>和一个观测序列<span class="math inline">\(x\)</span>，要求出满足<span class="math inline">\(P(y|x)\)</span>最大序列<span class="math inline">\(y\)</span>。</p>
<p>维特比算法是一个动态规划算法，利用了两个局部状态和对应的递推公式，从局部递推到整体，进而得解。</p>
<p>对于<code>linear-CRF</code>的维特比算法，第一个局部状态定义为<span class="math inline">\(\delta_i(l)\)</span>，表示在位置<span class="math inline">\(i\)</span>标记<span class="math inline">\(l\)</span>各个可能取值<span class="math inline">\((1, 2, …, m)\)</span>对应的非规范化概率的最大值。根据<span class="math inline">\(\delta_i(l)\)</span>的定义，我们递推在位置<span class="math inline">\(i+1\)</span>标记<span class="math inline">\(l\)</span>的表达式为: <span class="math display">\[
\delta_{i+1}(l) = \underset{1 \le j \le m}{max} 
\{ \delta_i{(j)}  + \sum_{k=1}^K w_kf_k(y_i = j, y_{i+1} = l, x, i) \}, \ \ l = 1, 2, ..., m
\]</span> 同时我们需要另一个局部状态<span class="math inline">\(\psi_{i+1}(l)\)</span>来记录<span class="math inline">\(\delta_{i+1}(l)\)</span>达到最大的位置<span class="math inline">\(i\)</span>的标记取值，这个值用来最终回溯最优解，<span class="math inline">\(\psi_{i+1}(l)\)</span>的递推表达式为： <span class="math display">\[
\psi_{i+1}(l) =arg \ \  \underset{1 \le j \le m}{max} 
\{ \delta_i{(j)}  + \sum_{k=1}^K w_kf_k(y_i = j, y_{i+1} = l, x, i) \}, \ \ l = 1, 2, ..., m
\]</span> <strong>3.4 <code>linear-CRF</code>模型维特比算法流程</strong></p>
<p>输入：模型的<span class="math inline">\(K\)</span>个特征函数，和对应的<span class="math inline">\(K\)</span>个权重。观测序列<span class="math inline">\(x=(x_1, x_2, …, x_n)\)</span>，可能的标记个数<span class="math inline">\(m\)</span></p>
<p>输出：最优标记序列<span class="math inline">\(y^* = (y_1^*, y_2^*, …, y_n^*)\)</span></p>
<p>1）初始化： <span class="math display">\[
\begin{aligned}
\delta_1(l) = \sum_{k=1}^K w_kf_k(y_0 &amp;= start, y_1=l, x, i), \ \ l = 1, 2, ..., m \\
\psi_1(l) &amp;= start, \ \ l = 1, 2, ..., m
\end{aligned}
\]</span></p>
<p>2）对于<span class="math inline">\(i = 1, 2, …, n-1\)</span>， 进行递推： <span class="math display">\[
\delta_{i+1}(l) = \underset{1 \le j \le m}{max} 
\{ \delta_i{(j)}  + \sum_{k=1}^K w_kf_k(y_i = j, y_{i+1} = l, x, i) \}, \ \ l = 1, 2, ..., m
\]</span></p>
<p><span class="math display">\[
\psi_{i+1}(l) =arg \ \  \underset{1 \le j \le m}{max} 
\{ \delta_i{(j)}  + \sum_{k=1}^K w_kf_k(y_i = j, y_{i+1} = l, x, i) \}, \ \ l = 1, 2, ..., m
\]</span></p>
<p>3）终止： <span class="math display">\[
y_n^* = arg \ \ \underset{1 \le j \le m}{max} \delta_n(j)
\]</span></p>
<p>4）回溯： <span class="math display">\[
y_i^* = \psi_{i+1}(y_{i+1}^*), \ \ i = n -1, n-2, ..., 1)
\]</span> 最终得到最优标记序列<span class="math inline">\(y^* = (y_1^*, y_2^*, …, y_n^*)\)</span></p>
<p><strong>四、<code>linear-CRF</code> vs <code>HMM</code></strong></p>
<p><code>linear-CRF</code>和<code>HMM</code>都有很多相似之处，尤其是三个经典问题非常类似，除了模型参数学习的问题求解方法不同以外，概率估计问题和解码问题使用的算法思路基本上也是相同的。同时，两者都可以用于序列模型。在<code>NLP</code>领域下都被广泛使用。</p>
<p>而在不同点上，主要区别如下：</p>
<ol type="1">
<li><code>linear-CRF</code>是判别模型，主要是优化求解条件概率<span class="math inline">\(P(y|x)，\)</span>而<code>HMM</code>是生成模型，主要解的是联合分布<span class="math inline">\(P(x, y)\)</span>。</li>
<li><code>linear-CRF</code>利用最大熵模型的思路去建立条件概率模型，对于观测序列并没有做马尔科夫假设。而<code>HMM</code>是对观测序列做了马尔科夫假设的前提下建立联合分布模型。</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/26/HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/06/26/HMM/" itemprop="url">
                  Hidden Markov Model
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-26T21:32:44+08:00">
                2018-06-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>拖了有一段时间了，今天终于有机会来总结一下<code>HMM</code>，这是一个非常优美的算法。</p>
<p>下文将从<code>基础介绍</code>、<code>HMM的三个问题</code>以及<code>代码</code>一步步了解<code>HMM</code>世界。</p>
<p><strong>一、基础介绍</strong></p>
<p>在这一节中，主要从<code>形式定义</code>、<code>隐马尔科夫模型的两个基本假设</code>以及<code>举例</code>对<code>HMM</code>有一个初步的认识。</p>
<p><strong>1.形式定义</strong></p>
<p>首先看下面这张图，对模型有一个感性的认识。</p>
<p><img src="/images/HMM_1.png"></p>
<p>描述：</p>
<p>图分成两行，第一行是<code>y</code>序列，第二行是<code>x</code>序列，每个<code>x</code>都只有一个<code>y</code>指向它，而每个<code>y</code>都会有另一个<code>y</code>指向它。在这里，我们就引出了<code>HMM</code>的定义了：</p>
<ul>
<li><strong>状态序列（I）</strong>：隐藏的马尔科夫链随机生成的状态序列，成为<code>状态序列(state sequence)</code></li>
<li><strong>观测序列（O）</strong>：每个状态生成一个观测，而由此产生的观测随机序列，成为<code>观测序列(observation sequence)</code></li>
<li><strong>隐马尔科夫模型</strong>：隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔科夫链随机生成<strong>不可观测的状态随机序列</strong>，再由各个<strong>状态</strong>生成一个观测而<strong>产生观测随机序列</strong>的过程。</li>
</ul>
<p>下面我们用数学语言定义：</p>
<p>设<span class="math inline">\(Q\)</span>是所有可能状态的集合，<span class="math inline">\(V\)</span>是所有可能观测的集合。 <span class="math display">\[
Q= \{q_1, q_2, ..., q_N\}, \ \ \ \ V = \{v_1, v_2, ..., v_M\}
\]</span> 其中，<span class="math inline">\(N\)</span>是可能的状态数，<code>M</code>是可能的观测数。</p>
<p><span class="math inline">\(I\)</span>是长度为<span class="math inline">\(T\)</span>的状态序列，<span class="math inline">\(O\)</span>是对应的观测序列。 <span class="math display">\[
I= (i_1, i_2, ..., i_T), \ \ \ \ O = (o_1, o_2, ..., o_T)
\]</span> <strong>A是状态转移概率矩阵：</strong> <span class="math inline">\(A = [a_{ij}]_{N \times N}\)</span></p>
<p><span class="math inline">\(i = 1, 2, ..., N; \ \ j = 1, 2, ..., N\)</span></p>
<p>其中，在时刻<span class="math inline">\(t\)</span>，处于状态<span class="math inline">\(q_i\)</span>的条件下在时刻<span class="math inline">\(t+1\)</span>转移到状态<span class="math inline">\(q_j\)</span>的概率为： <span class="math display">\[
a_{ij} = P(i_{t+1} = q_j | i_t = q_i)
\]</span> <strong>B是观测概率矩阵：</strong><span class="math inline">\(B = [b_j(k)]_{N \times M}\)</span></p>
<p><span class="math inline">\(k = 1, 2, ..., M; j = 1, 2, ..., N\)</span></p>
<p>其中，在时刻<span class="math inline">\(t\)</span>，处于状态<span class="math inline">\(q_j\)</span>的条件下生成观测<span class="math inline">\(v_k\)</span>的概率为： <span class="math display">\[
b_j(k) = P(o_t  = v_k | i_t = q_j)
\]</span></p>
<p><strong><span class="math inline">\(\pi\)</span>是初始状态概率向量</strong>： <span class="math inline">\(\pi = (\pi_i)\)</span></p>
<p>也就是说，在时刻<span class="math inline">\(t=1\)</span>处于状态<span class="math inline">\(q_i\)</span>的概率为<span class="math inline">\(\pi_i = P(i_i = q_i), \  \ \ i = 1, 2, ..., N\)</span></p>
<p><strong>总结一下</strong>，<code>隐马尔可夫模型(HMM)</code>由<strong>初始状态概率向量<span class="math inline">\(\pi\)</span></strong>、<strong>状态转移概率矩阵<span class="math inline">\(A\)</span></strong>和<strong>观测概率矩阵</strong><span class="math inline">\(B\)</span>决定。<span class="math inline">\(\pi\)</span>和<span class="math inline">\(A\)</span>决定状态序列，<span class="math inline">\(B\)</span>决定观测序列。因此，隐马尔科夫模型<span class="math inline">\(\lambda\)</span>可以由三元符号表述，即 <span class="math display">\[
\lambda = (A, B, \pi)
\]</span> <span class="math inline">\(A\)</span>、<span class="math inline">\(B\)</span>、<span class="math inline">\(\pi\)</span>成为隐马尔科夫模型的<strong>三要素</strong>。</p>
<p><strong>2.隐马尔科夫模型的两个基本假设</strong></p>
<ul>
<li><p><strong>齐次马尔科夫性假设：</strong>假设隐马尔可夫链在任意时刻<span class="math inline">\(t\)</span>的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻<span class="math inline">\(t\)</span>无关，即 <span class="math display">\[
P(i_t|i_{t-1}, o_{t-1}, ..., i_1, o_1) = P(i_t |i_{t-1}), \ \ \ t = 1, 2, ..., T
\]</span></p></li>
<li><p><strong>观测独立性假设：</strong>假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。 <span class="math display">\[
P(o_t | i_T, o_T, i_{T-1}, o_{T-1}, ..., i_{t+1}, o_{t+1}, i_t, o_{t-1}, ..., i_1, o_1) = p(o_t | i_t)
\]</span></p></li>
</ul>
<p><strong>3.例子</strong></p>
<p>在<a href="https://en.wikipedia.org/wiki/Viterbi_algorithm#Example" target="_blank" rel="noopener">wikipidia</a>上有一个关于感冒预测的例子，我觉得非常好，在这里我尝试简单的描述：</p>
<p>假设Peter是一个医生，眼前有一个病人Sophia，Peter的任务是确定Sophia是否得了感冒。</p>
<p>接下来我们就去套用<code>HMM</code>的定义：</p>
<ul>
<li>病人的状态(<span class="math inline">\(Q\)</span>)有两种： {<code>感冒</code>、<code>没有感冒</code>}</li>
<li>病人的感觉(观测<span class="math inline">\(V\)</span>)有三种：{<code>正常</code>、<code>冷</code>、<code>头晕</code>}</li>
<li>目前Peter有Sophia的病例，他可以从病例的第一天确定<span class="math inline">\(\pi\)</span><strong>（初始状态概率向量）</strong>，也就是Sopia在第一天<code>感冒</code>和<code>没有感冒</code>的概率。</li>
<li>Peter也掌握了Sophia某天是否感冒(<span class="math inline">\(i_t\)</span>)与第二天是否感冒(<span class="math inline">\(i_{t+1}\)</span>)的关系<strong>（状态转移矩阵）</strong>。</li>
<li>Peter还掌握了Sophia某天的感觉与是否感冒的关系<strong>(观测概率矩阵)</strong>。</li>
</ul>
<p>下面我们用一张图来看看Dr.Peter掌握了什么信息。</p>
<p><img src="/images/HMM_2.png"></p>
<p>下面我们用代码来体现我们目前的定义。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># 状态集合Q</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2">states <span class="op">=</span> (<span class="st">&#39;Healthy&#39;</span>, <span class="st">&#39;Fever&#39;</span>)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co"># 观测结合V</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">observations <span class="op">=</span> (<span class="st">&#39;dizzy&#39;</span>, <span class="st">&#39;cold&#39;</span>, <span class="st">&#39;normal&#39;</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="co"># 初始状态概率向量 \pi</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">start_probability <span class="op">=</span> {<span class="st">&#39;Healthy&#39;</span>: <span class="fl">0.6</span>, <span class="st">&#39;Fever&#39;</span>: <span class="fl">0.4</span>}</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co"># 状态转移概率矩阵A</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8">transition_probability <span class="op">=</span> {</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">    <span class="st">&#39;Healthy&#39;</span>: {<span class="st">&#39;Healthy&#39;</span>: <span class="fl">0.7</span>, <span class="st">&#39;Fever&#39;</span>: <span class="fl">0.3</span>},</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">    <span class="st">&#39;Fever&#39;</span>: {<span class="st">&#39;Healthy&#39;</span>: <span class="fl">0.4</span>, <span class="st">&#39;Fever&#39;</span>: <span class="fl">0.6</span>},</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="co"># 观测概率矩阵B</span></a>
<a class="sourceLine" id="cb1-13" data-line-number="13">emission_probability <span class="op">=</span> {</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">    <span class="st">&#39;Healthy&#39;</span>: {<span class="st">&#39;normal&#39;</span>: <span class="fl">0.5</span>, <span class="st">&#39;cold&#39;</span>: <span class="fl">0.4</span>, <span class="st">&#39;dizzy&#39;</span>: <span class="fl">0.1</span>},</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">    <span class="st">&#39;Fever&#39;</span>: {<span class="st">&#39;normal&#39;</span>: <span class="fl">0.1</span>, <span class="st">&#39;cold&#39;</span>: <span class="fl">0.3</span>, <span class="st">&#39;dizzy&#39;</span>: <span class="fl">0.6</span>},</a>
<a class="sourceLine" id="cb1-16" data-line-number="16">}</a></code></pre></div>
<p>然后为了方便后续的计算，我们需要把数据源的map形式转换成矩阵的形式。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> generate_index_map(labels):</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    id2label <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">    label2id <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">    i <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">    <span class="cf">for</span> l <span class="kw">in</span> labels:</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">        id2label[i] <span class="op">=</span> l</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">        label2id[l] <span class="op">=</span> i</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">        i <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9">    <span class="cf">return</span> id2label, label2id</a>
<a class="sourceLine" id="cb2-10" data-line-number="10"></a>
<a class="sourceLine" id="cb2-11" data-line-number="11">states_id2label, states_label2id <span class="op">=</span> generate_index_map(states)</a>
<a class="sourceLine" id="cb2-12" data-line-number="12">observations_id2label, observations_label2id <span class="op">=</span> generate_index_map(observations)</a>
<a class="sourceLine" id="cb2-13" data-line-number="13"><span class="bu">print</span>(states_id2label, states_label2id)</a>
<a class="sourceLine" id="cb2-14" data-line-number="14"><span class="bu">print</span>(observations_id2label, observations_label2id)</a>
<a class="sourceLine" id="cb2-15" data-line-number="15">[output]</a>
<a class="sourceLine" id="cb2-16" data-line-number="16">{<span class="dv">0</span>: <span class="st">&#39;Healthy&#39;</span>, <span class="dv">1</span>: <span class="st">&#39;Fever&#39;</span>} {<span class="st">&#39;Fever&#39;</span>: <span class="dv">1</span>, <span class="st">&#39;Healthy&#39;</span>: <span class="dv">0</span>}</a>
<a class="sourceLine" id="cb2-17" data-line-number="17">{<span class="dv">0</span>: <span class="st">&#39;dizzy&#39;</span>, <span class="dv">1</span>: <span class="st">&#39;cold&#39;</span>, <span class="dv">2</span>: <span class="st">&#39;normal&#39;</span>} {<span class="st">&#39;cold&#39;</span>: <span class="dv">1</span>, <span class="st">&#39;normal&#39;</span>: <span class="dv">2</span>, <span class="st">&#39;dizzy&#39;</span>: <span class="dv">0</span>}</a>
<a class="sourceLine" id="cb2-18" data-line-number="18"></a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="kw">def</span> convert_map_to_vector(map_, label2id):</a>
<a class="sourceLine" id="cb2-20" data-line-number="20">    <span class="co">&quot;&quot;&quot;将初始状态概率向量从dict转移成array&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-21" data-line-number="21">    v <span class="op">=</span> np.zeros(<span class="bu">len</span>(map_), dtype<span class="op">=</span><span class="bu">float</span>)</a>
<a class="sourceLine" id="cb2-22" data-line-number="22">    <span class="cf">for</span> e <span class="kw">in</span> map_:</a>
<a class="sourceLine" id="cb2-23" data-line-number="23">        v[label2id[e]] <span class="op">=</span> map_[e]</a>
<a class="sourceLine" id="cb2-24" data-line-number="24">    <span class="cf">return</span> v</a>
<a class="sourceLine" id="cb2-25" data-line-number="25"></a>
<a class="sourceLine" id="cb2-26" data-line-number="26">pi <span class="op">=</span> convert_map_to_vector(start_probability, states_label2id)</a>
<a class="sourceLine" id="cb2-27" data-line-number="27"><span class="bu">print</span>(pi)</a>
<a class="sourceLine" id="cb2-28" data-line-number="28">[output]</a>
<a class="sourceLine" id="cb2-29" data-line-number="29">array([<span class="fl">0.6</span>, <span class="fl">0.4</span>])</a>
<a class="sourceLine" id="cb2-30" data-line-number="30"></a>
<a class="sourceLine" id="cb2-31" data-line-number="31"><span class="kw">def</span> convert_map_to_matrix(map_, label2id1, label2id2):</a>
<a class="sourceLine" id="cb2-32" data-line-number="32">    <span class="co">&quot;&quot;&quot;将观测概率和状态转移概率从dict转换成矩阵&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-33" data-line-number="33">    m <span class="op">=</span> np.zeros((<span class="bu">len</span>(label2id1), <span class="bu">len</span>(label2id2)), dtype<span class="op">=</span><span class="bu">float</span>)</a>
<a class="sourceLine" id="cb2-34" data-line-number="34">    <span class="cf">for</span> line <span class="kw">in</span> map_:</a>
<a class="sourceLine" id="cb2-35" data-line-number="35">        <span class="cf">for</span> col <span class="kw">in</span> map_[line]:</a>
<a class="sourceLine" id="cb2-36" data-line-number="36">            m[label2id1[line]][label2id2[col]] <span class="op">=</span> map_[line][col]</a>
<a class="sourceLine" id="cb2-37" data-line-number="37">    <span class="cf">return</span> m</a>
<a class="sourceLine" id="cb2-38" data-line-number="38"></a>
<a class="sourceLine" id="cb2-39" data-line-number="39">A <span class="op">=</span> convert_map_to_matrix(transition_probability, states_label2id, states_label2id)</a>
<a class="sourceLine" id="cb2-40" data-line-number="40">B <span class="op">=</span> convert_map_to_matrix(emission_probability, states_label2id, observations_label2id)</a>
<a class="sourceLine" id="cb2-41" data-line-number="41"><span class="bu">print</span>(A)</a>
<a class="sourceLine" id="cb2-42" data-line-number="42"><span class="bu">print</span>(B)</a>
<a class="sourceLine" id="cb2-43" data-line-number="43">[output]</a>
<a class="sourceLine" id="cb2-44" data-line-number="44">array([[<span class="fl">0.7</span>, <span class="fl">0.3</span>],</a>
<a class="sourceLine" id="cb2-45" data-line-number="45">       [<span class="fl">0.4</span>, <span class="fl">0.6</span>]])</a>
<a class="sourceLine" id="cb2-46" data-line-number="46"></a>
<a class="sourceLine" id="cb2-47" data-line-number="47">array([[<span class="fl">0.1</span>, <span class="fl">0.4</span>, <span class="fl">0.5</span>],</a>
<a class="sourceLine" id="cb2-48" data-line-number="48">       [<span class="fl">0.6</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>]])</a></code></pre></div>
<p>然后我们来看看如何生成观测序列。</p>
<p>根据隐马尔科夫模型定义，可以将一个长度为<span class="math inline">\(T\)</span>的观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>的生成过程描述如下：</p>
<p><strong>输入</strong>：隐马尔科夫模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>，观测序列长度<span class="math inline">\(T\)</span>；</p>
<p><strong>输出</strong>：观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>.</p>
<p>（1）按照初始状态分布<span class="math inline">\(\pi\)</span>产生状态<span class="math inline">\(i_1\)</span></p>
<p>（2）令<span class="math inline">\(t=1\)</span></p>
<p>（3）按照状态<span class="math inline">\(i_t\)</span>的观测概率分布<span class="math inline">\(b_{i_t}(k)\)</span>生成<span class="math inline">\(o_t\)</span></p>
<p>（4）按照状态<span class="math inline">\(i_t\)</span>的状态转移概率分布<span class="math inline">\({ai_ti_{t+1}}\)</span>产生状态<span class="math inline">\(i_{t+1}\)</span>， <span class="math inline">\(i_{t+1} = 1, 2, ..., N\)</span></p>
<p>（5）令<span class="math inline">\(t=t+1\)</span>；如果<span class="math inline">\(t&lt;T\)</span>，转步（3）；否则，终止</p>
<p>下面我们来用代码实现。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">def</span> simulate(T):</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    </a>
<a class="sourceLine" id="cb3-3" data-line-number="3">    <span class="kw">def</span> draw_from(probs):</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">        <span class="cf">return</span> np.where(np.random.multinomial(<span class="dv">1</span>, probs) <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">    </a>
<a class="sourceLine" id="cb3-6" data-line-number="6">    observations <span class="op">=</span> np.zeros(T, dtype<span class="op">=</span><span class="bu">int</span>)</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    states <span class="op">=</span> np.zeros(T, dtype<span class="op">=</span><span class="bu">int</span>)</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">    states[<span class="dv">0</span>] <span class="op">=</span> draw_from(pi)</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">    observations[<span class="dv">0</span>] <span class="op">=</span> draw_from(B[states[<span class="dv">0</span>], :])</a>
<a class="sourceLine" id="cb3-10" data-line-number="10">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">        states[t] <span class="op">=</span> draw_from(A[states[t<span class="dv">-1</span>],:])</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">        observations[t] <span class="op">=</span> draw_from(B[states[t], :])</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">    <span class="cf">return</span> observations, states</a>
<a class="sourceLine" id="cb3-14" data-line-number="14"></a>
<a class="sourceLine" id="cb3-15" data-line-number="15">observations_data, states_data <span class="op">=</span> simulate(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb3-16" data-line-number="16"><span class="bu">print</span>(states_data)</a>
<a class="sourceLine" id="cb3-17" data-line-number="17"><span class="bu">print</span>(observations_data)</a>
<a class="sourceLine" id="cb3-18" data-line-number="18">[output]</a>
<a class="sourceLine" id="cb3-19" data-line-number="19">[<span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span>]</a>
<a class="sourceLine" id="cb3-20" data-line-number="20">[<span class="dv">1</span> <span class="dv">2</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">1</span> <span class="dv">0</span>]</a></code></pre></div>
<p><strong>二、HMM的三个问题</strong></p>
<p><code>HMM</code>有三个基本问题：</p>
<ul>
<li><p>概率计算问题：给定模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>和观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，计算在模型<span class="math inline">\(\lambda\)</span>下观测序列<span class="math inline">\(O\)</span>出现的概率<span class="math inline">\(P(O|\lambda)\)</span>.</p></li>
<li><p>学习问题：<strong>已知观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，估计模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>，使<span class="math inline">\(P(O|\lambda)\)</span>最大。即用</strong>极大似然估计的方法估计参数。</p></li>
<li><p>预测问题（也称为解码（decoding）问题）：已知模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>和观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，求对给定观测序列条件概率<span class="math inline">\(P(I|O)\)</span>最大的状态序列<span class="math inline">\(I=(i_1, i_2, ..., i_T)\)</span>。即给定观测序列，求最有可能的对应状态序列。</p></li>
</ul>
<p>回到感冒的例子，这三个问题就是：</p>
<ul>
<li><p><strong>概率计算问题</strong>：如果给定模型<span class="math inline">\(\lambda=(A, B, \pi)\)</span>，计算Sophia出现特定观测序列的概率。</p></li>
<li><p><strong>学习问题</strong>：根据Sophia某一系列观测序列，学习模型参数。</p></li>
<li><p><strong>预测问题：</strong>根据学到的模型，通过Sophia的观测序列判断这几天是否有感冒。</p></li>
</ul>
<h4 id="概率计算问题"><strong>2.1概率计算问题</strong></h4>
<p>概率计算问题计算的是：在模型<span class="math inline">\(\lambda\)</span>下观测序列<span class="math inline">\(O\)</span>出现的概率<span class="math inline">\(P(O|\lambda)\)</span></p>
<p>在这里有两种方法：<strong>直接计算法</strong>和<strong>前向（或者后向算法）</strong></p>
<p><strong>直接计算法：</strong></p>
<p>对于状态序列<span class="math inline">\(I=(i_1, i_2, ..., iT)\)</span>的概率是： <span class="math display">\[
P(I|\lambda) = \pi_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_T-1i_T}
\]</span> 对上面这种状态序列，产生观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>的概率是： <span class="math display">\[
P(O|I, \lambda) = b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)
\]</span> <span class="math inline">\(I\)</span>和<span class="math inline">\(O\)</span>的联合概率为： <span class="math display">\[
P(O, I|\lambda) =P(O|I, \lambda) \cdot P(I|\lambda)  = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)
\]</span> 对所有可能的<span class="math inline">\(I\)</span>进行求和，得到： <span class="math display">\[
P(O|\lambda) = \sum_I P(O, I|\lambda) =\sum_{i_1, ..., i_T} \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)
\]</span> 如果直接计算，时间负责度太高，是<span class="math inline">\(O(TN^T)\)</span>。</p>
<p><strong><u>前向算法</u></strong>：</p>
<p>首先引入<strong>前向概率</strong>：</p>
<p>给定模型<span class="math inline">\(\lambda\)</span>，定义到时刻<span class="math inline">\(t\)</span>部分观测序列为<span class="math inline">\(o_1, o_2, ..., o_t\)</span>且状态为<span class="math inline">\(q_i\)</span>的概率为前向概率。记作： <span class="math display">\[
\alpha_t(i) = P(o_1, o_2, ..., o_t, i_t = q_t | \lambda)
\]</span></p>
<p>用感冒例子描述，就是某一天是否感冒以及这天<span class="math inline">\(t\)</span>及这天之前<span class="math inline">\((1—t)\)</span>所有观测数据的联合概率。</p>
<p><strong>观测序列概率的前向算法</strong></p>
<p><strong>输入：</strong>隐马尔科夫模型<span class="math inline">\(\lambda\)</span>，观测序列<span class="math inline">\(O\)</span>；</p>
<p><strong>输出：</strong>观测序列概率<span class="math inline">\(P(O|\lambda)\)</span></p>
<p>（1）初值 <span class="math display">\[
\alpha_i(i) = \pi_ib_i(o_i)
\]</span> 前向概率的定义中一共限定了两个条件，一是到当前为止的观测序列，另外一个是当前的状态。所以初值的计算也有两项（观测和状态），一项是初始状态概率，另一项是发射到当前观测的概率。</p>
<p>（2）递推 对<span class="math inline">\(t=1, 2, .., T-1\)</span> <span class="math display">\[
\alpha_{t+1}(i) =     \begin{bmatrix}
 \sum_{j=1}^N \alpha_t(j)a_{ji}
    \end{bmatrix} b_i(o_{i+1}), \ \ \ \ \ i = 1, 2, ... ,N
\]</span> 每次递推同样由两部分构成，大括号中是当前状态为<span class="math inline">\(i\)</span>且观测序列的前<span class="math inline">\(t\)</span>个符合要求的概率，括号外的是状态<span class="math inline">\(j\)</span>发射到观测<span class="math inline">\(t+1\)</span>的概率。</p>
<p>（3）终止 <span class="math display">\[
P(O|\lambda) = \sum_{i=1}^{N} \alpha_T(i)
\]</span></p>
<p>由于到了时间<span class="math inline">\(T\)</span>，一共有N中状态发射了最后的那个预测，所以最终的结果要将这些概率加起来。</p>
<p><strong>前向算法理解：</strong></p>
<p>前向算法使用前向概率的概念，记录每个时间下的前向概率，使得在递推计算下一个前向概率时，只需要上一个时间点的所有前向概率。减少计算量的原因在于每一次计算直接引用前一个时刻的计算机过，避免重复计算。原理上是用空间换时间。时间复杂度是<span class="math inline">\(O(N^2T)\)</span>。</p>
<p>下面我们来看代码的实现。</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">def</span> forward(obs_seq):</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;前向算法&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">    N <span class="op">=</span> A.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb4-4" data-line-number="4">    T <span class="op">=</span> <span class="bu">len</span>(obs_seq)</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">    </a>
<a class="sourceLine" id="cb4-6" data-line-number="6">    <span class="co"># F保存前向概率矩阵</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">    F <span class="op">=</span> np.zeros((N,T))</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">    F[:,<span class="dv">0</span>] <span class="op">=</span> pi <span class="op">*</span> B[:, obs_seq[<span class="dv">0</span>]]</a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, T):</a>
<a class="sourceLine" id="cb4-11" data-line-number="11">        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(N):</a>
<a class="sourceLine" id="cb4-12" data-line-number="12">            F[n,t] <span class="op">=</span> np.dot(F[:,t<span class="dv">-1</span>], (A[:,n])) <span class="op">*</span> B[n, obs_seq[t]]</a>
<a class="sourceLine" id="cb4-13" data-line-number="13"></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">    <span class="cf">return</span> F</a></code></pre></div>
<p><strong><u>后向算法</u></strong></p>
<p>给定隐马尔科夫模型<span class="math inline">\(\lambda\)</span>，定义在时刻<span class="math inline">\(t\)</span>，状态为<span class="math inline">\(q_i\)</span>的条件下，从<span class="math inline">\(t+1\)</span>到到<span class="math inline">\(T\)</span>的部分观测序列为<span class="math inline">\(o_{t+1}, o_{t+2}, ..., o_T\)</span>的概率为后向概率，记作 <span class="math display">\[
\beta_t(i) = P(o_{t+1}, o_{t+2}, ..., o_T|i_t = q_i, \lambda)
\]</span></p>
<p>可以用递推的方法求得后向概率<span class="math inline">\(\beta_t(i)\)</span>及观测序列概率<span class="math inline">\(P(O|\lambda)\)</span>。</p>
<p><strong>观测序列概率的后向算法</strong></p>
<p><strong>输入</strong>：隐马尔科夫模型<span class="math inline">\(\lambda\)</span>，观测序列<span class="math inline">\(O\)</span>；</p>
<p><strong>输出：</strong>观测序列概率<span class="math inline">\(P(O|\lambda)\)</span></p>
<p>（1）初值 <span class="math display">\[
\beta_T(i) = 1, \ \ \ i = 1, 2, ..., N
\]</span> 根据定义，从<span class="math inline">\(T+1\)</span>到<span class="math inline">\(T\)</span>的部分观测序列其实不存在，所以硬性规定这个值是1。</p>
<p>（2）对<span class="math inline">\(t=T-1, T-2, ..., 1\)</span> <span class="math display">\[
\beta_t(i) = \sum_{j=1}^{N} a_{ij}b_j(o_{i+1})\beta_{t+1}(j), \ \ \ i = 1, 2, ..., N
\]</span> <span class="math inline">\(a_{ij}\)</span>表示状态<span class="math inline">\(i\)</span>转移到状态<span class="math inline">\(j\)</span>的概率，<span class="math inline">\(b_j\)</span>表示发射<span class="math inline">\(o_{i+1}\)</span>的观测概率，<span class="math inline">\(\beta_{t+1}(j)\)</span>表示<span class="math inline">\(j\)</span>后面的序列对应的后向概率。</p>
<p>（3） <span class="math display">\[
P(O|\lambda) = \sum_{i=1}^{N}\pi_ib_i(o_1)\beta_i(i)
\]</span> 最后求和是因为，在第一个时间点上有<span class="math inline">\(N\)</span>中后向概率都能输出从2到<span class="math inline">\(T\)</span>的观察序列，所以乘上输出<span class="math inline">\(O_1\)</span>的概率后求和得到最终结果。</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">def</span> backward(obs_seq):</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;后向算法&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb5-3" data-line-number="3">    N <span class="op">=</span> A.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb5-4" data-line-number="4">    T <span class="op">=</span> <span class="bu">len</span>(obs_seq)</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">    <span class="co"># X保存后向概率矩阵</span></a>
<a class="sourceLine" id="cb5-6" data-line-number="6">    X <span class="op">=</span> np.zeros((N, T))</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">    X[:, <span class="dv">-1</span>:] <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb5-8" data-line-number="8">    </a>
<a class="sourceLine" id="cb5-9" data-line-number="9">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(T<span class="dv">-1</span>)):</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(N):</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">            X[n, t] <span class="op">=</span> np.<span class="bu">sum</span>(X[:, t<span class="op">+</span><span class="dv">1</span>] <span class="op">*</span> A[n, :] <span class="op">*</span> B[:, obs_seq[t<span class="op">+</span><span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">            </a>
<a class="sourceLine" id="cb5-13" data-line-number="13">    <span class="cf">return</span> X</a></code></pre></div>
<p>利用前向概率和后向概率的定义可以将观测序列概率<span class="math inline">\(P(O|\lambda)\)</span>统一写成： <span class="math display">\[
P(O|\lambda) = \sum_{i=1}^N\sum_{j=1}^N \alpha_t(i)a_{ij}b_j o_{t+1}\beta_{t+1}(j), \ \ \ t = 1, 2, ..., T -1
\]</span></p>
<h4 id="学习问题"><strong>2.2学习问题</strong></h4>
<p>关于学习问题就是，<strong>已知观测序列<span class="math inline">\(O=(o_1, o_2, ..., o_T)\)</span>，估计模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>，使<span class="math inline">\(P(O|\lambda)\)</span>最大。即用</strong>极大似然估计的方法估计参数。</p>
<p>隐马尔科夫模型的学习，分为监督学习与无监督学习：</p>
<ul>
<li>监督学习：训练数据包括观测序列和对应的状态序列。</li>
<li>无监督学习：训练数据只有观测序列。</li>
</ul>
<p><strong>监督学习方法</strong></p>
<p>假设已给训练数据包含<span class="math inline">\(S\)</span>个长度相同的观测序列和对应的状态序列<span class="math inline">\({(O_1, I_1), (O_2, I_2), ..., (O_S, I_S)}\)</span>，那么可以利用极大似然估计法来估计隐马尔科夫模型的参数。具体方法如下：</p>
<p><strong>1.转移概率<span class="math inline">\(a_{ij}\)</span>的估计</strong></p>
<p>设样本中时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(i\)</span>时刻<span class="math inline">\(t+1\)</span>转移到状态<span class="math inline">\(j\)</span>的频数为<span class="math inline">\(A_{ij}\)</span>，则状态转移概率<span class="math inline">\(a_{ij}\)</span>的估计是 <span class="math display">\[
\hat a_{ij} = \frac{A_{ij}}{\sum_{i=1}^NA_{ij}} \ \ \ i =1 , 2, ..., N , \ \ \ j = 1, 2, ..., N
\]</span></p>
<p><strong>2.观测概率<span class="math inline">\(b_j(k)\)</span>的估计</strong></p>
<p>设样本中状态为<span class="math inline">\(j\)</span>并观测为<span class="math inline">\(k\)</span>的频数为<span class="math inline">\(B_{jk}\)</span>，那么状态为<span class="math inline">\(j\)</span>观测为<span class="math inline">\(k\)</span>的概率估计是 <span class="math display">\[
\hat b_j(k) = \frac{B_{jk}}{\sum_{k=1}^MB_{jk}}
\]</span></p>
<p><strong>3.初始状态概率<span class="math inline">\(\pi_i\)</span>的估计<span class="math inline">\(\hat \pi_i\)</span>为<span class="math inline">\(S\)</span>个样本中初始状态为<span class="math inline">\(q_i\)</span>的频率</strong></p>
<p>由于监督学习需要使用训练数据，而人工标注训练数据往往代价很高，有时就会利用非监督学习。</p>
<p><strong>Baum-Welch算法</strong></p>
<p>假设给定训练数据只包含<span class="math inline">\(S\)</span>个长度为<span class="math inline">\(T\)</span>的观测序列<span class="math inline">\({O_1, O_2, ..., O_S}\)</span>而没有对应的状态序列，目标是学习隐马尔科夫模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>的参数。我们将观测序列数据看作观测数据<span class="math inline">\(O\)</span>，状态序列数据看作不可观测的隐数据<span class="math inline">\(I\)</span>，则隐马尔科夫模型就能够抽象成一个含有隐变量的概率模型 <span class="math display">\[
P(O|\lambda) = \sum_t P(O|I, \lambda) P (I|\lambda)
\]</span> 它的参数学习可以由<code>EM算法</code>实现。</p>
<p><strong>1.确定完全数据的对数似然函数</strong></p>
<p>所有观测数据写成<span class="math inline">\(O = (o_1, o_2, ..., o_T)\)</span>，所有隐数据写成<span class="math inline">\(I=(i_1, i_2, ..., i_T)\)</span>，完全数据是<span class="math inline">\((O, I) = (o_1, o_2, ..., o_T, i_1, i_2, ..., i_T)\)</span>。完全数据的对数似然函数是<span class="math inline">\(logP(O,I|\lambda)\)</span>。</p>
<p><strong>2.EM算法的E步：求Q函数</strong><span class="math inline">\(Q(\lambda, \bar \lambda)\)</span> <span class="math display">\[
Q(\lambda, \bar \lambda) = \sum_I logP(O, I|\lambda) P(O, I | \bar \lambda)
\]</span></p>
<p>其中，<span class="math inline">\(\bar \lambda\)</span>是隐马尔科夫模型参数的当前估计值，<span class="math inline">\(\lambda\)</span>是要极大化的隐马尔科夫模型参数。而 <span class="math display">\[
P(O,I|\lambda) = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_T-1i_T}b_T(o_T)
\]</span> 于是函数<span class="math inline">\(Q(\lambda, \bar \lambda)\)</span>可以写成：</p>
<p><span class="math display">\[
\begin{aligned}
Q(\lambda, \bar \lambda)  &amp;= \sum_Ilog\pi_{i_1}P(O, I | \bar \lambda) \\\\
&amp; +\sum_I\Biggl(\sum_{t=1}^{T-1}loga_{i_1i_{i+1}}\Biggr)P(O,I|
\bar \lambda)  + \sum_I\Biggl(\sum_{t=1}^{T}logb_{i_1}(o_t)\Biggr) P (O,I|\bar \lambda)
\end{aligned}
\]</span></p>
<p><strong>3.EM算法的M步：极大化<span class="math inline">\(Q\)</span>函数<span class="math inline">\(Q(\lambda, \bar \lambda)\)</span>求模型参数<span class="math inline">\(A, B, \pi\)</span>。</strong></p>
<p>由于要极大化的参数在Q函数表达式中单独出现在3个项中，所以只需对各项分别极大化。</p>
<p><strong>第1项可以写成：</strong> <span class="math display">\[
\sum_Ilog\pi_{i_0}P(O, I | \bar \lambda) = \sum_{i=1}^Nlog\pi_iP(O, i_1= i | \lambda)
\]</span></p>
<p>注意到<span class="math inline">\(\pi_i\)</span>满足约束条件<span class="math inline">\(\sum_{i=1}^{N}\pi_i=1\)</span>，利用拉格朗日乘子法，写出拉格朗日函数： <span class="math display">\[
\sum_{i=1}^N log\pi_i P(O, i_1 = i | \bar \lambda) + \gamma\biggl(\sum_{i=1}^N\pi_i - 1\biggr)
\]</span></p>
<p>对其求偏导并令结果为0 <span class="math display">\[
\frac{\partial}{\partial \pi_i}[\sum_{i=1}^N log\pi_i P(O, i_1 = i | \bar \lambda) + \gamma\biggl(\sum_{i=1}^N\pi_i - 1\biggr)] = 0
\]</span></p>
<p>得到 <span class="math display">\[
P(O, i_1 = 1| \bar \lambda)  + \gamma\pi_i = 0
\]</span> 对<span class="math inline">\(i\)</span>求和得到<span class="math inline">\(\gamma\)</span> <span class="math display">\[
\gamma = - P(O|\bar \lambda)
\]</span></p>
<p>代入<span class="math inline">\(P(O, i_1 = 1| \bar \lambda) + \gamma\pi_i = 0\)</span>中得到 <span class="math display">\[
\pi_i = \frac{P(O, i_1 = i|\bar \lambda)}{P(O|\bar \lambda)}
\]</span></p>
<p><strong>第2项可以写成</strong> <span class="math display">\[
\sum_I\Biggl(\sum_{t=1}^{T-1}loga_{i_1i_{i+1}}\Biggr)P(O,I|
\bar \lambda) = \sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}log a_{ij}P(O, i_t = i, i_{t+1} = j \ |\  \bar \lambda)
\]</span></p>
<p>应用约束条件<span class="math inline">\(\sum_{j=1}^{N}a_{ij} =1\)</span>，利用拉格朗日乘子法，可以求出 <span class="math display">\[
a_{ij} = \frac{\sum_{t=1}^{T-1} P (O, i_t = 1, i_{t+1} = j \ | \ \bar \lambda)}{\sum^{T-1}_{t=1}P(O, i_t=i \ | \ \bar \lambda)}
\]</span></p>
<p><strong>第3项可以写成</strong> <span class="math display">\[
\sum_I\Biggl(\sum_{t=1}^{T}logb_{i_1}(o_t)\Biggr) P (O,I|\bar \lambda)  = \sum_{j=1}^{N}\sum_{t=1}^Tlogb_j(o_t) P(O, i_t = j \ | \ \bar \lambda )
\]</span> 应用约束条件<span class="math inline">\(\sum_{k=1}^{N}b_{j}(k) =1\)</span>，利用拉格朗日乘子法（注意，只有在<span class="math inline">\(o_t = v_k\)</span>时<span class="math inline">\(b_j(o_t)\)</span>对<span class="math inline">\(b_j(k)\)</span>的偏导数才不为0，以<span class="math inline">\(I(o_t = v_k)表示\)</span>），求出 <span class="math display">\[
b_j(k) = \frac{\sum_{t=1}^{T}P(O, i_t = j \ | \ \bar \lambda) I (o_t = v_k)}{\sum_{t=1}^{T} P (O, i_t = j \ | \ \bar \lambda)}
\]</span> <strong>Baum-Welch模型参数估计公式</strong></p>
<p>将这三个式子中的各概率分别简写如下： <span class="math display">\[
a_{ij} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1}\gamma_t(i)}
\]</span></p>
<p><span class="math display">\[
b_j(k) = \frac{\sum_{t=1,o_t=v_k}^T \gamma_t(j)}{\sum_{t=1}^T \gamma_t(j)}
\]</span></p>
<p><span class="math display">\[
\pi_i = \gamma_1(i)
\]</span></p>
<p>我们定义一个概率<span class="math inline">\(\gamma\)</span>， 它表示，给定模型参数和所有预测，时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_i\)</span>的概率。 <span class="math display">\[
\begin{aligned}
\gamma_t(i) &amp;= P(i_t = q_i | O, \lambda) \\\\
&amp; = \frac{P(i_t=q_i, O|\lambda)}{P(O|\lambda)}\\\\
&amp; =\frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^{N}\alpha_t(j)\beta_t(j)}
\end{aligned}
\]</span> 我们定义一个概率<span class="math inline">\(\xi\)</span>，它表示，给定模型参数和所有观测，时刻<span class="math inline">\(t\)</span>处于状态<span class="math inline">\(q_i\)</span>且时刻<span class="math inline">\(t+1\)</span>处于状态<span class="math inline">\(q_j\)</span>的概率。 <span class="math display">\[
\begin{aligned}
\xi_t(i, j) &amp;= P(i_t = q_i, i_{t+1} =q_j | O, \lambda) \\\\
&amp; =\frac{P(i_t = q_i, i_{t+1} = q_j , O \ | \ \lambda) }{P(O|\lambda)} \\\\
&amp; = \frac{P(i_t = q_i , i_{t+1} = q_j, O | \lambda)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(i_t = q_i, i_{t+1} = q_j, O \ | \ \lambda)}  \\\\
&amp; = \frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^{N}\sum_{j=1}^{N}P(i_t = q_i, i_{t+1} = q_j, O \ | \ \lambda)}
\end{aligned}
\]</span> 然后我们来看看<strong>Baum-Welch</strong>算法的Python实现</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">def</span> baum_welch_train(observations, A, B, pi, criterion<span class="op">=</span><span class="fl">0.05</span>):</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;无监督学习算法——Baum-Weich算法&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">    n_states <span class="op">=</span> A.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb6-4" data-line-number="4">    n_samples <span class="op">=</span> <span class="bu">len</span>(observations)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5"></a>
<a class="sourceLine" id="cb6-6" data-line-number="6">    done <span class="op">=</span> <span class="va">False</span></a>
<a class="sourceLine" id="cb6-7" data-line-number="7">    <span class="cf">while</span> <span class="kw">not</span> done:</a>
<a class="sourceLine" id="cb6-8" data-line-number="8">        <span class="co"># alpha_t(i) = P(O_1 O_2 ... O_t, q_t = S_i | hmm)</span></a>
<a class="sourceLine" id="cb6-9" data-line-number="9">        <span class="co"># Initialize alpha</span></a>
<a class="sourceLine" id="cb6-10" data-line-number="10">        alpha <span class="op">=</span> forward(observations)</a>
<a class="sourceLine" id="cb6-11" data-line-number="11"></a>
<a class="sourceLine" id="cb6-12" data-line-number="12">        <span class="co"># beta_t(i) = P(O_t+1 O_t+2 ... O_T | q_t = S_i , hmm)</span></a>
<a class="sourceLine" id="cb6-13" data-line-number="13">        <span class="co"># Initialize beta</span></a>
<a class="sourceLine" id="cb6-14" data-line-number="14">        beta <span class="op">=</span> backward(observations)</a>
<a class="sourceLine" id="cb6-15" data-line-number="15">        <span class="co"># ξ_t(i,j)=P(i_t=q_i,i_{i+1}=q_j|O,λ)</span></a>
<a class="sourceLine" id="cb6-16" data-line-number="16">        xi <span class="op">=</span> np.zeros((n_states,n_states,n_samples<span class="dv">-1</span>))</a>
<a class="sourceLine" id="cb6-17" data-line-number="17">        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(n_samples<span class="dv">-1</span>):</a>
<a class="sourceLine" id="cb6-18" data-line-number="18">            denom <span class="op">=</span> np.dot(np.dot(alpha[:,t].T, A) <span class="op">*</span> B[:,observations[t<span class="op">+</span><span class="dv">1</span>]].T, beta[:,t<span class="op">+</span><span class="dv">1</span>])</a>
<a class="sourceLine" id="cb6-19" data-line-number="19">            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_states):</a>
<a class="sourceLine" id="cb6-20" data-line-number="20">                numer <span class="op">=</span> alpha[i,t] <span class="op">*</span> A[i,:] <span class="op">*</span> B[:,observations[t<span class="op">+</span><span class="dv">1</span>]].T <span class="op">*</span> beta[:,t<span class="op">+</span><span class="dv">1</span>].T</a>
<a class="sourceLine" id="cb6-21" data-line-number="21">                xi[i,:,t] <span class="op">=</span> numer <span class="op">/</span> denom</a>
<a class="sourceLine" id="cb6-22" data-line-number="22"></a>
<a class="sourceLine" id="cb6-23" data-line-number="23">        <span class="co"># γ_t(i)：gamma_t(i) = P(q_t = S_i | O, hmm)</span></a>
<a class="sourceLine" id="cb6-24" data-line-number="24">        gamma <span class="op">=</span> np.<span class="bu">sum</span>(xi,axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb6-25" data-line-number="25">        <span class="co"># Need final gamma element for new B</span></a>
<a class="sourceLine" id="cb6-26" data-line-number="26">        <span class="co"># xi的第三维长度n_samples-1，少一个，所以gamma要计算最后一个</span></a>
<a class="sourceLine" id="cb6-27" data-line-number="27">        prod <span class="op">=</span>  (alpha[:,n_samples<span class="dv">-1</span>] <span class="op">*</span> beta[:,n_samples<span class="dv">-1</span>]).reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb6-28" data-line-number="28">        gamma <span class="op">=</span> np.hstack((gamma,  prod <span class="op">/</span> np.<span class="bu">sum</span>(prod))) <span class="co">#append one more to gamma!!!</span></a>
<a class="sourceLine" id="cb6-29" data-line-number="29">        </a>
<a class="sourceLine" id="cb6-30" data-line-number="30">        <span class="co"># 更新模型参数</span></a>
<a class="sourceLine" id="cb6-31" data-line-number="31">        newpi <span class="op">=</span> gamma[:,<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb6-32" data-line-number="32">        newA <span class="op">=</span> np.<span class="bu">sum</span>(xi,<span class="dv">2</span>) <span class="op">/</span> np.<span class="bu">sum</span>(gamma[:,:<span class="op">-</span><span class="dv">1</span>],axis<span class="op">=</span><span class="dv">1</span>).reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb6-33" data-line-number="33">        newB <span class="op">=</span> np.copy(B)</a>
<a class="sourceLine" id="cb6-34" data-line-number="34">        num_levels <span class="op">=</span> B.shape[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb6-35" data-line-number="35">        sumgamma <span class="op">=</span> np.<span class="bu">sum</span>(gamma,axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb6-36" data-line-number="36">        <span class="cf">for</span> lev <span class="kw">in</span> <span class="bu">range</span>(num_levels):</a>
<a class="sourceLine" id="cb6-37" data-line-number="37">            mask <span class="op">=</span> observations <span class="op">==</span> lev</a>
<a class="sourceLine" id="cb6-38" data-line-number="38">            newB[:,lev] <span class="op">=</span> np.<span class="bu">sum</span>(gamma[:,mask],axis<span class="op">=</span><span class="dv">1</span>) <span class="op">/</span> sumgamma</a>
<a class="sourceLine" id="cb6-39" data-line-number="39">        </a>
<a class="sourceLine" id="cb6-40" data-line-number="40">        <span class="co"># 检查是否满足阈值</span></a>
<a class="sourceLine" id="cb6-41" data-line-number="41">        <span class="cf">if</span> np.<span class="bu">max</span>(<span class="bu">abs</span>(pi <span class="op">-</span> newpi)) <span class="op">&lt;</span> criterion <span class="kw">and</span> <span class="op">\</span></a>
<a class="sourceLine" id="cb6-42" data-line-number="42">                        np.<span class="bu">max</span>(<span class="bu">abs</span>(A <span class="op">-</span> newA)) <span class="op">&lt;</span> criterion <span class="kw">and</span> <span class="op">\</span></a>
<a class="sourceLine" id="cb6-43" data-line-number="43">                        np.<span class="bu">max</span>(<span class="bu">abs</span>(B <span class="op">-</span> newB)) <span class="op">&lt;</span> criterion:</a>
<a class="sourceLine" id="cb6-44" data-line-number="44">            done <span class="op">=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb6-45" data-line-number="45">        A[:], B[:], pi[:] <span class="op">=</span> newA, newB, newpi</a>
<a class="sourceLine" id="cb6-46" data-line-number="46">    <span class="cf">return</span> newA, newB, newpi</a></code></pre></div>
<p>回到预测感冒的问题，下面我们先自己建立一个HMM模型，再模拟出一个观测序列和一个状态序列。</p>
<p>然后，只用观测序列去学习模型，获得模型参数。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1">A <span class="op">=</span> np.array([[<span class="fl">0.5</span>, <span class="fl">0.5</span>],[<span class="fl">0.5</span>, <span class="fl">0.5</span>]])</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">B <span class="op">=</span> np.array([[<span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>],[<span class="fl">0.3</span>, <span class="fl">0.3</span>, <span class="fl">0.3</span>]])</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">pi <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="fl">0.5</span>])</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"></a>
<a class="sourceLine" id="cb7-5" data-line-number="5">observations_data, states_data <span class="op">=</span> simulate(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">newA, newB, newpi <span class="op">=</span> baum_welch_train(observations_data, A, B, pi)</a>
<a class="sourceLine" id="cb7-7" data-line-number="7"><span class="bu">print</span>(<span class="st">&quot;newA: &quot;</span>, newA)</a>
<a class="sourceLine" id="cb7-8" data-line-number="8"><span class="bu">print</span>(<span class="st">&quot;newB: &quot;</span>, newB)</a>
<a class="sourceLine" id="cb7-9" data-line-number="9"><span class="bu">print</span>(<span class="st">&quot;newpi: &quot;</span>, newpi)</a>
<a class="sourceLine" id="cb7-10" data-line-number="10">[output]</a>
<a class="sourceLine" id="cb7-11" data-line-number="11">newA:  [[<span class="fl">0.5</span> <span class="fl">0.5</span>]</a>
<a class="sourceLine" id="cb7-12" data-line-number="12"> [<span class="fl">0.5</span> <span class="fl">0.5</span>]]</a>
<a class="sourceLine" id="cb7-13" data-line-number="13">newB:  [[<span class="fl">0.26</span> <span class="fl">0.38</span> <span class="fl">0.36</span>]</a>
<a class="sourceLine" id="cb7-14" data-line-number="14"> [<span class="fl">0.26</span> <span class="fl">0.38</span> <span class="fl">0.36</span>]]</a>
<a class="sourceLine" id="cb7-15" data-line-number="15">newpi:  [<span class="fl">0.5</span> <span class="fl">0.5</span>]</a></code></pre></div>
<h4 id="预测问题"><strong>2.3预测问题</strong></h4>
<p>考虑到预测问题是求给定预测序列条件概率<span class="math inline">\(P(I|O)\)</span>最大的状态序列<span class="math inline">\(I = (i_1, i_2, ..., i_T)\)</span>，类比这个问题和最短路径问题：</p>
<p>我们可以把求<span class="math inline">\(P(I|O)\)</span>的最大值类比成求节点间距离的最小值，于是考虑<strong>类似于动态规划的viterbi算法</strong>。</p>
<p>首先导入两个变量$<span class="math inline">\(和\)</span>$</p>
<p>定义<strong>在时刻<span class="math inline">\(t\)</span>状态为<span class="math inline">\(i\)</span>的所有单个路径</strong><span class="math inline">\((i_1, i_2, ..., i_t)\)</span>中概率值最大为 <span class="math display">\[
\delta_t(i) = max_{i_1, i_2, ..., i_{t-1}} P(i_t= i , i_{t-1} , ...,  i_1, o_t, ..., o_1 | \lambda) \ \ \ i = 1, 2, ..., N
\]</span> 根据定义，可得递推公式 <span class="math display">\[
\begin{aligned}
\delta_{t+1}(i) &amp;= max_{i_1, i_2, ..., i_t} P(i_{t+1}= i , i_{t} , ...,  i_1, o_t, ..., o_1 | \lambda) \\\\
&amp; = max_{1\le j \le N}[\delta_{t}(j)a_{ji}]b_i(o_{t+1})
\end{aligned}
\]</span> 定义<strong>在时刻<span class="math inline">\(t\)</span>状态为<span class="math inline">\(i\)</span>的所有单个路径</strong><span class="math inline">\((i_1, i_2, ..., i_t)\)</span>中概率值最大的路径的第<span class="math inline">\(t-1\)</span>个结点为 <span class="math display">\[
\psi_t(i)  =argmax_{\ 1 \le j \le N} [\delta_{t-1}(j)a_{ji}] \ \ \ \ i = 1, 2, ..., N
\]</span> <strong><u>维特比(viterbi)算法(动态规划）</u></strong></p>
<p><strong>输入：</strong>模型<span class="math inline">\(\lambda = (A, B, \pi)\)</span>和观测<span class="math inline">\(O = (o_1, o_2, ..., o_T)\)</span></p>
<p><strong>输出</strong>：最优路径<span class="math inline">\(I* = (i^*_i, i^*_2, ..., i^*_T)\)</span></p>
<p>（1）初始化 <span class="math display">\[
\delta_1(i) = \pi_ib_i(o_1)
\]</span></p>
<p><span class="math display">\[
\psi_1(i) = 0
\]</span></p>
<p>（2）递推. 对<span class="math inline">\(t=2, 3, ..., T\)</span> <span class="math display">\[
\delta_{t}(i) = max_{1\le j \le N}[\delta_{t-1}(j)a_{ji}]b_i(o_{t})  \ \ \ \ i = 1, 2, ..., N
\]</span></p>
<p><span class="math display">\[
\psi_t(i)  =argmax_{1 \le j \le N} [\delta_{t-1}(j)a_{ji}] \ \ \ \ i = 1, 2, ..., N
\]</span></p>
<p>（3）终止 <span class="math display">\[
P^* = max_{1 \le i \le N } \delta _T(i)
\]</span></p>
<p><span class="math display">\[
i^*_T = argmax_{1 \le i \le N}[\delta_T(i)]
\]</span></p>
<p>（4）最优路径回溯， 对<span class="math inline">\(t=T-1, T-2, ..., 1\)</span> <span class="math display">\[
i^*_t = \psi_{t+1} (i^*_{t+1})
\]</span></p>
<p>求得最优路径<span class="math inline">\(I* = (i^*_1, i^*_2, ..., i^*_T)\)</span></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/16/EM-Algorithm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/06/16/EM-Algorithm/" itemprop="url">
                  EM Algorithm
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-16T17:16:35+08:00">
                2018-06-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天总结一下<code>EM</code>算法，这是一个非常有趣的算法。</p>
<p>下文将从<code>知识准备</code>、<code>EM算法介绍</code>、<code>EM推导过程</code>以及<code>EM和K-means的联系</code>开展对<code>EM</code>算法讨论。</p>
<p><strong>一、知识准备</strong></p>
<p>在介绍<code>EM</code>算法前，我们需要先回顾以下几个公式：<code>贝叶斯公式</code>、<code>全概率</code>、<code>联合概率</code>、<code>极大似然估计</code>、<code>Jensen不等式</code>和<code>数学期望</code>。</p>
<p><strong>1.贝叶斯</strong></p>
<p>简单来看，贝叶斯可以用以下公式表示： <span class="math display">\[
P(A\ |\ B) = \frac{P(A) \times P(B\ |\ A)}{P(B)}
\]</span> 更一般，有 <span class="math display">\[
P(A_i\ |\ B) = \frac{P(B\ |\ A_i) \  P(A_i)}{\sum_j P(B\ |\ A_j) \ P(A_j)}
\]</span> <strong>2.全概率</strong></p>
<p><span class="math display">\[
P(A) = \sum_n P(A \ | \ B_n) \ P(B_n)
\]</span> <strong>3.联合概率（似然函数）</strong> <span class="math display">\[
L(\theta) = L(x_1, ..., x_n;\theta) = \prod _{i=1}^{n} p (x_i;\theta)
\]</span> <strong>4.极大似然估计</strong> <span class="math display">\[
\begin{aligned}
 \widehat \theta &amp;= argmax L(\theta) \\\\
H(\theta) = lnL(\theta) &amp;= ln \prod_{i=1}^{n}p(x_i;\theta) = \sum_{i=1}^{n}ln \ p(x_i; \theta)
\end{aligned}
\]</span> <strong>5.Jensen不等式</strong></p>
<p>设 <span class="math inline">\(f\)</span> 是定义域为实数的函数，如果对于所有的实数<span class="math inline">\(x\)</span>，<span class="math inline">\(f(x)\)</span>的二次导数大于等于0，则 <span class="math inline">\(f\)</span> 为凸函数。</p>
<p>Jensen不等式给出了积分的凸函数值和凸函数的积分值之间的关系。过一个凸函数上任意两点所作割线一定在这两点间的函数图像的上方。即： <span class="math display">\[
tf(x_1) + (1-t)f(x_2) \ge f(tx_1 + (1-t)x_2), \ \ 0 \le t \le 1.
\]</span> 概率论版本 <span class="math display">\[
E(f(X)) \ge f(E(X))
\]</span> 如果 <span class="math inline">\(f\)</span> 是严格凸函数，那么只有当 <span class="math inline">\(X = E[X]\)</span> 恒成立时，上式取等号（即<span class="math inline">\(X\)</span>是一个常量），或者说当<span class="math inline">\(X = E[X]\)</span>成立的概率是1时，上式取等号。</p>
<p><strong>6.数学期望</strong></p>
<p>如果<span class="math inline">\(X\)</span>是离散随机变量，输出值为$x_1, x_2, …, $ 和输出值相应的概率为<span class="math inline">\(p_1, p_2, ...\)</span>，如果<span class="math inline">\(\sum_ip_ix_i\)</span>绝对收敛，那么期望值<span class="math inline">\(E[X]\)</span>是一个无限数列的和。 <span class="math display">\[
E[X] = \sum_i p_ix_i
\]</span> 如果<span class="math inline">\(X\)</span>是连续随机变量，存在一个相应的概率密度函数（PDF）<span class="math inline">\(f(x)\)</span>，数学期望为： <span class="math display">\[
E[X] = \int xf(x) dx
\]</span> <strong>二、EM算法介绍</strong></p>
<p>之前看到一个介绍EM算法很生动的对话。首先看看做机器学习理论的学姐如何回答。</p>
<p>Q：学姐学姐，EM算法是什么呢？</p>
<p>A：EM算法啊，就是解决包含隐变量的参数估计问题。</p>
<p>Q：什么是隐变量啊？</p>
<p>A：隐变量主要是指那些不能被直接观察到，但是对系统的状态和能观察到的输出存在影响的变量。隐变量可以通过使用数学模型，依据观察得到的数据推断出来。</p>
<p>下面来看看一位做工程的学长是如何回答</p>
<p>Q：学长学长，EM算法是什么呢？</p>
<p>A：EM算法就那样啊，就是先用有标签的样本训练一个分类器，再给未知标签的样本贴标签，然后再拿全部样本再训练分类器，就这样来回倒腾</p>
<p><code>EM</code>算法全称是<strong>Expectation maximization</strong>，<code>EM</code>在概率模型中寻找参数极大似然估计或者极大后验估计，其中概率模型依赖于无法观测的隐变量。</p>
<p>EM提供解决问题的通用框架，例子包括：</p>
<ul>
<li><strong>半监督学习</strong>：即利用包含缺失类别标签的数据的混合数据集训练分类器。</li>
<li><strong>数据预处理</strong>：给缺失某一维特征的值的数据补上缺失值。</li>
<li><strong>聚类</strong>：对，聚类。</li>
<li><p><strong>隐马尔科夫模型</strong>：训练隐马尔科夫模型中的参数。</p></li>
<li><p>…</p></li>
</ul>
<p><code>EM</code>算法是一个在已知部分相关变量的情况下，估计未知变量的迭代算法。每次迭代由两步组成：</p>
<ol type="1">
<li>E步：计算期望（Expectation）。利用对隐藏变量的现有估计值，计算其极大似然估计。</li>
<li>M步：最大化（M），最大化在E步上求得的极大似然估计值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</li>
</ol>
<p><strong>三、EM算法推导</strong></p>
<p><code>EM</code>算法的目标是<strong>使包含隐变量的数据集的后验概率或似然函数最大化，进而得到最有的参数估计</strong>。</p>
<p>下面我们来看加入了隐变量后的似然函数。 <span class="math display">\[
\begin{aligned}
\ell(\theta)  &amp; = \sum_{i=1}^{m}\  log\  p(x; \theta) \\\\
&amp; = \sum_{i=1}^{m} \ log \ \sum_z \ p(x, z;\theta)
\end{aligned}
\]</span> 因为有隐变量 <span class="math inline">\(z\)</span> 的存在，所以我们无法直接计算第二步，仍然需要继续推导。</p>
<p>对于每个 <span class="math inline">\(i\)</span> ，我们来令 <span class="math inline">\(Q_i\)</span> 作为隐变量 <span class="math inline">\(z\)</span> 的概率分布（其中<span class="math inline">\(\sum_zQ_i(z) = 1, \ Q_i(z) \ge 0\)</span>） <span class="math display">\[
\begin{aligned}
\sum_i log \ p(x^{(i)}; \theta) &amp; = \sum_i log \sum_{z^{(i)}} p (x^{(i)}, z^{(i)}; \theta) \\\\
&amp;= \sum_i log \sum_{z^{i}} Q_i (z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
\end{aligned}
\]</span> 这时候就出现了数学期望的形式了。下面我们作一些表示简化。</p>
<p><span class="math display">\[
\begin{aligned}
Q &amp;= Q_i(z^{(i)}) \\\\
Y&amp;=\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \\\\
P&amp;(Y=\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}) = Q \\\\
\end{aligned}
\]</span></p>
<p>则有</p>
<p><span class="math display">\[
log\sum_Z Q \cdot \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q} = log\sum_Y P(Y)Y = logE(Y)
\]</span></p>
<p>构造好数学期望的形式之后，因为<code>log</code>函数是个凹函数，因此下面用<code>Jensen不等式</code>的凹函数版本进行缩放。</p>
<p><span class="math display">\[
\begin{aligned}
logE(Y) \ge E(logY)&amp; = \sum_Y P(Y)\ logY \\\\
&amp; = \sum_Z Q\ log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q}
\end{aligned}
\]</span></p>
<p>有了这一步推导，我们就可以从整体来看了</p>
<p><span class="math display">\[
\begin{aligned}
\sum_i log \ p(x^{(i)}; \theta) &amp; = \sum_i log \sum_{z^{(i)}} p (x^{(i)}, z^{(i)}; \theta) \\\\
&amp;= \sum_i log \sum_{z^{i}} Q_i (z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \\\\
&amp;\ge \sum_i\sum_{z^{(i)}}Q_i(z^{(i)}) \ log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
\end{aligned}
\]</span></p>
<p>这个过程可以看作是对<span class="math inline">\(\ell(\theta)\)</span>求了下界。如果<span class="math inline">\(\theta\)</span>已经给定，那么<span class="math inline">\(\ell(\theta)\)</span>的值就决定于<span class="math inline">\(Q_i(z^{(i)})\)</span>和<span class="math inline">\(p(x^{(i)},z^{(i)})\)</span>了。在下界确定的情况下，我们可以通过调整这两个概率使下界不断上升，以逼近<span class="math inline">\(\ell(\theta)\)</span>的真实值。因此我们需要让等号成立，这样才能做后续的概率调整。</p>
<p>上面我们已经知道，根据<code>Jensen不等式</code>等号成立的条件，随机变量<span class="math inline">\(X\)</span>必须恒等于常数。也就是说： <span class="math display">\[
\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \equiv c
\]</span>  可得 <span class="math display">\[
Q(z) = \frac{p(x_i, z; \theta)}{c}
\]</span> 又因为 <span class="math display">\[
\sum_ZQ(Z) = \sum_Z \frac{p(x_i, z; \theta)}{c} = 1
\]</span> 可得 <span class="math display">\[
c = \sum_Zp(x_i, z;\theta)
\]</span> 因此</p>
<p><span class="math display">\[
\begin{aligned}
Q_i(z^{(i)}) &amp;= \frac{p(x_i, z^{(i)};\theta)}{\sum_zp(x^{(i)}, z; \theta)} \\\\
&amp; = \frac{p(x_i, z^{(i)};\theta)}{p(x^{(i)}; \theta)} \\\\
&amp; = p(z^{(i)}|x^{(i)};\theta)
\end{aligned}
\]</span></p>
<p>最后发现得到了每个样本的隐变量的后验概率，得到使下界拉升的<span class="math inline">\(Q(Z)\)</span>的计算公式就是计算<span class="math inline">\(Z\)</span>的后验概率。</p>
<p>就是说，我们解决了几个问题</p>
<p>1.就解决了<span class="math inline">\(Q(Z)\)</span>如何选择的问题</p>
<p>2.得到<code>Jensen不等式</code>的结果</p>
<p>3.让<span class="math inline">\(\ell(\theta)\)</span>可计算，最终得到似然函数。然后极大化<span class="math inline">\(\ell(\theta)\)</span>的下界。</p>
<p>于是<code>EM</code>算法出炉：</p>
<p>首先，初始化参数<span class="math inline">\(\theta\)</span>，</p>
<p><code>E-Step</code>：根据参数<span class="math inline">\(\theta\)</span>计算每个样本属于<span class="math inline">\(z_i\)</span>的概率Q。</p>
<p><code>M-Step</code>：根据Q，求出含有<span class="math inline">\(\theta\)</span>的似然函数的下界并最大化它，得到新的参数<span class="math inline">\(\theta\)</span>。</p>
<p>至此，我们已经完成了<code>EM</code>算法的基本推导，最终得到我们要顾及的最优参数<span class="math inline">\(\theta\)</span>，也得到了每个样本隐变量的取值。</p>
<p><code>EM</code>算法在一般情况是收敛的，但是不保证收敛到全局最优，即有可能进入局部的最优。</p>
<p><strong>四、EM和K-means的联系</strong></p>
<p><code>K-means</code>中每个样本所属的类就可以看成是一个隐变量，在E步中，固定每个类的中心，通过对每一个样本选择最近的类优化目标函数。在M步，重新更新每个类的中心点，然后不断迭代，直到得到某种停止条件（迭代次数/簇中心收敛/MSE）。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/20/Perceptron/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/20/Perceptron/" itemprop="url">
                  Perceptron
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-20T15:16:57+08:00">
                2018-05-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天总结一下感知机。</p>
<p>感知机是非常重要的模型，虽然《统计学习方法》把它放在了比较前的部分。说实话一年前看《统计学习方法》，看到感知机的时候，是完全不知道它在说什么。经过了一年，现在看回感知机，才体会到它的优美之处。</p>
<p>感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取<code>+1</code>和<code>-1</code>。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。</p>
<p>感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。</p>
<p>首先定义一个由输入空间到输出空间的函数： <span class="math display">\[
f(x)  = sign(w \cdot x + b) = 
\begin{cases} +1, \ \ x\ge 0\\[2ex]
-1, \ \ x\lt 0
\end{cases}
\]</span></p>
<p>其中，<code>w·x + b</code>为分离超平面。</p>
<p>我们希望能够求的所有误分类点到超平面的总距离，然后将损失函数极小化。</p>
<p>点到超平面的距离： <span class="math display">\[
\frac{1}{||w||}|w \cdot x_0 + b|
\]</span> 对于误分类的数据(xi, yi)， 有： <span class="math display">\[
 -y_i(w \cdot x_i + b) \gt 0
\]</span> 假设有M个误分类点，则这些点到超平面的距离为： <span class="math display">\[
-\frac{1}{||w||}\sum_{x_i\in M}y_i(w \cdot x_i + b) 
\]</span> 因为w是可以无限任意自由伸缩的，因此我们可以直接令||w|| = 1，也就是使其成为单位法向量。因此感知机的学习损失函数： <span class="math display">\[
L(w, b) =- \sum_{x_i\in M}y_i(w \cdot x_i + b)
\]</span> 在得到损失函数之后，我们的目标函数就变成： <span class="math display">\[
\min_{w, b} L(w, b) = -\sum_{x_i \in M} y_i (w \cdot x_i + b)
\]</span> 然后针对误分类点用随机梯度下降<code>(stochastic gradient descent, SGD)</code>极小化函数，直到训练集中没有误分类点： <span class="math display">\[
\begin{aligned}
&amp; \nabla_wL(w, b) = -\sum_{x_i \in M} y_ix_i \\\\
&amp; \nabla_bL(w, b) = -\sum_{x_i \in M}y_i \\\\
&amp; w \leftarrow w + \eta \ y_ix_i \\\\
&amp; b \leftarrow b + \eta \ y_i
\end{aligned}
\]</span> 通俗地讲，当一个实例点被误分类时，即位于分离超平面的错误一侧时，则调整w、b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</p>
<p>而通过<strong>Novikoff定理</strong>可知，对于线性可分数据集，经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。但感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程汇中误分类点的选择顺序。当训练集线性不可分，感知机学习算法不收敛，迭代结果会发生震荡。</p>
<p>如果把整个梯度下降求偏导迭代形式转为用线性代数来求解，这样的<strong>求解速度会提高很多</strong>。这时候就衍生出感知机学习算法的对偶形式。</p>
<p>我们假设样本点(xi, yi)在更新过程中被使用了<code>n_i</code>次，因此没从原始形式的学习过程可以得到，最后学习到的w和b可以分别表示为： <span class="math display">\[
w = \sum_{i=1}^{N} n_i \eta \ y_i x_i  = \sum_{i=1}^{N}\alpha_i y_i x_i
\]</span> <span class="math display">\[
b  = \sum_{i=1}^{N} n_i \eta \ y_i = \sum_{i=1}^{N} \alpha_i y_i
\]</span></p>
<p>如果某个实例点更新的次数越多，<code>n_i</code>的值越大，意味着这个样本点经常被误分类，也就是说它距离分离超平面越近，<strong>结合到SVM，这种点很可能就是支持向量</strong>。</p>
<p>因此我们的感知机模型成: <span class="math display">\[
f(x) = sign(w \cdot x + b) = sign(\sum_{i=1}^{N}n_i \eta \ y_i x_i  \cdot x + \sum_{i=1}^{N} n_i \eta \ y_i ) \\\\
\]</span> 此时，学习的目标不再是w和b，而是<code>n_i</code>, i = 1, 2, … ,N。 <span class="math display">\[
\alpha_i \leftarrow \eta(n_i + 1)  = \eta \ n_i + \eta = \alpha_i + \eta
\]</span></p>
<p><span class="math display">\[
b \leftarrow b + \eta \ y_i
\]</span></p>
<p>b和原始一样，不同的只是对w的迭代，转化了对α的迭代。迭代完求出所有的α之后，计算w就只是O(1)的复杂度。这样就实现了从不同的角度去解答相似的问题，也就是对偶思维。</p>
<p>下面就直接上<code>python</code>代码吧。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> random <span class="im">import</span> seed</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">from</span> random <span class="im">import</span> randrange</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">from</span> csv <span class="im">import</span> reader</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"> </a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="kw">def</span> load_csv(filename):</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">    dataset <span class="op">=</span> <span class="bu">list</span>()</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> <span class="bu">file</span>:</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">        csv_reader <span class="op">=</span> reader(<span class="bu">file</span>)</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">        <span class="cf">for</span> row <span class="kw">in</span> csv_reader:</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">            <span class="cf">if</span> <span class="kw">not</span> row:</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">                <span class="cf">continue</span></a>
<a class="sourceLine" id="cb1-12" data-line-number="12">                dataset.append(row)</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">                <span class="cf">return</span> dataset</a>
<a class="sourceLine" id="cb1-14" data-line-number="14"> </a>
<a class="sourceLine" id="cb1-15" data-line-number="15"><span class="kw">def</span> str_column_to_float(dataset, column):</a>
<a class="sourceLine" id="cb1-16" data-line-number="16">    <span class="cf">for</span> row <span class="kw">in</span> dataset:</a>
<a class="sourceLine" id="cb1-17" data-line-number="17">        row[column] <span class="op">=</span> <span class="bu">float</span>(row[column].strip())</a>
<a class="sourceLine" id="cb1-18" data-line-number="18"> </a>
<a class="sourceLine" id="cb1-19" data-line-number="19"><span class="kw">def</span> str_column_to_int(dataset, column):</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">    class_values <span class="op">=</span> [row[column] <span class="cf">for</span> row <span class="kw">in</span> dataset]</a>
<a class="sourceLine" id="cb1-21" data-line-number="21">    unique <span class="op">=</span> <span class="bu">set</span>(class_values)</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">    lookup <span class="op">=</span> <span class="bu">dict</span>()</a>
<a class="sourceLine" id="cb1-23" data-line-number="23">    <span class="cf">for</span> i, value <span class="kw">in</span> <span class="bu">enumerate</span>(unique):</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">        lookup[value] <span class="op">=</span> i</a>
<a class="sourceLine" id="cb1-25" data-line-number="25">        <span class="cf">for</span> row <span class="kw">in</span> dataset:</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">            row[column] <span class="op">=</span> lookup[row[column]]</a>
<a class="sourceLine" id="cb1-27" data-line-number="27">    <span class="cf">return</span> lookup</a>
<a class="sourceLine" id="cb1-28" data-line-number="28"> </a>
<a class="sourceLine" id="cb1-29" data-line-number="29"><span class="kw">def</span> cross_validation_split(dataset, n_folds):</a>
<a class="sourceLine" id="cb1-30" data-line-number="30">    dataset_split <span class="op">=</span> <span class="bu">list</span>()</a>
<a class="sourceLine" id="cb1-31" data-line-number="31">    dataset_copy <span class="op">=</span> <span class="bu">list</span>(dataset)</a>
<a class="sourceLine" id="cb1-32" data-line-number="32">    fold_size <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(dataset) <span class="op">/</span> n_folds)</a>
<a class="sourceLine" id="cb1-33" data-line-number="33">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_folds):</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">        fold <span class="op">=</span> <span class="bu">list</span>()</a>
<a class="sourceLine" id="cb1-35" data-line-number="35">        <span class="cf">while</span> <span class="bu">len</span>(fold) <span class="op">&lt;</span> fold_size:</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">            index <span class="op">=</span> randrange(<span class="bu">len</span>(dataset_copy))</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">            fold.append(dataset_copy.pop(index))</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">        dataset_split.append(fold)</a>
<a class="sourceLine" id="cb1-39" data-line-number="39">    <span class="cf">return</span> dataset_split</a>
<a class="sourceLine" id="cb1-40" data-line-number="40"> </a>
<a class="sourceLine" id="cb1-41" data-line-number="41"><span class="kw">def</span> accuracy_metric(actual, predicted):</a>
<a class="sourceLine" id="cb1-42" data-line-number="42">    correct <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb1-43" data-line-number="43">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(actual)):</a>
<a class="sourceLine" id="cb1-44" data-line-number="44">        <span class="cf">if</span> actual[i] <span class="op">==</span> predicted[i]:</a>
<a class="sourceLine" id="cb1-45" data-line-number="45">            correct <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-46" data-line-number="46">    <span class="cf">return</span> correct <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(actual)) <span class="op">*</span> <span class="fl">100.0</span></a>
<a class="sourceLine" id="cb1-47" data-line-number="47"> </a>
<a class="sourceLine" id="cb1-48" data-line-number="48"><span class="kw">def</span> evaluate_algorithm(dataset, algorithm, n_folds, <span class="op">*</span>args):</a>
<a class="sourceLine" id="cb1-49" data-line-number="49">    folds <span class="op">=</span> cross_validation_split(dataset, n_folds)</a>
<a class="sourceLine" id="cb1-50" data-line-number="50">    scores <span class="op">=</span> <span class="bu">list</span>()</a>
<a class="sourceLine" id="cb1-51" data-line-number="51">    <span class="cf">for</span> fold <span class="kw">in</span> folds:</a>
<a class="sourceLine" id="cb1-52" data-line-number="52">        train_set <span class="op">=</span> <span class="bu">list</span>(folds)</a>
<a class="sourceLine" id="cb1-53" data-line-number="53">        train_set.remove(fold)</a>
<a class="sourceLine" id="cb1-54" data-line-number="54">        train_set <span class="op">=</span> <span class="bu">sum</span>(train_set, [])</a>
<a class="sourceLine" id="cb1-55" data-line-number="55">        test_set <span class="op">=</span> <span class="bu">list</span>()</a>
<a class="sourceLine" id="cb1-56" data-line-number="56">        <span class="cf">for</span> row <span class="kw">in</span> fold:</a>
<a class="sourceLine" id="cb1-57" data-line-number="57">            row_copy <span class="op">=</span> <span class="bu">list</span>(row)</a>
<a class="sourceLine" id="cb1-58" data-line-number="58">            test_set.append(row_copy)</a>
<a class="sourceLine" id="cb1-59" data-line-number="59">            row_copy[<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb1-60" data-line-number="60">            predicted <span class="op">=</span> algorithm(train_set, test_set, <span class="op">*</span>args)</a>
<a class="sourceLine" id="cb1-61" data-line-number="61">            actual <span class="op">=</span> [row[<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> row <span class="kw">in</span> fold]</a>
<a class="sourceLine" id="cb1-62" data-line-number="62">            accuracy <span class="op">=</span> accuracy_metric(actual, predicted)</a>
<a class="sourceLine" id="cb1-63" data-line-number="63">            scores.append(accuracy)</a>
<a class="sourceLine" id="cb1-64" data-line-number="64">            <span class="cf">return</span> scores</a>
<a class="sourceLine" id="cb1-65" data-line-number="65"></a>
<a class="sourceLine" id="cb1-66" data-line-number="66"><span class="kw">def</span> predict(row, weights):</a>
<a class="sourceLine" id="cb1-67" data-line-number="67">    activation <span class="op">=</span> weights[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb1-68" data-line-number="68">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(row)<span class="op">-</span><span class="dv">1</span>):</a>
<a class="sourceLine" id="cb1-69" data-line-number="69">        activation <span class="op">+=</span> weights[i <span class="op">+</span> <span class="dv">1</span>] <span class="op">*</span> row[i]</a>
<a class="sourceLine" id="cb1-70" data-line-number="70">    <span class="cf">return</span> <span class="fl">1.0</span> <span class="cf">if</span> activation <span class="op">&gt;=</span> <span class="fl">0.0</span> <span class="cf">else</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb1-71" data-line-number="71"> </a>
<a class="sourceLine" id="cb1-72" data-line-number="72"><span class="kw">def</span> train_weights(train, l_rate, n_epoch):</a>
<a class="sourceLine" id="cb1-73" data-line-number="73">    weights <span class="op">=</span> [<span class="fl">0.0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(train[<span class="dv">0</span>]))]</a>
<a class="sourceLine" id="cb1-74" data-line-number="74">    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epoch):</a>
<a class="sourceLine" id="cb1-75" data-line-number="75">        <span class="cf">for</span> row <span class="kw">in</span> train:</a>
<a class="sourceLine" id="cb1-76" data-line-number="76">            prediction <span class="op">=</span> predict(row, weights)</a>
<a class="sourceLine" id="cb1-77" data-line-number="77">            error <span class="op">=</span> row[<span class="op">-</span><span class="dv">1</span>] <span class="op">-</span> prediction</a>
<a class="sourceLine" id="cb1-78" data-line-number="78">            weights[<span class="dv">0</span>] <span class="op">=</span> weights[<span class="dv">0</span>] <span class="op">+</span> l_rate <span class="op">*</span> error</a>
<a class="sourceLine" id="cb1-79" data-line-number="79">            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(row)<span class="op">-</span><span class="dv">1</span>):</a>
<a class="sourceLine" id="cb1-80" data-line-number="80">                weights[i <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> weights[i <span class="op">+</span> <span class="dv">1</span>] <span class="op">+</span> l_rate <span class="op">*</span> error <span class="op">*</span> row[i]</a>
<a class="sourceLine" id="cb1-81" data-line-number="81">    <span class="cf">return</span> weights</a>
<a class="sourceLine" id="cb1-82" data-line-number="82"> </a>
<a class="sourceLine" id="cb1-83" data-line-number="83"><span class="kw">def</span> perceptron(train, test, l_rate, n_epoch):</a>
<a class="sourceLine" id="cb1-84" data-line-number="84">    predictions <span class="op">=</span> <span class="bu">list</span>()</a>
<a class="sourceLine" id="cb1-85" data-line-number="85">    weights <span class="op">=</span> train_weights(train, l_rate, n_epoch)</a>
<a class="sourceLine" id="cb1-86" data-line-number="86">    <span class="cf">for</span> row <span class="kw">in</span> test:</a>
<a class="sourceLine" id="cb1-87" data-line-number="87">        prediction <span class="op">=</span> predict(row, weights)</a>
<a class="sourceLine" id="cb1-88" data-line-number="88">        predictions.append(prediction)</a>
<a class="sourceLine" id="cb1-89" data-line-number="89">    <span class="cf">return</span>(predictions)</a></code></pre></div>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/18/统计学习方法概论/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/18/统计学习方法概论/" itemprop="url">
                  统计学习方法概论及Python应用
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-18T10:41:37+08:00">
                2018-05-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近开始重新整理机器学习的内容。</p>
<p>今天总结以下几个内容：</p>
<ol type="1">
<li>有监督学习、无监督学习的比较、分类和回归方法的比较</li>
<li>过拟合和欠拟合的定义及解决方法</li>
<li>交叉验证</li>
<li>模型评估指标（精确率、召回率、F值、ROC、AUC、Explained_variance、MSE和MSLE）</li>
</ol>
<h3 id="一有监督学习无监督学习分类和回归方法">一、有监督学习、无监督学习、分类和回归方法</h3>
<p>​ 机器学习中有四个主要的分类：</p>
<ul>
<li>有监督学习<code>(supervised learning)</code></li>
<li>无监督学习<code>(unsupervised learning)</code></li>
<li>半监督学习<code>(semisupervised learning)</code></li>
<li>强化学习<code>(reinforcement learning)</code>。</li>
</ul>
<p>今天先讲监督学习<code>(supervised learning)</code>和无监督学习 <code>(unsupervised learning)</code>。</p>
<ol type="1">
<li><u>监督学习</u><code>(supervised learning)</code></li>
</ol>
<p>它是由训练数据中学到或者建立一个模式（函数或者是学习模型），使模型能够对任意给定的输入，对其相应的输出做出一个预测。</p>
<p>训练数据通常是一个向量和一个预期输出组成，如果函数的输出是一个连续的数值，则称为回归分析。如果函数的输出是一个分类标签，则称为分类问题<code>(classification)</code>。其中有以下模型：</p>
<blockquote>
<p>​ 决策树 · 表征(Bagging、Boosting、随即森林) · k-NN · 线性回归 · 朴素贝叶斯 · 神经网络 · 逻辑回归 · 感知器 · 支持向量机(SVM) · 相关向量机(RVM)</p>
</blockquote>
<p>现在我们来看点例子吧。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> sklearn <span class="im">import</span> linear_model</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">from</span> sklearn <span class="im">import</span> datasets</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">from</span> sklearn <span class="im">import</span> svm</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> StratifiedKFold</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">iris <span class="op">=</span> datasets.load_iris()</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">iris_X <span class="op">=</span> iris.data</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">iris_y <span class="op">=</span> iris.target</a>
<a class="sourceLine" id="cb1-10" data-line-number="10"></a>
<a class="sourceLine" id="cb1-11" data-line-number="11"><span class="co"># 打印分数</span></a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="kw">def</span> display_scores(scores):</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">    <span class="bu">print</span>(<span class="st">&quot;Scores : &quot;</span>, scores)</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">    <span class="bu">print</span>(<span class="st">&quot;Mean : </span><span class="sc">%.2f</span><span class="st">&quot;</span> <span class="op">%</span> scores.mean())</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">    <span class="bu">print</span>(<span class="st">&quot;Accuracy: </span><span class="sc">%0.2f</span><span class="st"> (+/- </span><span class="sc">%0.2f</span><span class="st">)&quot;</span> <span class="op">%</span> (scores.mean(), scores.std() <span class="op">*</span> <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1-16" data-line-number="16"></a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="co"># 输入model，X，y</span></a>
<a class="sourceLine" id="cb1-18" data-line-number="18"><span class="co"># 输出分数</span></a>
<a class="sourceLine" id="cb1-19" data-line-number="19"><span class="kw">def</span> model_scores(model, X, y):</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">    cv <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1-21" data-line-number="21">    scores <span class="op">=</span> cross_val_score(model, X, y, cv<span class="op">=</span>cv)</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">    display_scores(scores)</a>
<a class="sourceLine" id="cb1-23" data-line-number="23">    </a>
<a class="sourceLine" id="cb1-24" data-line-number="24"><span class="co"># LinearRegression</span></a>
<a class="sourceLine" id="cb1-25" data-line-number="25">reg <span class="op">=</span> linear_model.LinearRegression()</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">model_scores(reg, iris_X, iris_y)</a>
<a class="sourceLine" id="cb1-27" data-line-number="27">[output]</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">Scores :  [<span class="fl">0.93427277</span> <span class="fl">0.9395803</span>  <span class="fl">0.89930025</span> <span class="fl">0.90288946</span> <span class="fl">0.95617435</span>]</a>
<a class="sourceLine" id="cb1-29" data-line-number="29">Mean : <span class="fl">0.93</span></a>
<a class="sourceLine" id="cb1-30" data-line-number="30">Accuracy: <span class="fl">0.93</span> (<span class="op">+/-</span> <span class="fl">0.04</span>)</a>
<a class="sourceLine" id="cb1-31" data-line-number="31"></a>
<a class="sourceLine" id="cb1-32" data-line-number="32"><span class="co"># Logistic Regression</span></a>
<a class="sourceLine" id="cb1-33" data-line-number="33">logistic <span class="op">=</span> linear_model.LogisticRegression(C<span class="op">=</span><span class="fl">1e5</span>)</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">model_scores(logistic, iris_X, iris_y)</a>
<a class="sourceLine" id="cb1-35" data-line-number="35">[output]</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">Scores :  [<span class="fl">0.96666667</span> <span class="fl">0.96666667</span> <span class="fl">0.93333333</span> <span class="fl">0.93333333</span> <span class="fl">1.</span>        ]</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">Mean : <span class="fl">0.96</span></a>
<a class="sourceLine" id="cb1-38" data-line-number="38">Accuracy: <span class="fl">0.96</span> (<span class="op">+/-</span> <span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb1-39" data-line-number="39"></a>
<a class="sourceLine" id="cb1-40" data-line-number="40"><span class="co"># SVM</span></a>
<a class="sourceLine" id="cb1-41" data-line-number="41">clf <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>, C<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1-42" data-line-number="42">model_scores(clf, iris_X, iris_y)</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">[output]</a>
<a class="sourceLine" id="cb1-44" data-line-number="44">Scores :  [<span class="fl">0.96666667</span> <span class="fl">1.</span>         <span class="fl">0.96666667</span> <span class="fl">0.96666667</span> <span class="fl">1.</span>        ]</a>
<a class="sourceLine" id="cb1-45" data-line-number="45">Mean : <span class="fl">0.98</span></a>
<a class="sourceLine" id="cb1-46" data-line-number="46">Accuracy: <span class="fl">0.98</span> (<span class="op">+/-</span> <span class="fl">0.03</span>)</a></code></pre></div>
<ol start="2" type="1">
<li><u>无监督学习</u><code>(unsupervised learning)</code></li>
</ol>
<p>无监督学习没有输入标签，一种常见的无监督学习就是数据聚类，在人工神经网络中，生成对抗网络<code>(GAN)</code>、自组织映射<code>(SOM)</code>和适应性共振理论<code>(ART)</code>则是最常用的无监督学习。在聚类中，主要有以下模型：</p>
<blockquote>
<p>BIRCH · Hierarchical clustering · k平均 · 期望最大化（EM）· DBSCAN · OPTICS · 均值飘逸(Mean shifit)</p>
</blockquote>
<p>现在我们来看些例子吧。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="op">%</span>matplotlib inline</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="im">import</span> seaborn <span class="im">as</span> sns<span class="op">;</span> sns.<span class="bu">set</span>()</a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="im">from</span> sklearn.datasets.samples_generator <span class="im">import</span> make_blobs</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">X, y_true <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">300</span>, centers<span class="op">=</span><span class="dv">4</span>,</a>
<a class="sourceLine" id="cb2-9" data-line-number="9">                      cluster_std<span class="op">=</span><span class="fl">0.60</span>, random_state<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2-10" data-line-number="10">plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb2-11" data-line-number="11"></a>
<a class="sourceLine" id="cb2-12" data-line-number="12">kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb2-13" data-line-number="13">kmeans.fit(X)</a>
<a class="sourceLine" id="cb2-14" data-line-number="14">y_kmeans <span class="op">=</span> kmeans.predict(X)</a>
<a class="sourceLine" id="cb2-15" data-line-number="15"></a>
<a class="sourceLine" id="cb2-16" data-line-number="16">plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y_kmeans, s<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</a>
<a class="sourceLine" id="cb2-17" data-line-number="17"></a>
<a class="sourceLine" id="cb2-18" data-line-number="18">centers <span class="op">=</span> kmeans.cluster_centers_</a>
<a class="sourceLine" id="cb2-19" data-line-number="19">plt.scatter(centers[:, <span class="dv">0</span>], centers[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">&#39;black&#39;</span>, s<span class="op">=</span><span class="dv">200</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</a></code></pre></div>
<p><img src="/images/download.png"><img src="/images/download-1.png"></p>
<ol start="3" type="1">
<li>回归和分类的比较</li>
</ol>
<p>在上面已经针对回归和分类有了一个比较通俗的解读：如果函数的输出是一个连续的数值，则称为回归分析。如果函数的输出是一个分类标签，则称为分类问题<code>(classification)</code>。</p>
<p>回归方法一览：</p>
<blockquote>
<p>SGD Regressor · Lasso · ElasticNet · RidgeRegression · SVR(kernel=‘linear’ · SVR(kernel=‘rbf’) · EnsembleRegressors</p>
</blockquote>
<p>分类方法一览：</p>
<blockquote>
<p>SGD Classifier · Linear SVC · KNeighbors Classifier · kernel approximation · SVC · Ensemble Classifiers · Naive Bayes</p>
</blockquote>
<h3 id="二overfitting与underfitting定义及解决方法">二、<code>overfitting</code>与<code>underfitting</code>定义及解决方法</h3>
<p><code>方差</code>：方差度量了同样大小的训练集变动所导致的学习性能变化，即刻画了数据扰动所造成的影响。</p>
<p><code>偏差</code>: 偏差度量了学习算法的期望预测真是结果的偏离程度，即刻画了学习算法本身的拟合能力。</p>
<p><code>噪声</code>: 噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p>
<h3 id="underfitting欠拟合"><u>underfitting欠拟合</u></h3>
<p><strong>定义</strong>：欠拟合指的是模型并没有很好地捕捉到数据特征，不能很好地拟合数据。</p>
<p><strong>特点</strong>：</p>
<ol type="1">
<li>模型的方差<code>variance</code>较小，预测输出和期望输出的差值较小</li>
<li>偏差<code>bias</code>较大，期望预测和真实值的差值较大</li>
<li>模型复杂度较小</li>
</ol>
<p><strong>解决方法</strong>：</p>
<ol type="1">
<li>添加其他特征项（可通过组合、泛化和相关性这三种手段）</li>
<li>添加多项式特征，增加模型复杂度</li>
<li>减少正则化参数，减少对复杂模型的惩罚</li>
<li>增加更多的数据</li>
</ol>
<h3 id="overfitting-过拟合"><u>overfitting 过拟合</u></h3>
<p><strong>定义</strong>：过拟合指的是模型对数据的太过拟合，对新的数据泛化能力较差</p>
<p><strong>特点</strong>：</p>
<ol type="1">
<li>模型的方差<code>variance</code>较大，预测输出和期望输出的差值较大</li>
<li>偏差<code>bias</code>一般较大，期望预测和真实值的差值一般较大</li>
<li>模型复杂度较大</li>
</ol>
<p><strong>解决方法</strong>：</p>
<ol type="1">
<li>重新清洗数据</li>
<li>增大数据的训练量</li>
<li>采用正则化</li>
<li>采用<code>Dropout</code>方法，在训练时让神经元以一定的概率不工作</li>
</ol>
<h3 id="三交叉验证cross-validation">三、交叉验证(cross validation)</h3>
<p>其实在上面的例子中， 我们已经使用了交叉验证。交叉验证直白地说就是为了验证模型的表现。学习预测函数的参数并且在相同的数据中测试它是一种错误的方法：因为只重复看到的样本标签的模型本身会有一个完美的表现，但它并不能预测任何有用的信息，对于未见过的数据，模型的泛化能力会非常差。这种情况称为过拟合。</p>
<p>为了避免这种情况，对于有监督学习，通常会将数据集划分为训练集和测试集，然后进行模型表现的评估。这种做法称为交叉验证。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="im">from</span> sklearn <span class="im">import</span> datasets</a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="im">from</span> sklearn <span class="im">import</span> svm</a>
<a class="sourceLine" id="cb3-5" data-line-number="5"></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">iris <span class="op">=</span> datasets.load_iris()</a>
<a class="sourceLine" id="cb3-7" data-line-number="7">iris_X <span class="op">=</span> iris.data</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">iris_y <span class="op">=</span> iris.target</a>
<a class="sourceLine" id="cb3-9" data-line-number="9"></a>
<a class="sourceLine" id="cb3-10" data-line-number="10">X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">    iris_X, iris_y, test_size<span class="op">=</span><span class="fl">0.4</span>, random_state<span class="op">=</span><span class="dv">0</span></a>
<a class="sourceLine" id="cb3-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb3-13" data-line-number="13"></a>
<a class="sourceLine" id="cb3-14" data-line-number="14">clf <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>, C<span class="op">=</span><span class="dv">1</span>).fit(X_train, y_train)</a>
<a class="sourceLine" id="cb3-15" data-line-number="15"><span class="bu">print</span>(clf.score(X_test, y_test))</a>
<a class="sourceLine" id="cb3-16" data-line-number="16">[output]</a>
<a class="sourceLine" id="cb3-17" data-line-number="17"><span class="fl">0.9666666666666667</span></a></code></pre></div>
<p>常见的交叉验证有：</p>
<ul>
<li>Holdout验证：随机从最初样本选取部分，形成交叉验证数据，而剩余的就当作训练数据。一般来说，少于原样本三分之一的数据被选作验证数据。</li>
<li>K-Fold交叉验证</li>
</ul>
<p>K次交叉验证，将训练集分割成K个子样本， 一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用作训练。交叉验证重复K次，每个子样本验证一次。这种方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10次交叉验证是最常用的。</p>
<ul>
<li>留一验证（LOOCV）</li>
</ul>
<p>事实上这跟K-Fold是一样的，只不过这里的K是样本个数。</p>
<p>常见的误差估计：均方差和方根均方差。</p>
<h3 id="四模型评估指标精确率召回率f值rocauc">四、模型评估指标（精确率、召回率、F值、ROC、AUC）</h3>
<p>首先我们来看一张<code>sklearn</code>上模型评估指标。</p>
<p><img src="/images/Screen%20Shot%202018-05-19%20at%2012.41.31%20AM.png"></p>
<p>对于<strong><u>分类问题</u></strong>，一般使用<code>accuracy</code>、<code>f1</code>、<code>neg_log_loss</code>、<code>recall</code>和<code>roc_auc</code></p>
<ol type="1">
<li><code>recall</code>、<code>precision</code>、<code>accuracy</code>和<code>f1</code></li>
</ol>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">预测结果</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>真实结果</strong></td>
<td style="text-align: center;">正例（Yes）</td>
<td style="text-align: center;">反例（No）</td>
</tr>
<tr class="even">
<td style="text-align: center;">正例（Yes）</td>
<td style="text-align: center;">TP</td>
<td style="text-align: center;">FN</td>
</tr>
<tr class="odd">
<td style="text-align: center;">反例（No）</td>
<td style="text-align: center;">FP</td>
<td style="text-align: center;">TN</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\begin{aligned}
Precision\ \ (P)  &amp; = \frac {TP}{TP + FP} \\
\\
Recall\ \ (R) &amp; = \frac {TP}{TP + FN} \\
\\
F1 = \frac{2P \cdot R}{P + R} &amp; = \frac{2TP}{2TP+FP+FN} \\
\\
Accuracy &amp; = \frac{TP+TN}{TP+TN+FN+FP}
\end{aligned}
\]</span></p>
<ul>
<li><code>查准率(Precision)</code>: 指在所有预测为正例的情况中，预测正例且实际正例的比例。<code>查准率</code>关注的是预测正例正确占所有预测为正例情况的比例，白话文就是说，<code>查准率</code>关注的是你预测了一大堆正例的例子，然后里面有多少是预测正确的，就是准不准嘛。</li>
<li><code>查全率(Recall)</code>: 指在实际为正例的情况中，预测为正例的比例。白话文就是说，<code>查全率</code>关注的是你正确预测了一大堆正例，数量为A，真实情况下正例的数量是B，然后看B里面有多少比例的A，然后去判断预测正确的情况全不全。</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="im">from</span> sklearn <span class="im">import</span> svm, datasets</a>
<a class="sourceLine" id="cb4-2" data-line-number="2"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="im">from</span> sklearn.metrics <span class="im">import</span> average_precision_score, precision_recall_curve</a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score</a>
<a class="sourceLine" id="cb4-6" data-line-number="6"><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</a>
<a class="sourceLine" id="cb4-7" data-line-number="7"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb4-8" data-line-number="8"></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">iris <span class="op">=</span> datasets.load_iris()</a>
<a class="sourceLine" id="cb4-11" data-line-number="11">X <span class="op">=</span> iris.data</a>
<a class="sourceLine" id="cb4-12" data-line-number="12">y <span class="op">=</span> iris.target</a>
<a class="sourceLine" id="cb4-13" data-line-number="13"></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">random_state <span class="op">=</span> np.random.RandomState(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb4-15" data-line-number="15">n_samples, n_features <span class="op">=</span> X.shape</a>
<a class="sourceLine" id="cb4-16" data-line-number="16">X <span class="op">=</span> np.c_[X, random_state.randn(n_samples, <span class="dv">200</span> <span class="op">*</span> n_features)]</a>
<a class="sourceLine" id="cb4-17" data-line-number="17"></a>
<a class="sourceLine" id="cb4-18" data-line-number="18">X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X[y <span class="op">&lt;</span> <span class="dv">2</span>], y[y <span class="op">&lt;</span> <span class="dv">2</span>],</a>
<a class="sourceLine" id="cb4-19" data-line-number="19">                                                    test_size<span class="op">=</span>.<span class="dv">5</span>,</a>
<a class="sourceLine" id="cb4-20" data-line-number="20">                                                   random_state<span class="op">=</span>random_state)</a>
<a class="sourceLine" id="cb4-21" data-line-number="21"></a>
<a class="sourceLine" id="cb4-22" data-line-number="22">clf <span class="op">=</span> svm.LinearSVC(random_state<span class="op">=</span>random_state)</a>
<a class="sourceLine" id="cb4-23" data-line-number="23">clf.fit(X_train, y_train)</a>
<a class="sourceLine" id="cb4-24" data-line-number="24">y_score <span class="op">=</span> clf.decision_function(X_test)</a>
<a class="sourceLine" id="cb4-25" data-line-number="25"></a>
<a class="sourceLine" id="cb4-26" data-line-number="26">average_precision <span class="op">=</span> average_precision_score(y_test, y_score)</a>
<a class="sourceLine" id="cb4-27" data-line-number="27"><span class="bu">print</span>(<span class="st">&#39;Average precision-recall score: </span><span class="sc">{0:0.2f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(average_precision))</a>
<a class="sourceLine" id="cb4-28" data-line-number="28">[output]</a>
<a class="sourceLine" id="cb4-29" data-line-number="29">Average precision<span class="op">-</span>recall score: <span class="fl">0.88</span></a>
<a class="sourceLine" id="cb4-30" data-line-number="30">    </a>
<a class="sourceLine" id="cb4-31" data-line-number="31"><span class="bu">print</span>(precision_score(y_test, clf.predict(X_test)))</a>
<a class="sourceLine" id="cb4-32" data-line-number="32"><span class="bu">print</span>(recall_score(y_test, clf.predict(X_test)))</a>
<a class="sourceLine" id="cb4-33" data-line-number="33"><span class="bu">print</span>(f1_score(y_test, clf.predict(X_test)))</a>
<a class="sourceLine" id="cb4-34" data-line-number="34"><span class="bu">print</span>(classification_report(y_test, clf.predict(X_test)))</a>
<a class="sourceLine" id="cb4-35" data-line-number="35">[output]</a>
<a class="sourceLine" id="cb4-36" data-line-number="36"><span class="fl">0.75</span></a>
<a class="sourceLine" id="cb4-37" data-line-number="37"><span class="fl">0.8076923076923077</span></a>
<a class="sourceLine" id="cb4-38" data-line-number="38"><span class="fl">0.7777777777777779</span></a>
<a class="sourceLine" id="cb4-39" data-line-number="39"></a>
<a class="sourceLine" id="cb4-40" data-line-number="40">            precision    recall  f1<span class="op">-</span>score   support</a>
<a class="sourceLine" id="cb4-41" data-line-number="41"></a>
<a class="sourceLine" id="cb4-42" data-line-number="42">          <span class="dv">0</span>       <span class="fl">0.77</span>      <span class="fl">0.71</span>      <span class="fl">0.74</span>        <span class="dv">24</span></a>
<a class="sourceLine" id="cb4-43" data-line-number="43">          <span class="dv">1</span>       <span class="fl">0.75</span>      <span class="fl">0.81</span>      <span class="fl">0.78</span>        <span class="dv">26</span></a>
<a class="sourceLine" id="cb4-44" data-line-number="44"></a>
<a class="sourceLine" id="cb4-45" data-line-number="45">avg <span class="op">/</span> total       <span class="fl">0.76</span>      <span class="fl">0.76</span>      <span class="fl">0.76</span>        <span class="dv">50</span></a>
<a class="sourceLine" id="cb4-46" data-line-number="46"></a>
<a class="sourceLine" id="cb4-47" data-line-number="47"></a>
<a class="sourceLine" id="cb4-48" data-line-number="48">precision, recall, _ <span class="op">=</span> precision_recall_curve(y_test, y_score)</a>
<a class="sourceLine" id="cb4-49" data-line-number="49">plt.step(recall, precision, color<span class="op">=</span><span class="st">&#39;b&#39;</span>, alpha<span class="op">=</span><span class="fl">0.2</span>,</a>
<a class="sourceLine" id="cb4-50" data-line-number="50">        where<span class="op">=</span><span class="st">&#39;post&#39;</span>)</a>
<a class="sourceLine" id="cb4-51" data-line-number="51">plt.fill_between(recall, precision, step<span class="op">=</span><span class="st">&#39;post&#39;</span>, alpha<span class="op">=</span><span class="fl">0.2</span>,</a>
<a class="sourceLine" id="cb4-52" data-line-number="52">                color<span class="op">=</span><span class="st">&#39;b&#39;</span>)</a>
<a class="sourceLine" id="cb4-53" data-line-number="53"></a>
<a class="sourceLine" id="cb4-54" data-line-number="54">plt.xlabel(<span class="st">&#39;Recall&#39;</span>)</a>
<a class="sourceLine" id="cb4-55" data-line-number="55">plt.ylabel(<span class="st">&#39;Precision&#39;</span>)</a>
<a class="sourceLine" id="cb4-56" data-line-number="56">plt.ylim([<span class="fl">0.0</span>, <span class="fl">1.05</span>])</a>
<a class="sourceLine" id="cb4-57" data-line-number="57">plt.xlim([<span class="fl">0.0</span>, <span class="fl">1.0</span>])</a>
<a class="sourceLine" id="cb4-58" data-line-number="58">plt.title(<span class="st">&#39;2-class Precision-Recall curve: AP=</span><span class="sc">{0:0.2f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(average_precision))    </a></code></pre></div>
<p><img src="/images/download-2.png"></p>
<ul>
<li><code>准确率(Accuracuy)</code>: 对于给定的测试集数据，分类器正确分类的样本数与总样本之比。</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">y_pred <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>]</a>
<a class="sourceLine" id="cb5-4" data-line-number="4">y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</a>
<a class="sourceLine" id="cb5-5" data-line-number="5"><span class="bu">print</span>(accuracy_score(y_true, y_pred))</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">[output]</a>
<a class="sourceLine" id="cb5-7" data-line-number="7"><span class="fl">0.5</span></a></code></pre></div>
<ol start="2" type="1">
<li><code>ROC</code>和<code>AUC</code></li>
</ol>
<p><strong><u>ROC(Receiver operating characteristic)和AUC(Area Under ROC Curve)</u></strong></p>
<p>ROC曲线是二值分类器常用的工具，是反映敏感性和特异性连续变量的综合指标，roc曲线上每个点反映着对同一信号刺激的感受性。</p>
<p>它跟<code>precision/recall</code>曲线非常相似，只不过ROC曲线描绘的是TPR(True Positive Rate，Recall另一种叫法)和FPR(False Positive Rate)的关系。</p>
<p>在我看来，ROC曲线更多的描述多个模型的性能比较，如果一个模型A的ROC曲线被另一个模型B的ROC包围，那很明显另外一个模型B的表现优于模型A。因此比较ROC曲线下的面积，即AUC(Area Under ROC Curve) <span class="math display">\[
\begin{aligned}
Recall\ \ (R) &amp; = TPR =  \frac {TP}{TP + FN} \\
\\
FPR  &amp;= \frac{FP}{FP + TN} \\\\
&amp;= 1 - TNR  \\\\ 
&amp; = 1 - \frac{TN}{FP+TN} \\\\
&amp;= 1-specificity
\end{aligned}
\]</span> ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。</p>
<p>理想情况下，TPR应该接近1，FPR应该接近0。故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，Sensitivity、Specificity越大效果越好。</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">fpr, tpr, thresholds <span class="op">=</span> roc_curve(y_test, clf.predict(X_test))</a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="kw">def</span> plot_roc_curve(fpr, tpr, label<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb6-4" data-line-number="4">    plt.plot(fpr, tpr, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span>label)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5">    plt.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">&#39;k--&#39;</span>)</a>
<a class="sourceLine" id="cb6-6" data-line-number="6">    plt.axis([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>])</a>
<a class="sourceLine" id="cb6-7" data-line-number="7">    plt.xlabel(<span class="st">&#39;False Positive Rate&#39;</span>)</a>
<a class="sourceLine" id="cb6-8" data-line-number="8">    plt.ylabel(<span class="st">&#39;True Positive Rate&#39;</span>)</a>
<a class="sourceLine" id="cb6-9" data-line-number="9">    </a>
<a class="sourceLine" id="cb6-10" data-line-number="10">plot_roc_curve(fpr, tpr)</a>
<a class="sourceLine" id="cb6-11" data-line-number="11">plt.show()</a></code></pre></div>
<p><img src="/images/download-3.png"></p>
<ol start="3" type="1">
<li><code>neg_log_loss</code></li>
</ol>
<p>对于<code>log_loss</code>，它计算的是交叉熵。</p>
<p>对于<u>回归问题</u>，一般使用<code>explained_variance_score</code>、<code>Mean squared error</code>、<code>neg_mean_squared_log_error</code></p>
<ol type="1">
<li><code>Explained_variance_score</code></li>
</ol>
<p>最好的表现是1， 数值越低表现越差 <span class="math display">\[
explained_variance(y,  \hat y) = 1- \frac{Var\{y - \hat y\}}{Var\{y\} }
\]</span></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> explained_variance_score</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">y_true <span class="op">=</span> [<span class="dv">3</span>, <span class="fl">-0.5</span>, <span class="dv">2</span>, <span class="dv">7</span>]</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">y_pred <span class="op">=</span> [<span class="fl">2.5</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">8</span>]</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="bu">print</span>(explained_variance_score(y_true, y_pred))</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">[output]</a>
<a class="sourceLine" id="cb7-6" data-line-number="6"><span class="fl">0.9571734475374732</span></a></code></pre></div>
<ol start="2" type="1">
<li><code>Mean squared error(MSE)</code></li>
</ol>
<p>MSE越小，模型拟合表现越好。 <span class="math display">\[
MSE(y, \hat y) = \frac{1}{n_{samples}} \sum^{n_{samples} - 1}_{i=0} \ \ (y_i - \hat y_i) ^ 2
\]</span></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">y_true <span class="op">=</span> [<span class="dv">3</span>, <span class="fl">-0.5</span>, <span class="dv">2</span>, <span class="dv">7</span>]</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">y_pred <span class="op">=</span> [<span class="fl">2.5</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">8</span>]</a>
<a class="sourceLine" id="cb8-4" data-line-number="4"><span class="bu">print</span>(mean_squared_error(y_true, y_pred))</a>
<a class="sourceLine" id="cb8-5" data-line-number="5">[output]</a>
<a class="sourceLine" id="cb8-6" data-line-number="6"><span class="fl">0.375</span></a></code></pre></div>
<ol start="3" type="1">
<li><code>Mean_squared_log_error(MSLE)</code></li>
</ol>
<p>MSLE越小，模型拟合表现越好。当两个数值都是巨大数字时，如果不想惩罚巨大的差异，就可以使用它。当具有指数增长的目标（如人口数量，商品在一定年限内的平均销售额等）时，最好使用此度量标准。请注意，此度量标准对低于预测的估算值的惩罚高于超出预测的估算值。 <span class="math display">\[
MSLE(y, \hat y) = \frac{1}{n_{samples}} \sum_{i=0}^{n_{samples}-1} (log_e(1+y_i) - log_e(1 + \hat y_i))^ 2
\]</span></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_log_error</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">y_true <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">5</span>, <span class="fl">2.5</span>, <span class="dv">7</span>]</a>
<a class="sourceLine" id="cb9-3" data-line-number="3">y_pred <span class="op">=</span> [<span class="fl">2.5</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">8</span>]</a>
<a class="sourceLine" id="cb9-4" data-line-number="4"><span class="bu">print</span>(mean_squared_log_error(y_true, y_pred))</a>
<a class="sourceLine" id="cb9-5" data-line-number="5">[output]</a>
<a class="sourceLine" id="cb9-6" data-line-number="6"><span class="fl">0.0397</span></a></code></pre></div>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/07/TF-IDF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/07/TF-IDF/" itemprop="url">
                  TF-IDF
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-07T16:25:17+08:00">
                2018-04-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近在聚类分析中用<code>TF-IDF</code>特别多。然后今天上了一个<code>NLP</code>的课程，觉得老师把<code>TF-IDF</code>讲得非常简单易懂。下面我就来梳理一下<code>TF-IDF</code>吧。</p>
<p>首先，我们在处理一句话的时候，我们应该是希望有以下两点：</p>
<ol type="1">
<li>对于关键词而言，其具有较强的标示性</li>
<li>对于关键词而言，停用词应该自动排除</li>
</ol>
<p>而<code>TF-IDF</code>有以下好处：</p>
<ul>
<li>可以查询关键词</li>
<li><code>TF-IDF</code>其实表示的是每个单词的重要性，<code>TF-IDF</code>倾向于过滤掉常见的词语，保留重要的词语。也就是说，字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</li>
<li>可以为每个文章，句子建模。</li>
</ul>
<p><code>TF-IDF</code>分为两个部分：</p>
<ul>
<li><p>一个是<code>TF(Term Frequency)</code> <span class="math display">\[
tf = \frac{Term_{num}}{Word_{num}}
\]</span> ​</p></li>
<li><p>一个是<code>IDF(inverse document frequency)</code>。</p>
<ul>
<li><code>log(D/D_w)</code>; <code>D_w</code> is all the related documents. 而在这里为什么用<code>log</code>，我认为是为了抑制高频词汇的重要性。 <span class="math display">\[
idf = log(\frac{D_{all}}{D_w})
\]</span></li>
</ul></li>
</ul>
<p><span class="math display">\[
tfidf = tf \cdot idf
\]</span></p>
<p>那么下面我们就可以来手写一个<code>TF-IDF</code>了。当然，<code>sklearn</code>里面就有<code>TF-IDF</code>的<code>API</code></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> pandas <span class="im">as</span> pd</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> jieba</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">from</span> collections <span class="im">import</span> Counter</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="im">from</span> functools <span class="im">import</span> <span class="bu">reduce</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="im">import</span> operator <span class="im">as</span> op</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="kw">def</span> cut(string): <span class="cf">return</span> jieba.cut(string)</a>
<a class="sourceLine" id="cb1-9" data-line-number="9"></a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="co"># 统计每个分词在某一特定文档的频率</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11"><span class="kw">def</span> term_frequency(word, i):</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">    words <span class="op">=</span> cut(content.iloc[i].content)</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">    word_counts <span class="op">=</span> Counter(words)</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">    <span class="cf">return</span> word_counts[word] <span class="op">/</span> <span class="bu">sum</span>(word_counts.values())</a>
<a class="sourceLine" id="cb1-15" data-line-number="15"></a>
<a class="sourceLine" id="cb1-16" data-line-number="16"><span class="co"># 统计前n篇文章中的分词出现次数，然后返回一个列表</span></a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="kw">def</span> documents_words_counter(docu_num):</a>
<a class="sourceLine" id="cb1-18" data-line-number="18">    <span class="cf">return</span> [Counter(cut(<span class="bu">str</span>(content.iloc[i].content))) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(docu_num)]</a>
<a class="sourceLine" id="cb1-19" data-line-number="19"></a>
<a class="sourceLine" id="cb1-20" data-line-number="20"><span class="kw">def</span> inverse_document_frequency(word):</a>
<a class="sourceLine" id="cb1-21" data-line-number="21">    <span class="cf">return</span> np.log(<span class="bu">len</span>(documents_words_counter(docu_num) <span class="op">/</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> counter <span class="kw">in</span> documents_words_counter <span class="cf">if</span> word <span class="kw">in</span> counter)))</a>
<a class="sourceLine" id="cb1-22" data-line-number="22"></a>
<a class="sourceLine" id="cb1-23" data-line-number="23"><span class="kw">def</span> tfidf(word, i): <span class="cf">return</span> term_frequency(word, i) <span class="op">*</span> inverse_document_frequency(word)</a>
<a class="sourceLine" id="cb1-24" data-line-number="24"></a>
<a class="sourceLine" id="cb1-25" data-line-number="25"><span class="co"># 然后我们用tfidf的结果，去表示一个文档，然后去判断两个文档的相似性。</span></a>
<a class="sourceLine" id="cb1-26" data-line-number="26"><span class="co"># words ==&gt; vector ==&gt; 意思相似程度</span></a>
<a class="sourceLine" id="cb1-27" data-line-number="27"><span class="kw">def</span> doc_vec(w, i):</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">    all_words_counter <span class="op">=</span> <span class="bu">reduce</span>(op.add, documents_words_counter(docu_num)):</a>
<a class="sourceLine" id="cb1-29" data-line-number="29">    <span class="cf">return</span> [tfidf(w, <span class="dv">1</span>) <span class="cf">for</span> w <span class="kw">in</span> all_words_counter]</a>
<a class="sourceLine" id="cb1-30" data-line-number="30"></a>
<a class="sourceLine" id="cb1-31" data-line-number="31"><span class="co"># scikit-learn的Example用法</span></a>
<a class="sourceLine" id="cb1-32" data-line-number="32"><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</a>
<a class="sourceLine" id="cb1-33" data-line-number="33"><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> TfidfVectorizer().fit_transform(sequnece)</a></code></pre></div>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/22/Ensemble-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/22/Ensemble-Learning/" itemprop="url">
                  Ensemble Learning
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-22T20:32:11+08:00">
                2018-03-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>集成学习<code>(Ensemble Learning)</code>本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等。集成算法的目的是根据某个算法组合多个基础模型的预测值从而提高模型的泛化能力。</p>
<p>主要有两种方法：</p>
<ul>
<li><code>Averaging方法</code>：个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，基本上是独立的建多个模型然后对它们的预测结果做平均，从而降低泛化误差。代表算法是<code>bagging(boostrap aggregating)</code>和<code>随机森林(random forest)</code>。</li>
<li><code>Boosting方法</code>：个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，是有序列的、有依赖的建立多个基模型，后一个模型用来修正上一个模型的bias。代表算法是<code>boosting</code>系列算法。</li>
</ul>
<p><img src="/images/Screen%20Shot%202018-03-22%20at%208.48.18%20PM.png"></p>
<h3 id="一集成学习之结合策略"><u>一、</u>集成学习之结合策略</h3>
<h3 id="平均法"><u>平均法</u></h3>
<p>对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干弱学习器的输出进行平均得到最终的预测输出。</p>
<p>最简单的平均是算术平均。 <span class="math display">\[
H(x) = \frac{1}{T}\sum_1^{T}h_i(x)
\]</span> 如果每个个体学习器有一个权重<code>w</code>，则最终预测是 <span class="math display">\[
\begin{aligned}
H(x) &amp;= \sum_{i=1}^{T}w_ih_i(x) \\\\
w_i &amp; \ge 0, \ \ \  \sum_{i=1}^{T}w_i = 1
\end{aligned}
\]</span></p>
<h3 id="投票法"><u>投票法</u></h3>
<p>对于分类问题的预测，通常使用的是投票法。假设预测类别是<code>{C1, C2, ..., Ck}</code>，对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是<code>(h1(x), h2(x), ..., hT(x))</code></p>
<p>最简单的投票法是相对多数投票法，也就说T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。</p>
<p>而我们可以在投票总数上加限制，也就是绝对多数投票法，在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数，否则会拒绝预测结果。</p>
<p>更加复杂的就是加权投票法，类似加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值的对应的类别为最终类别。</p>
<h3 id="学习法"><u>学习法</u></h3>
<p>对弱学习器的结果做平均或者投票相对比较简单，但学习误差较大。所以才有了学习法。学习发的代表是<code>stacking</code>。但使用<code>stacking</code>的结合策略时，我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p>
<p>在这种情况下，我们将弱学习器成为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，首先用初级学习器预测一次，结果为次级学习器的输入样本，再用次级学习器预测一遍，得到最终的预测结果。</p>
<p><img src="/images/Screen%20Shot%202018-03-10%20at%206.24.38%20PM.png"></p>
<h3 id="二集成方法"><u>二、集成方法</u></h3>
<h3 id="bagging"><u>Bagging</u></h3>
<p><img src="/images/Screen%20Shot%202018-03-22%20at%209.23.32%20PM.png"></p>
<p><code>随机采样(bootstrap)</code>就是从我们的训练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于<code>Bagging</code>算法，一般会随机采集和训练样本数m一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有m个样本训练集做T次的随机采样，则由于随机性，T个采样集各不相同。</p>
<p>在每次抽样中，每个样本都可能会有<code>36.8%</code>的概率没有被抽中，所以我们可以设置<code>oob_score=True</code>来用训练好的模型去预测这部分没有被抽取的数据，评判模型的好坏，这样就不会浪费数据集。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1">sklearn.ensemble.BaggingClassifier(base_estimator<span class="op">=</span><span class="va">None</span>, n_estimators<span class="op">=</span><span class="dv">10</span>, max_samples<span class="op">=</span><span class="fl">1.0</span>, </a>
<a class="sourceLine" id="cb1-2" data-line-number="2">                                   max_features<span class="op">=</span><span class="fl">1.0</span>, bootstrap<span class="op">=</span><span class="va">True</span>, bootstrap_features<span class="op">=</span><span class="va">False</span>, </a>
<a class="sourceLine" id="cb1-3" data-line-number="3">                                   oob_score<span class="op">=</span><span class="va">False</span>, warm_start<span class="op">=</span><span class="va">False</span>, n_jobs<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="va">None</span>, </a>
<a class="sourceLine" id="cb1-4" data-line-number="4">                                   verbose<span class="op">=</span><span class="dv">0</span>)</a></code></pre></div>
<ul>
<li><p>算法</p>
<p>输入：样本集D，弱学习器算法，弱分类器迭代次数T。</p>
<p>输出：最终的强分类器<code>f(x)</code></p>
<ol type="1">
<li>对于t=1, 2, …, T，对训训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dm。然后用采样集Dm训练第m个弱学习器Gm(x)</li>
<li>如果是分类算法预测，则T个弱学习器投出最多票数的类别或类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。</li>
</ol></li>
</ul>
<h3 id="随机森林"><u>随机森林</u></h3>
<p><code>随机森林</code>是<code>bagging</code>的一个特化进阶版，也就是说随机森林的弱学习器都是<code>CART决策树</code>，而且随机森林在<code>bagging</code>的样本随机采样基础上，又加上了特征的随机选择。随机森林通过随机选择节点上的一部分样本特征，这个数字小于n，假设为n_sub，然后在这些随机选择的n_sub个样本特征中，选择一个最优特征来做决策树的左右子树划分，这样进一步增强了模型的泛化能力。</p>
<p>如果n_sub=n，则此时随机森林的CART决策树和普通CART决策树没有区别，n_sub越小，则模型越健壮，但对训练集的拟合程度越差。也就是说，n_sub越小，模型的方差会减小，也就是说预测值和预测值的期望值的差越小，数据扰动造成的影响较小，但是偏差会越大，也就是说期望预测和真实标记的误差越大，也就是说学习算法本身的拟合能力越差。换句话说，就是模型过于简单了。</p>
<p>这个时候一般会通过<code>cross validation</code>调参获取一个合适的n_sub值。</p>
<ul>
<li><p>算法</p>
<p>输入：样本集D，弱学习器算法，弱分类器迭代次数T。</p>
<p>输出：最终的强分类器<code>f(x)</code></p>
<ol type="1">
<li>对于t=1, 2, …, T，对训训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集Dm。用采样集Dm训练第m个决策树模型Gm(x)，在训练决策树模型的节点时候，在节点上所有的样本特征中选择一部分样本特征，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分</li>
<li>如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。</li>
</ol></li>
<li><p>优点</p></li>
</ul>
<ol type="1">
<li>训练可以高度并行化，具有速度优势</li>
<li>由于可以随机选择决策树节点划分特征，这样在样本特征维度很高时候，仍然能够高效的训练模型。</li>
<li>在训练后，可以给出各个特征对于输出的重要性。</li>
<li>由于采用了随机采样，训练出的模型方差小，泛化能力强</li>
<li>实现比较简单</li>
<li>对部分特征不敏感</li>
</ol>
<ul>
<li>缺点</li>
</ul>
<ol type="1">
<li>在某些噪音比较大的样本集上，随机森林模型容易过拟合</li>
<li>取值划分比较多的特征容易对随机森林的决策产生更大的影响</li>
</ol>
<h3 id="boosting"><u>Boosting</u></h3>
<p><img src="/images/Screen%20Shot%202018-03-22%20at%2010.31.01%20PM.png"></p>
<p>从图上可以看出，<code>Boosting</code>算法的工作机制是首先从训练集用初始权重训练出一个弱分析器1，根据弱学习器的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练学习器2.如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p>
<p><code>Boosting</code>系列算法里最著名算法主要有<code>AdaBoost</code>算法提升树(<code>boosting tree</code>)系列算法。提升树系列最广泛的是梯度提升树(<code>Gradient Boosting Tree</code>)。</p>
<p>Boosting算法中要解决四个问题：</p>
<ol type="1">
<li>如何计算学习误差率e</li>
<li>如何得到弱学习器权重系数a</li>
<li>如何更新样本权重D</li>
<li>使用何种结合策略</li>
</ol>
<h3 id="boostingadaboostadaptive-boosting">Boosting——<u>AdaBoost</u>(Adaptive Boosting)</h3>
<p>假设给定一个二元分类的训练集数据集 <span class="math display">\[
T = {(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)}
\]</span></p>
<ul>
<li><p>算法</p>
<p>输入：训练数据集T</p>
<p>输出：最终分类器G(x)</p></li>
</ul>
<ol type="1">
<li>初始化训练集在第k个弱学习器的权重分布</li>
</ol>
<p><span class="math display">\[
D(k)=(w_{l1}, w_{l2}, ..., w_{lN}); \ \ \ \ w_{li}=\frac{1}{N};\ \ \ \ \  i= 1, 2, ..., N
\]</span></p>
<ol start="2" type="1">
<li><p>对m=1, 2, …, M</p>
<ul>
<li><p>使用具有权值分布Dm的训练数据集学习，得到基本分类器 <span class="math display">\[
G_m(x) :\mathcal{X} \to \{-1, +1\}
\]</span></p></li>
<li><p>计算Gm(x)在训练数据集上的分类误差率。第m个弱分类器Gm(x)在训练集上的加权误差率为</p></li>
</ul>
<p><span class="math display">\[
e_k = P(G_m(x_i) \neq y_i) = \sum_{i=1}^{m}w_{mi}I \ \ \ (G_m(_i)\neq y_i)
\]</span></p>
<p>​</p>
<ul>
<li><p>计算<code>弱分类器Gm(x)的权重系数</code> <span class="math display">\[
\alpha_k = \frac{1}{2}log\frac{1- e_k}{e_k}
\]</span> 从上式可以看出，如果分类误差ek越大，则对应的<code>弱分类器权重ak</code>越小。也就是说，误差率小的<code>弱分类器权重系数</code>越大。</p></li>
<li><p>更新样本权重D，假设第k个弱分类器的样本集权重系数为 <span class="math display">\[
D(k)=(w_{k1}, w_{k2}, ..., w_{km})
\]</span> 则对应的第k+1个弱分类器的样本集权重系数为 <span class="math display">\[
\begin{aligned}
w_{k+1, i} &amp; = \begin{cases}
\frac{w_{ki}}{Z_k}e^{-\alpha_m}, \ \ \ \ G_m(x_i) = y_i\\[2ex]
\frac{w_{ki}}{Z_k}e^{\alpha_m}, \ \ \ \ G_m(x_i) \neq y_i
\end{cases}\\\\
  &amp; Z 为规范化因子 \\\\
Z_k &amp;= \sum_{i=1}^{m}w_{ki}exp(-\alpha_ky_iG_k(x_i))
\end{aligned}
\]</span> 从<code>w_{k+1, i}</code>计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)&lt;0, 导致样本的权重在第k+1个弱分类器中增大，如果分类正确，则样本权重在第k+1个弱分类器中减少</p></li>
</ul></li>
<li><p>构建基本分类器的线性组合。<code>AdaBoost</code>采用的是加权平均法，最终的强分类器为 <span class="math display">\[
G(x) = sign(f(x)) = sign(\sum_{m=1}^{M}\alpha_mG_m(x))
\]</span> ​</p></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/17/KNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/17/KNN/" itemprop="url">
                  K-Nearest Neighbor
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-17T01:38:56+08:00">
                2018-02-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><code>KNN(k-nearest neighbor)</code>应该是分类和回归算法里面最简单的一个了。这个笔记主要分享下<code>KNN</code>以及<code>KD树</code>。</p>
<p><code>k近邻法</code>假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，<code>k近邻法</code>不具有显式的学习过程。<code>k近邻法</code>实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。</p>
<p><code>k值的选择</code>、<code>距离度量</code>及<code>分类决策规则</code>是<code>k近邻法</code>的三个基本要素。</p>
<h2 id="距离度量"><u>距离度量</u></h2>
<p>特征空间中两个实例点的距离是两个实例点相似程度的反映。</p>
<p>设特征空间是n维实数向量空间R^n， 其中 <span class="math display">\[
\begin{aligned}
&amp; x_i, x_j \in \mathcal{X} \\
\\
&amp; x_i = (x_i^{(1)}, x_i^{(2)},  ... , x_i^{(n)})^T \\
\\
&amp; x_j = (x_j^{(1)}, x_j^{(2)},  ... , x_j^{(n)})^T \\
\end{aligned}
\]</span> 因此，<code>x_i</code>, <code>x_j</code>的<code>L_p</code>距离定义为 <span class="math display">\[
L_p(x_i, x_j)  = \Biggl( \sum_{i=1}^n|x_i^{(l)} - x_j^{(l)}|^p \Biggr)^{\frac {1}{p}}, \ \ \ p \ge1
\]</span> 当<code>p=2</code>， 称为<code>欧式距离(Euclidean distance)</code>， 即 <span class="math display">\[
L_2(x_i, x_j)  = \Biggl( \sum_{i=1}^n|x_i^{(l)} - x_j^{(l)}|^2 \Biggr)^{\frac {1}{2}}
\]</span> 当<code>p=1</code>， 称为<code>曼哈顿距离(Manhattan distance)</code>， 即 <span class="math display">\[
L_1(x_i, x_j)  = \sum_{i=1}^n|x_i^{(l)} - x_j^{(l)}|
\]</span> 当<code>p=∞</code>，它是各个坐标距离的最大值，即 <span class="math display">\[
L_\infty(x_i, x_j)  = \max_{l}|x_i^{(l)} - x_j^{(l)}|
\]</span></p>
<h2 id="k值的选择"><u>k值的选择</u></h2>
<p>如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测。“学习”的<code>近似误差(approximation error)</code>会减少，只有与输入实例较近的训练实例才会对预测结果起作用。但“学习”的<code>估计误差(estimation error)</code>会增大。k值的减小意味着整体模型变得复杂，容易发生过拟合。</p>
<p>如果选择较大的k值，相当于用较大领域中的训练实例进行预测，整体模型变得简单，“学习”的<code>估计误差(estimation error)</code>会减少，但是“学习”的<code>近似误差(approximation error)</code>会增大。</p>
<p>在应用中，k值一般取一个比较小的值，通常采用交叉验证法来选取最优的k值。</p>
<h2 id="k近邻法的实现kd树"><u>k近邻法的实现：kd树</u></h2>
<p>实现<code>k近邻法</code>时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。其中比较高效的方法是<code>kd树</code>。</p>
<p><code>kd树</code>是二叉树，表示对k维空间的一个划分。构造<code>kd树</code>的过程就是不断地对第l维<code>(l={1, 2, ..., p})</code>维取中位数的过程，中位数对应的对象就是二叉树中的结点。</p>
<h3 id="构造平衡kd树"><u>构造平衡kd树</u></h3>
<p>首先给出伪代码：</p>
<ol type="1">
<li>历遍所有维度，找到方差最大的维度</li>
<li>以这个维度上的点的数值进行排序，找到其中间点</li>
<li>以这个点为划分，递归建立左子树</li>
<li>以这个点为划分，递归建立右子树</li>
<li>当数据集内没有点时，退出函数</li>
</ol>
<p>这里给出<code>两个重要概念</code>：</p>
<ul>
<li><p>以方差最大维度为划分的维度：方差越大，代表着这个维度上的数据波动越大，代表着以这个维度划分数据，可以最广泛的把数据集分开</p></li>
<li><p>取中位点为划分点，有助有构造一个平衡二叉树，不至于出现二叉树有时候会出现的极端，即是一个父节点只有一个孩子节点，使树的深度大大加深，增加搜索的复杂度。</p></li>
</ul>
<p><code>输入</code>： k维空间的数据集<code>T={x1, x2, ..., xN}</code></p>
<p>其中<code>x_i = (xi1, xi2, ..., xik)^T</code>, i = 1, 2, … , N</p>
<p><code>输出</code>： kd树</p>
<ol type="1">
<li><p>开始：选择根结点。</p>
<ul>
<li><p>选择<code>x^(1)</code>为坐标轴，以T中所有实例的<code>x^1</code>坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域，切分由通过切分点并与坐标轴<code>x^1</code>垂直的超平面实现。</p></li>
<li><p>由根结点生成深度为1的左、右子结点：左子结点对应坐标<code>x^(i)</code>小于切分点的子区域，右子结点对应于坐标<code>x^(i)</code>大于切分点的子区域。</p></li>
</ul></li>
<li><p>重复：</p>
<ul>
<li>对深度为<code>j</code>的结点，选择<code>x^(l)</code>为切分的坐标轴，<code>l=j(modk)+1</code>。以该结点的区域中所有实例的<code>x^(l)</code>坐标的中位数为切分点，将该节点对应的矩形区域切分为两个子区域。切分由通过切分点并与坐标轴<code>x^(l)</code>垂直的超平面实现。</li>
<li>由根结点生成深度为<code>j+1</code>的左、右子结点：左子结点对应坐标<code>x^(l)</code>小于切分点的子区域，右子结点对应于坐标<code>x^(l)</code>大于切分点的子区域。</li>
</ul></li>
<li><p>直到两个子区域没有实例存在时停止。从而形成<code>kd树</code>的划分。</p></li>
</ol>
<h3 id="查找k个最小值"><u>查找K个最小值</u></h3>
<ol type="1">
<li>若该节点是叶子结点，则返回</li>
<li>若不是叶子结点，则比较待预测节点与该节点被划分的维度上的值，若小于，则去其左子树。</li>
<li>若不是叶子结点，则比较待预测节点与该节点被划分的维度上的值，若大于，则去其右子树。</li>
</ol>
<p>我们用一个k大小的优先队列来存储k个节点的值：</p>
<ol type="1">
<li>若队列的长度不满k个，则把当前节点入队，并且去该父节点的另外一个子节点比较。</li>
<li>若已经满了k个，则取距离最长的节点，计算其距离，设为k。在计算预测结点到该节点的父节点的所划分的维度的距离，设为d。如K&gt;d，则去改父节点的另一个子节点查找。否则，继续回退到该节点的父节点的父节点。</li>
</ol>
<p>然后我们来看看如何用代码来实现。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># 首先写一个简单的KNN。</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">import</span> operator</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="im">from</span> os <span class="im">import</span> listdir</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="kw">def</span> createDataSet():</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">    group <span class="op">=</span> np.array([[<span class="fl">1.0</span>, <span class="fl">1.1</span>], [<span class="fl">1.0</span>, <span class="fl">1.0</span>], [<span class="dv">0</span>, <span class="dv">0</span>],[<span class="dv">0</span>, <span class="fl">0.1</span>]])</a>
<a class="sourceLine" id="cb1-9" data-line-number="9">    labels <span class="op">=</span> [<span class="st">&#39;A&#39;</span>, <span class="st">&#39;A&#39;</span>, <span class="st">&#39;B&#39;</span>, <span class="st">&#39;B&#39;</span>]</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">    <span class="cf">return</span> group, labels</a>
<a class="sourceLine" id="cb1-11" data-line-number="11"></a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="kw">def</span> knn(inX, dataSet, labels, k):</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">    dataSetSise <span class="op">=</span> dataSet.shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">    diffMat <span class="op">=</span> np.tile(inX, (dataSetSise, <span class="dv">1</span>)) <span class="op">-</span> dataSet</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">    sqDiffMat <span class="op">=</span> diffMat <span class="op">**</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb1-16" data-line-number="16">    sqDistances <span class="op">=</span> sqDiffMat.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1-17" data-line-number="17">    distances <span class="op">=</span> sqDistances <span class="op">**</span> <span class="fl">0.5</span></a>
<a class="sourceLine" id="cb1-18" data-line-number="18">    sortedDistIndices <span class="op">=</span> distances.argsort()</a>
<a class="sourceLine" id="cb1-19" data-line-number="19">    classCount <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</a>
<a class="sourceLine" id="cb1-21" data-line-number="21">        voteIlabel <span class="op">=</span> labels[sortedDistIndices[i]]</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">        classCount[voteIlabel] <span class="op">=</span> classCount.get(voteIlabel, <span class="dv">0</span>) <span class="op">+</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-23" data-line-number="23">    sortedDistIndices <span class="op">=</span> <span class="bu">sorted</span>(classCount.items(), key<span class="op">=</span>operator.itemgetter(<span class="dv">1</span>), reverse<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">    <span class="cf">return</span> sortedDistIndices[<span class="dv">0</span>][<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb1-25" data-line-number="25"></a>
<a class="sourceLine" id="cb1-26" data-line-number="26">dataSet, labels <span class="op">=</span> createDataSet()</a>
<a class="sourceLine" id="cb1-27" data-line-number="27"><span class="bu">print</span>(knn([<span class="dv">0</span>, <span class="dv">0</span>], dataSet, labels, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb1-28" data-line-number="28"></a>
<a class="sourceLine" id="cb1-29" data-line-number="29">[output]</a>
<a class="sourceLine" id="cb1-30" data-line-number="30">B</a>
<a class="sourceLine" id="cb1-31" data-line-number="31"></a>
<a class="sourceLine" id="cb1-32" data-line-number="32"><span class="co"># 下面就是kd树。</span></a>
<a class="sourceLine" id="cb1-33" data-line-number="33"><span class="kw">class</span> KD_node(<span class="bu">object</span>):</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, point<span class="op">=</span><span class="va">None</span>, split<span class="op">=</span><span class="va">None</span>, LL<span class="op">=</span><span class="va">None</span>, RR<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb1-35" data-line-number="35">        <span class="va">self</span>.point <span class="op">=</span> point</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">        <span class="va">self</span>.split <span class="op">=</span> split</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">        <span class="va">self</span>.left <span class="op">=</span> LL</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">        <span class="va">self</span>.right <span class="op">=</span> RR</a>
<a class="sourceLine" id="cb1-39" data-line-number="39"></a>
<a class="sourceLine" id="cb1-40" data-line-number="40"><span class="kw">def</span> createKDTree(root, data_list):</a>
<a class="sourceLine" id="cb1-41" data-line-number="41">    length <span class="op">=</span> <span class="bu">len</span>(data_list)</a>
<a class="sourceLine" id="cb1-42" data-line-number="42">    <span class="cf">if</span> length <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">        <span class="cf">return</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb1-44" data-line-number="44">    dimension <span class="op">=</span> <span class="bu">len</span>(data_list[<span class="dv">0</span>])</a>
<a class="sourceLine" id="cb1-45" data-line-number="45">    max_var <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb1-46" data-line-number="46">    split <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb1-47" data-line-number="47">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dimension):</a>
<a class="sourceLine" id="cb1-48" data-line-number="48">        l1 <span class="op">=</span> []</a>
<a class="sourceLine" id="cb1-49" data-line-number="49">        <span class="cf">for</span> t <span class="kw">in</span> data_list:</a>
<a class="sourceLine" id="cb1-50" data-line-number="50">            l1.append(t[i])</a>
<a class="sourceLine" id="cb1-51" data-line-number="51">        var <span class="op">=</span> computeVariance(l1)</a>
<a class="sourceLine" id="cb1-52" data-line-number="52">        <span class="cf">if</span> var <span class="op">&gt;</span> max_var:</a>
<a class="sourceLine" id="cb1-53" data-line-number="53">            max_var <span class="op">=</span> var</a>
<a class="sourceLine" id="cb1-54" data-line-number="54">            split <span class="op">=</span> i</a>
<a class="sourceLine" id="cb1-55" data-line-number="55"></a>
<a class="sourceLine" id="cb1-56" data-line-number="56">    data_list <span class="op">=</span> <span class="bu">sorted</span>(data_list, key<span class="op">=</span><span class="kw">lambda</span> x: x[split])</a>
<a class="sourceLine" id="cb1-57" data-line-number="57">    point <span class="op">=</span> data_list[<span class="bu">int</span>(length<span class="op">/</span><span class="dv">2</span>)]</a>
<a class="sourceLine" id="cb1-58" data-line-number="58">    root <span class="op">=</span> KD_node(point, split)</a>
<a class="sourceLine" id="cb1-59" data-line-number="59">    root.left <span class="op">=</span> createKDTree(root.left, data_list[<span class="dv">0</span>:<span class="bu">int</span>(length<span class="op">/</span><span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb1-60" data-line-number="60">    root.right <span class="op">=</span> createKDTree(root.right, data_list[<span class="bu">int</span>(length<span class="op">/</span><span class="dv">2</span>)<span class="op">+</span><span class="dv">1</span>:length])</a>
<a class="sourceLine" id="cb1-61" data-line-number="61">    <span class="cf">return</span> root</a>
<a class="sourceLine" id="cb1-62" data-line-number="62"></a>
<a class="sourceLine" id="cb1-63" data-line-number="63"><span class="kw">def</span> computeVariance(arrayList):</a>
<a class="sourceLine" id="cb1-64" data-line-number="64">    arrayList <span class="op">=</span> np.array(arrayList)</a>
<a class="sourceLine" id="cb1-65" data-line-number="65">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(arrayList)):</a>
<a class="sourceLine" id="cb1-66" data-line-number="66">        arrayList[i] <span class="op">=</span> <span class="bu">float</span>(arrayList[i])</a>
<a class="sourceLine" id="cb1-67" data-line-number="67">    length <span class="op">=</span> <span class="bu">len</span>(arrayList)</a>
<a class="sourceLine" id="cb1-68" data-line-number="68">    sum1 <span class="op">=</span> arrayList.<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb1-69" data-line-number="69">    array2 <span class="op">=</span> arrayList <span class="op">*</span> arrayList</a>
<a class="sourceLine" id="cb1-70" data-line-number="70">    sum2 <span class="op">=</span> array2.<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb1-71" data-line-number="71">    mean <span class="op">=</span> sum1 <span class="op">/</span> length</a>
<a class="sourceLine" id="cb1-72" data-line-number="72">    variance <span class="op">=</span> sum2 <span class="op">/</span> length <span class="op">-</span> mean <span class="op">**</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb1-73" data-line-number="73"></a>
<a class="sourceLine" id="cb1-74" data-line-number="74"><span class="kw">def</span> computerDistance(pt1, pt2):</a>
<a class="sourceLine" id="cb1-75" data-line-number="75">    <span class="bu">sum</span> <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span></a>
<a class="sourceLine" id="cb1-76" data-line-number="76">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(pt1)):</a>
<a class="sourceLine" id="cb1-77" data-line-number="77">        <span class="bu">sum</span> <span class="op">+=</span> (pt1[i] <span class="op">-</span> pt2[i]) <span class="op">**</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb1-78" data-line-number="78">    <span class="cf">return</span> <span class="bu">sum</span> <span class="op">**</span> <span class="fl">0.5</span></a>
<a class="sourceLine" id="cb1-79" data-line-number="79"></a>
<a class="sourceLine" id="cb1-80" data-line-number="80"><span class="kw">def</span> findNN(root, query, k):</a>
<a class="sourceLine" id="cb1-81" data-line-number="81">    min_dist <span class="op">=</span> computerDistance(query, root.point)</a>
<a class="sourceLine" id="cb1-82" data-line-number="82">    node_k <span class="op">=</span> []</a>
<a class="sourceLine" id="cb1-83" data-line-number="83">    nodeList <span class="op">=</span> []</a>
<a class="sourceLine" id="cb1-84" data-line-number="84">    temp_root <span class="op">=</span> root</a>
<a class="sourceLine" id="cb1-85" data-line-number="85">    <span class="cf">while</span> temp_root:</a>
<a class="sourceLine" id="cb1-86" data-line-number="86">        nodeList.append(temp_root)</a>
<a class="sourceLine" id="cb1-87" data-line-number="87">        dd <span class="op">=</span> computerDistance(query, temp_root.point)</a>
<a class="sourceLine" id="cb1-88" data-line-number="88">        <span class="cf">if</span> <span class="bu">len</span>(node_k) <span class="op">&lt;</span> k:</a>
<a class="sourceLine" id="cb1-89" data-line-number="89">            node_k.append(dd)</a>
<a class="sourceLine" id="cb1-90" data-line-number="90">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb1-91" data-line-number="91">            max_dist <span class="op">=</span> <span class="bu">max</span>(node_k)</a>
<a class="sourceLine" id="cb1-92" data-line-number="92">            <span class="cf">if</span> dd <span class="op">&lt;</span> max_dist:</a>
<a class="sourceLine" id="cb1-93" data-line-number="93">                index <span class="op">=</span> node_k.index(max_dist)</a>
<a class="sourceLine" id="cb1-94" data-line-number="94">                <span class="kw">del</span>(node_k[index])</a>
<a class="sourceLine" id="cb1-95" data-line-number="95">                node_k.append(dd)</a>
<a class="sourceLine" id="cb1-96" data-line-number="96">        ss <span class="op">=</span> temp_root.split</a>
<a class="sourceLine" id="cb1-97" data-line-number="97">        <span class="cf">if</span> query[ss] <span class="op">&lt;=</span> temp_root.point[ss]:</a>
<a class="sourceLine" id="cb1-98" data-line-number="98">            temp_root <span class="op">=</span> temp_root.left</a>
<a class="sourceLine" id="cb1-99" data-line-number="99">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb1-100" data-line-number="100">            temp_root <span class="op">=</span> temp_root.right</a>
<a class="sourceLine" id="cb1-101" data-line-number="101">    <span class="bu">print</span>(<span class="st">&#39;node_k: &#39;</span>, node_k)</a>
<a class="sourceLine" id="cb1-102" data-line-number="102"></a>
<a class="sourceLine" id="cb1-103" data-line-number="103">    <span class="cf">while</span> nodeList:</a>
<a class="sourceLine" id="cb1-104" data-line-number="104">        back_point <span class="op">=</span> nodeList.pop()</a>
<a class="sourceLine" id="cb1-105" data-line-number="105">        ss <span class="op">=</span> back_point.split</a>
<a class="sourceLine" id="cb1-106" data-line-number="106">        max_dist <span class="op">=</span> <span class="bu">max</span>(node_k)</a>
<a class="sourceLine" id="cb1-107" data-line-number="107">        <span class="cf">if</span> <span class="bu">len</span>(node_k) <span class="op">&lt;</span> k <span class="kw">or</span> <span class="bu">abs</span>(query[ss] <span class="op">-</span> back_point.point[ss]) <span class="op">&lt;</span> max_dist:</a>
<a class="sourceLine" id="cb1-108" data-line-number="108">            <span class="cf">if</span> query[ss] <span class="op">&lt;=</span> back_point.point[ss]:</a>
<a class="sourceLine" id="cb1-109" data-line-number="109">                temp_root <span class="op">=</span> back_point.right</a>
<a class="sourceLine" id="cb1-110" data-line-number="110">            <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb1-111" data-line-number="111">                temp_root <span class="op">=</span> back_point.left</a>
<a class="sourceLine" id="cb1-112" data-line-number="112">            <span class="cf">if</span> temp_root:</a>
<a class="sourceLine" id="cb1-113" data-line-number="113">                nodeList.append(temp_root)</a>
<a class="sourceLine" id="cb1-114" data-line-number="114">                curDist <span class="op">=</span> computerDistance(temp_root.point, query)</a>
<a class="sourceLine" id="cb1-115" data-line-number="115">                <span class="cf">if</span> max_dist <span class="op">&gt;</span> curDist <span class="kw">and</span> <span class="bu">len</span>(node_k) <span class="op">==</span> k:</a>
<a class="sourceLine" id="cb1-116" data-line-number="116">                    index <span class="op">=</span> node_k.index(max_dist)</a>
<a class="sourceLine" id="cb1-117" data-line-number="117">                    <span class="kw">del</span>(node_k[index])</a>
<a class="sourceLine" id="cb1-118" data-line-number="118">                    node_k.append(curDist)</a>
<a class="sourceLine" id="cb1-119" data-line-number="119">                <span class="cf">elif</span> <span class="bu">len</span>(node_k) <span class="op">&lt;</span> k:</a>
<a class="sourceLine" id="cb1-120" data-line-number="120">                    node_k.append(curDist)</a>
<a class="sourceLine" id="cb1-121" data-line-number="121">    <span class="cf">return</span> node_k</a></code></pre></div>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/04/决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Peter Tsung">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/pandas.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="彼得的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/04/决策树/" itemprop="url">
                  Decision Tree
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-04T19:38:58+08:00">
                2018-02-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine_learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>今天终于有时间来总结一下决策树了。</p>
<p>决策树是一种用于分类或者回归的<code>无参数学习</code>的监督方法。决策树的特点是它总是在沿着特征做切分，随着层层递进，这个划分会越来越细。</p>
<p>决策树学习包括三个步骤： <code>特征选择</code>、<code>决策树的生成</code>和<code>决策树的修剪</code>。</p>
<p>用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点。每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶节点。最后将实例分到叶结点的类中。</p>
<p>决策树的路径或其对应的<code>if-then</code>规则集合具有一个重要性质：互斥并且完备。就是说每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。</p>
<h2 id="特征选择"><u>特征选择</u></h2>
<p>特征选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。通常特征选择的准则是<code>信息增益</code>或<code>信息增益比</code>。</p>
<h3 id="信息增益"><u>信息增益</u></h3>
<p>熵<code>(entropy)</code>是随机变量不确定性的度量。</p>
<ul>
<li>设X是一个取有限个值的离散随机变量，其概率分布为</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
P(X=X_i) = p_ i, \ \ \ i = 1, 2, ...,n
\end{aligned}
\]</span></p>
<p>则随机变量X的熵定义为 <span class="math display">\[
H(X)=-\sum_{i=1}^{n}p_ilog\ p_i
\]</span> 熵通常以<code>2</code>为底或以<code>e</code>为底（自然对数），这是熵的单位分别称作<code>比特(bit)</code>或<code>纳特(nat)</code>。由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作<code>H(p)</code>。 <span class="math display">\[
H(p)=-\sum_{i=1}^{n}p_ilog\ p_i
\]</span> 熵越大， 随机变量的不确定性就越大。从上面式子我们可知 <span class="math display">\[
0 \le H(p) \le log\ n
\]</span> 如果随机变量只取两个值，当p为0或1的时候，熵为0，随机变量完全没有不确定性，当p为0.5时，<code>H(p)=1</code>，熵取值最大，随机变量不确定性最大。</p>
<ul>
<li>设有随机变量(X, Y)，其联合概率分布为</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
P(X=x_i, Y=y_i) = p_ {ij}, \ \ \ i = 1, 2, ...,n; \ \ \ j = 1, 2, ..., m
\end{aligned}
\]</span></p>
<p>条件熵<code>H(X|Y)</code>表示在已知随机变量<code>X</code>的条件下随机变量<code>Y</code>的不确定性，随机变量X给定的条件下随机变量<code>Y</code>的<code>条件熵(Condition entropy)H(Y|X)</code>，定义为X给定条件下<code>Y</code>的条件概率分布的熵对<code>X</code>的数学期望 <span class="math display">\[
\begin{aligned}
H(Y|X) &amp; = \sum_{i=1}^{n}p_iH(Y|X=x_i), \\  
\\
&amp;p_i = P(X=x_i), i = 1, 2, ..., n
\end{aligned}
\]</span></p>
<ul>
<li><code>特征A</code>对训练数据集D的信息增益<code>g(D, A)</code>，定义为<code>集合D</code>的经<code>验熵H(D)</code>与<code>特征A</code>给定条件下D的<code>经验条件熵H(D|A)</code>之差，即 <span class="math display">\[
g(D, A) = H(D) - H(D|A)
\]</span></li>
</ul>
<p>信息增益越大，该特征越适合作为分裂点。</p>
<p>对<code>训练数据集（或子集）D</code>，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>
<p>但是，信息增益存在偏向于选择取值较多的特征的问题，使用<code>信息增益比(information gain ratio)</code>可以对这一问题进行校正。</p>
<p>特征A对<code>训练数据D</code>的<code>信息增益比gR(D, A)</code>定义为其<code>信息增益g(D, A)</code>与<code>训练集D</code>关于<code>特征A</code>的值的<code>熵HA(D)</code>之比，即 <span class="math display">\[
\begin{aligned}
g_R(D, A) &amp;= \frac{g(D, A)}{H_A(D)} \\
\\
H_A(D) &amp;= -\sum_{i=1}^{n}\frac{|D_i|}{D}log_2\frac{D_i}{D},   \ \ \ n是特征A取值的个数。
\end{aligned}
\]</span> 下面是<code>Python</code>代码。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">def</span> createDataSet():</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">    dataSet <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">1</span>, <span class="st">&#39;yes&#39;</span>],</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">               [<span class="dv">1</span>, <span class="dv">1</span>, <span class="st">&#39;yes&#39;</span>],</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">               [<span class="dv">1</span>, <span class="dv">0</span>, <span class="st">&#39;no&#39;</span>],</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">               [<span class="dv">0</span>, <span class="dv">1</span>, <span class="st">&#39;no&#39;</span>],</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">               [<span class="dv">0</span>, <span class="dv">1</span>, <span class="st">&#39;no&#39;</span>]]</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">    labels <span class="op">=</span> [<span class="st">&#39;no surfacing&#39;</span>,<span class="st">&#39;flippers&#39;</span>]</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">    <span class="cf">return</span> dataSet, labels</a>
<a class="sourceLine" id="cb1-9" data-line-number="9"></a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="kw">def</span> calcShannonEnt(dataSet):</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">    numEntries <span class="op">=</span> <span class="bu">len</span>(dataSet)</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">    labelCounts <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb1-13" data-line-number="13">    <span class="cf">for</span> featVec <span class="kw">in</span> dataSet:</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">        currentLabel <span class="op">=</span> featVec[<span class="op">-</span><span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">        <span class="cf">if</span> currentLabel <span class="kw">not</span> <span class="kw">in</span> labelCounts.keys(): labelCounts[currentLabel] <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb1-16" data-line-number="16">        labelCounts[currentLabel] <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-17" data-line-number="17">        </a>
<a class="sourceLine" id="cb1-18" data-line-number="18">    shannonEnt <span class="op">=</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb1-19" data-line-number="19">    <span class="cf">for</span> key <span class="kw">in</span> labelCounts:</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">        prob <span class="op">=</span> <span class="bu">float</span>(labelCounts[key])<span class="op">/</span>numEntries</a>
<a class="sourceLine" id="cb1-21" data-line-number="21">        shannonEnt <span class="op">-=</span> prob <span class="op">*</span> log(prob, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">    <span class="cf">return</span> shannonEnt</a>
<a class="sourceLine" id="cb1-23" data-line-number="23"></a>
<a class="sourceLine" id="cb1-24" data-line-number="24"><span class="kw">def</span> splitDataSet(dataSet, axis, value):</a>
<a class="sourceLine" id="cb1-25" data-line-number="25">    retDataSet <span class="op">=</span> []</a>
<a class="sourceLine" id="cb1-26" data-line-number="26">    <span class="cf">for</span> featVec <span class="kw">in</span> dataSet:</a>
<a class="sourceLine" id="cb1-27" data-line-number="27">        <span class="cf">if</span> featVec[axis] <span class="op">==</span> value:</a>
<a class="sourceLine" id="cb1-28" data-line-number="28">            reducedFeatVec <span class="op">=</span> featVec[:axis]</a>
<a class="sourceLine" id="cb1-29" data-line-number="29">            reducedFeatVec.extend(featVec[axis<span class="op">+</span><span class="dv">1</span>:])</a>
<a class="sourceLine" id="cb1-30" data-line-number="30">            retDataSet.append(reducedFeatVec)</a>
<a class="sourceLine" id="cb1-31" data-line-number="31">    <span class="cf">return</span> retDataSet</a>
<a class="sourceLine" id="cb1-32" data-line-number="32"></a>
<a class="sourceLine" id="cb1-33" data-line-number="33"><span class="kw">def</span> chooseBestFeatureToSplit(dataSet):</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">    numFeatures <span class="op">=</span> <span class="bu">len</span>(dataSet[<span class="dv">0</span>]) <span class="op">-</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-35" data-line-number="35">    baseEntropy <span class="op">=</span> calcShannonEnt(dataSet)</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">    bestInfoGain <span class="op">=</span> <span class="fl">0.0</span><span class="op">;</span> bestFeature <span class="op">=</span> <span class="dv">-1</span></a>
<a class="sourceLine" id="cb1-37" data-line-number="37">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(numFeatures):</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">        featList <span class="op">=</span> [example[i] <span class="cf">for</span> example <span class="kw">in</span> dataSet]</a>
<a class="sourceLine" id="cb1-39" data-line-number="39">        uniqueVals <span class="op">=</span> <span class="bu">set</span>(featList)</a>
<a class="sourceLine" id="cb1-40" data-line-number="40">        newEntropy <span class="op">=</span> <span class="fl">0.0</span></a>
<a class="sourceLine" id="cb1-41" data-line-number="41">        <span class="cf">for</span> value <span class="kw">in</span> uniqueVals:</a>
<a class="sourceLine" id="cb1-42" data-line-number="42">            subDataSet <span class="op">=</span> splitDataSet(dataSet, i, value)</a>
<a class="sourceLine" id="cb1-43" data-line-number="43">            prob <span class="op">=</span> <span class="bu">len</span>(subDataSet) <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(dataSet))</a>
<a class="sourceLine" id="cb1-44" data-line-number="44">            newEntropy <span class="op">+=</span> prob <span class="op">*</span> calcShannonEnt(subDataSet)</a>
<a class="sourceLine" id="cb1-45" data-line-number="45">        infoGain <span class="op">=</span> baseEntropy <span class="op">-</span> newEntropy</a>
<a class="sourceLine" id="cb1-46" data-line-number="46">        <span class="cf">if</span> (infoGain <span class="op">&gt;</span> bestInfoGain):</a>
<a class="sourceLine" id="cb1-47" data-line-number="47">            bestInfoGain <span class="op">=</span> infoGain</a>
<a class="sourceLine" id="cb1-48" data-line-number="48">            bestFeature <span class="op">=</span> i</a>
<a class="sourceLine" id="cb1-49" data-line-number="49">    <span class="cf">return</span> bestFeature</a></code></pre></div>
<h2 id="决策树的生成"><u>决策树的生成</u></h2>
<h3 id="id3算法"><u>ID3算法</u></h3>
<p>输入：训练数据集D，特征集A，阈值e</p>
<p>输出：决策树T</p>
<ul>
<li>算法
<ol type="1">
<li>若D中所有的实例属于同一类Ck，则T为单节点树，并将类Ck作为该结点的类标记，返回T</li>
<li>若A=空集，则T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T</li>
<li>否则，按照计算A中各特征对D的信息增益，选择信息增益最大的特征Ag</li>
<li>如果Ag的信息增益小于阈值e，则置T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T</li>
<li>否则，对Ag的每一可能值ai，将D分割为若干非空子集Di，将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T</li>
<li>对第i个子节点，以Di为训练集，以A-{Ag}为特征集，递归地调用1-5，得到子树Ti，返回Ti。</li>
</ol></li>
<li>问题：
<ol type="1">
<li>不能处理连续值</li>
<li>容易偏向取值较多的特征</li>
<li>过拟合</li>
</ol></li>
</ul>
<p><code>ID3</code>算法生成的树容易产生过拟合，而且它不能直接处理连续型特征，只有事先将连续型特征转换成离散型，才能在<code>ID3</code>算法中使用。但这种转换过程会破坏连续型变量的内在性值。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> majorityCnt(classList):</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    classCount<span class="op">=</span>{}</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">    <span class="cf">for</span> vote <span class="kw">in</span> classList:</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">        <span class="cf">if</span> vote <span class="kw">not</span> <span class="kw">in</span> classCount.keys(): classCount[vote] <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">        classCount[vote] <span class="op">+=</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6">    sortedClassCount <span class="op">=</span> <span class="bu">sorted</span>(classCount.iteritems(), key<span class="op">=</span>operator.itemgetter(<span class="dv">1</span>), reverse<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">    <span class="cf">return</span> sortedClassCount[<span class="dv">0</span>][<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb2-8" data-line-number="8"></a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="kw">def</span> createTree(dataSet,labels):</a>
<a class="sourceLine" id="cb2-10" data-line-number="10">    classList <span class="op">=</span> [example[<span class="op">-</span><span class="dv">1</span>] <span class="cf">for</span> example <span class="kw">in</span> dataSet]</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">    <span class="cf">if</span> classList.count(classList[<span class="dv">0</span>]) <span class="op">==</span> <span class="bu">len</span>(classList): </a>
<a class="sourceLine" id="cb2-12" data-line-number="12">        <span class="cf">return</span> classList[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb2-13" data-line-number="13">    <span class="cf">if</span> <span class="bu">len</span>(dataSet[<span class="dv">0</span>]) <span class="op">==</span> <span class="dv">1</span>: </a>
<a class="sourceLine" id="cb2-14" data-line-number="14">        <span class="cf">return</span> majorityCnt(classList)</a>
<a class="sourceLine" id="cb2-15" data-line-number="15">    bestFeat <span class="op">=</span> chooseBestFeatureToSplit(dataSet)</a>
<a class="sourceLine" id="cb2-16" data-line-number="16">    bestFeatLabel <span class="op">=</span> labels[bestFeat]</a>
<a class="sourceLine" id="cb2-17" data-line-number="17">    myTree <span class="op">=</span> {bestFeatLabel:{}}</a>
<a class="sourceLine" id="cb2-18" data-line-number="18">    <span class="kw">del</span>(labels[bestFeat])</a>
<a class="sourceLine" id="cb2-19" data-line-number="19">    featValues <span class="op">=</span> [example[bestFeat] <span class="cf">for</span> example <span class="kw">in</span> dataSet]</a>
<a class="sourceLine" id="cb2-20" data-line-number="20">    uniqueVals <span class="op">=</span> <span class="bu">set</span>(featValues)</a>
<a class="sourceLine" id="cb2-21" data-line-number="21">    <span class="cf">for</span> value <span class="kw">in</span> uniqueVals:</a>
<a class="sourceLine" id="cb2-22" data-line-number="22">        subLabels <span class="op">=</span> labels[:]       </a>
<a class="sourceLine" id="cb2-23" data-line-number="23">        myTree[bestFeatLabel][value] <span class="op">=</span> createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</a>
<a class="sourceLine" id="cb2-24" data-line-number="24">    <span class="cf">return</span> myTree                            </a></code></pre></div>
<h3 id="c4.5算法"><u>C4.5算法</u></h3>
<p>输入：训练数据集D，特征集A，阈值e</p>
<p>输出：决策树T</p>
<ul>
<li>算法
<ol type="1">
<li>若D中所有实例属于同一类Ck，则置T为单节点树，并将Ck作为该节点的类，返回T</li>
<li>如果A为空集，则置T为单节点树，并将D中实例数最大的类Ck作为该结点的类，返回T</li>
<li>否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征A</li>
<li>如果Ag的信息增益比小于阈值e，则置T为单节点树，并将D中实例数最大的类Ck作为该节点的类，返回T</li>
<li>否则，对Ag的每一可能值ai，依Ag=ai将D分割为子集若干非空Di，将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T</li>
<li>对结点i，以Di为训练集，以A-{Ag}为特征集，递归地调用1-5，得到子树Ti，返回Ti。</li>
</ol></li>
<li>优点
<ol type="1">
<li><code>C4.5</code>在生成过程中，用信息增益比来选择特征，与<code>ID3</code>算法相似，而且它可以处理连续的和有缺失值的数据。</li>
</ol></li>
<li>缺点
<ol type="1">
<li>生成的是多叉树</li>
<li>只能用于分类</li>
<li>使用了熵模型，涉及大量的对数运算，很耗时</li>
</ol></li>
</ul>
<h3 id="cartclassification-and-regression-tree算法"><u>CART</u>(Classification and regression tree)算法</h3>
<p><code>CART</code>是目前最主流的决策树算法，既能生成分类树，也能生成回归树。</p>
<h3 id="cart分类树"><u>CART分类树</u></h3>
<p><code>CART</code>在处理连续值的时候，思想与<code>C4.5</code>是类似的，唯一的不同就是<code>CART</code>使用<code>基尼系数</code>最小化准则作为特征选择指标。<code>基尼系数</code>代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。</p>
<p>在分类问题中，有K个类别，第k个类别的概率为pk，则概率分布的基尼系数表达式为： <span class="math display">\[
Gini(p) = \sum_{k=1}^{K}p_{k}(1-p_k) = 1 - \sum_{k=1}^{K}p_{k}^{2}
\]</span> 对于给定的样本D，假设有K个类别，第k个类别的数量为Ck，则样本D的基尼系数表达式为 <span class="math display">\[
Gini(D) = 1- \sum_{k=1}^{K}\biggl(\frac{|C_k|}{D}\biggr)^2
\]</span> 对于样本D，如果根据特征A的某个值a，把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为： <span class="math display">\[
Gini(D, A) = \frac{D_1}{D}Gini(D_1) + \frac{D_2}{D}Gini(D2)
\]</span> 对于离散特征，<code>CART</code>构建的是二叉树而不是多叉树。复杂度一般情况下是O(logN)</p>
<p>输入：训练集D，基尼系数的阈值，样本个数阈值</p>
<p>输出：决策树T</p>
<ul>
<li>算法
<ol type="1">
<li>对于当前节点的数据集D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。</li>
<li>计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。</li>
<li>计算当前节点现有的各个特征值对数据集D的基尼系数。</li>
<li>在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，左节点的数据集D，右节点的数据集D为D2。</li>
<li>对左右的子节点递归的调用1-4步，生成决策树。</li>
</ol></li>
</ul>
<h3 id="cart回归树"><u>CART回归树</u></h3>
<p><code>CART回归树</code>用平方误差最小化准则进行特征选择，<code>CART回归树</code>的度量目标是对于任意划分特征A，对应的任意划分点s两边划分成数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小对应的特征和特征值划分点。 <span class="math display">\[
\min_{j,s}\left[ \min_{c1}\sum_{x_i\subseteq R_{1}(j,s)}(y_i-c_1)^2+\min_{c_{2}}\sum_{x_i\subseteq R_{2}(j,s)}(y_i-c_1)^2\right]
\]</span> 其中C1为D1数据集的样本输出均值，C2为D2数据集的样本输出均值。</p>
<p><code>CART回归树</code>采用的是<code>最终叶子节点的均值</code>或者<code>中位数</code>来预测输出结果。</p>
<p>输入：训练数据集D</p>
<p>输出：回归树<code>f(x)</code> <span class="math display">\[
f(x) = \sum_{m=1}^{M}c_mI(x\subseteq R_m)
\]</span> 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p>
<ul>
<li><p>算法</p>
<ol type="1">
<li><p>选择最优切分变量<code>j</code>与切分点<code>s</code>， 求解 <span class="math display">\[
\min_{j,s}\left[ \min_{c1}\sum_{x_i\subseteq R_{1}(j,s)}(y_i-c_1)^2+\min_{c_{2}}\sum_{x_i\subseteq R_{2}(j,s)}(y_i-c_1)^2\right]
\]</span> 遍历变量<code>j</code>，对固定的切分变量<code>j</code>扫描切分点<code>s</code>，选择上式达到最小值的对<code>(j, s)</code></p></li>
<li><p>用选定的对<code>(j, s)</code>划分区域并决定相应的输出值。 <span class="math display">\[
R_1(j, s) = \{x|x^{(j)}\le s\}  \ \ \ 和 \ \ \ R_2(j, s) = \{x|x^{(j)}\gt s\}
\]</span></p>
<p><span class="math display">\[
\hat c_m = \frac{1}{N_m}\sum_{x_i\subseteq R_{m}(j,s)}y_i,   \ \ \ \ \ \ x\subseteq R_m, m = 1, 2
\]</span></p></li>
<li><p>继续对两个子区域调用步骤(1), (2)，直至满足停止条件</p></li>
<li><p>将输入空间划分为M个区域R1, R2, …, RM, 生成决策树： <span class="math display">\[
f(x) = \sum_{m=1}^{M}\hat c_mI(x\subseteq R_m)
\]</span></p></li>
</ol></li>
</ul>
<h2 id="决策树的剪枝"><u>决策树的剪枝</u></h2>
<p>在决策树学习中将已生成的树进行简化的过程称为<code>剪枝(pruning)</code>。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。</p>
<p>决策树的剪枝通过极小化决策树整体的损失函数(<code>loss function</code>)或代价函数(<code>cost function</code>)</p>
<p>设<code>树T</code>的叶结点个数为<code>|T|</code>，<code>t</code>是<code>树T</code>的叶结点，该叶结点有<code>Nt</code>个样本点，其中<code>k</code>类的样本点有<code>Ntk</code>个，k=1, 2, …, K，<code>Ht(T)</code>为叶结点<code>t</code>上的经验熵，<code>a</code>大于等于0为参数，决策树学习的损失函数定义为 <span class="math display">\[
\begin{aligned}
C_{\alpha}(T) &amp; =\sum_{t=1}^{|T|}N_tH_t(H)+\alpha|T| \\
\\
H_t(T) &amp; = -\sum_k \frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t} \\
\\
令\ \ \  C(T) &amp; = \sum_{t=1}^{|T|}N_tH_t(H) = -\sum_{t=1}^{|T|}\sum_k^{K}N_{tk}log\frac{N_{tk}}{N_t}
\\
\\
C_{\alpha}(T) &amp; = C(T) + \alpha(T)
\end{aligned}
\]</span> <code>C(T)</code>表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，<code>|T|</code>表示模型复杂度，参数<code>a&gt;=0</code>控制两者之间的影响。</p>
<p>剪枝，就是当<code>a</code>确定时，选择损失函数最小的模型，即损失函数最小的子树。可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合，而决策树剪枝通过优化损失函数还考虑了减少模型复杂度，决策树生成学习局部的模型，而决策树剪枝学习整体的模型。</p>
<p><code>剪枝</code>分为<code>预剪枝</code>和<code>后剪枝</code>。</p>
<p><code>预剪枝</code>是指事先给定某个误差计算方法，找到数据集最佳的二元切分方式，另外，还要确定什么时候停止划分，一旦停止划分会生成一个叶结点。在决策树生成过程中，在划分节点时，若该节点的划分没有提高其在训练集上的准确率，则不进行划分。</p>
<p><code>后剪枝</code>是利用测试集来对树进行剪枝。首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝。然后从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并。</p>
<p>采取类似于伪代码的形式来进行叙述：</p>
<p><strong>1. 输入</strong>：生成算法产生的原始决策树T，惩罚因子α</p>
<p><strong>2. 过程</strong>：</p>
<ol type="1">
<li><p>从下往上地获取T中所有 Node，存入列表<code>_tmp_nodes</code></p></li>
<li><p>对<code>_tmp_nodes</code>中的所有 Node 计算损失，存入列表<code>_old</code></p></li>
<li><p>计算<code>_tmp_nodes</code>中所有 Node 进行局部剪枝后的损失，存入列表<code>_new</code></p></li>
<li><p>进入循环体：</p>
<ol type="1">
<li><p>若<code>_new</code>中所有损失都大于<code>_old</code>中对应的损失、则退出循环体</p></li>
<li><p>否则，设p满足： <span class="math display">\[
p=\mathop{\arg\min}_{p}\_new[p]≤\_old[p]
\]</span> 则对<code>_tmp_nodes[p]</code>进行局部剪枝</p></li>
<li><p>在完成局部剪枝后，更新<code>_old</code>、<code>_new</code>、<code>_tmp_nodes</code>等变量。具体而言，我们无需重新计算它们， 只需更新“被影响到的” Node 所对应的位置的值即可</p></li>
</ol></li>
<li><p>最后调用<code>self.reduce_nodes</code>方法、将被剪掉的 Node 从<code>nodes</code>中除去</p></li>
</ol>
<p><strong>3.输出</strong>：修剪过后的决策树<code>Tα</code></p>
<h2 id="决策树优缺点">决策树优缺点</h2>
<ul>
<li><h4 id="优点">优点</h4></li>
</ul>
<ol type="1">
<li>决策树易于理解和解释</li>
<li>数据的准备往往是简单或者是不必要的</li>
<li>能够同时处理数据型和<em>类别型</em>属性。其他的技术往往要求数据属性的单一</li>
<li>在相对短的时间内能够对大型数据源做出可行且效果良好的结果</li>
<li>可以对有许多属性的数据集构造决策树</li>
<li>决策树常用于集成算法，如随机森林，提升树等</li>
</ol>
<ul>
<li><h4 id="缺点">缺点</h4></li>
</ul>
<ol type="1">
<li>在决策树当中,信息增益的结果偏向于那些具有更多数值的特征</li>
<li>决策树处理缺失数据时的困难。</li>
<li>出现过拟合</li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/pandas.jpeg"
               alt="Peter Tsung" />
          <p class="site-author-name" itemprop="name">Peter Tsung</p>
           
              <p class="site-description motion-element" itemprop="description">I Never Save Anything For The Swim Back.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Peter Tsung</span>
</div>

<!--
<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
                    TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
                            messageStyle: "none"
                                }); 
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Queue(function() {
                    var all = MathJax.Hub.getAllJax(), i;
                            for(i=0; i < all.length; i += 1) {
                                            all[i].SourceElement().parentNode.className += ' has-jax';
                                                    }
                                                        });
        </script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

-->


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  

  

  

  


  <script type="text/javascript" src="/js/google-code-prettify/prettify.js"></script>
  <script type="text/javascript">
  $(window).load(function(){
     $('pre').addClass('prettyprint linenums').attr('style', 'overflow:auto;');
     prettyPrint();
   })    
  </script>
</body>
</html>
